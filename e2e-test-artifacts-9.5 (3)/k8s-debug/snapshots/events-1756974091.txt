NAMESPACE                   LAST SEEN   TYPE      REASON                    OBJECT                                                            MESSAGE
default                     6m14s       Normal    Starting                  node/helm-test-cluster-control-plane
kube-system                 6m27s       Normal    Pulled                    pod/etcd-helm-test-cluster-control-plane                          Container image "registry.k8s.io/etcd:3.5.16-0" already present on machine
default                     6m27s       Normal    NodeHasNoDiskPressure     node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasNoDiskPressure
default                     6m27s       Normal    NodeHasSufficientMemory   node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientMemory
kube-system                 6m27s       Normal    Pulled                    pod/kube-controller-manager-helm-test-cluster-control-plane       Container image "registry.k8s.io/kube-controller-manager:v1.32.0" already present on machine
default                     6m27s       Normal    NodeAllocatableEnforced   node/helm-test-cluster-control-plane                              Updated Node Allocatable limit across pods
kube-system                 6m27s       Normal    Pulled                    pod/kube-apiserver-helm-test-cluster-control-plane                Container image "registry.k8s.io/kube-apiserver:v1.32.0" already present on machine
default                     6m27s       Normal    NodeHasSufficientPID      node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientPID
kube-system                 6m27s       Normal    Pulled                    pod/kube-scheduler-helm-test-cluster-control-plane                Container image "registry.k8s.io/kube-scheduler:v1.32.0" already present on machine
kube-system                 6m26s       Normal    Created                   pod/kube-scheduler-helm-test-cluster-control-plane                Created container: kube-scheduler
kube-system                 6m26s       Normal    Started                   pod/kube-scheduler-helm-test-cluster-control-plane                Started container kube-scheduler
kube-system                 6m26s       Normal    Created                   pod/kube-apiserver-helm-test-cluster-control-plane                Created container: kube-apiserver
kube-system                 6m26s       Normal    Started                   pod/kube-apiserver-helm-test-cluster-control-plane                Started container kube-apiserver
kube-system                 6m26s       Normal    Created                   pod/kube-controller-manager-helm-test-cluster-control-plane       Created container: kube-controller-manager
kube-system                 6m26s       Normal    Started                   pod/kube-controller-manager-helm-test-cluster-control-plane       Started container kube-controller-manager
kube-system                 6m25s       Normal    Created                   pod/etcd-helm-test-cluster-control-plane                          Created container: etcd
kube-system                 6m25s       Normal    Started                   pod/etcd-helm-test-cluster-control-plane                          Started container etcd
default                     6m21s       Normal    NodeAllocatableEnforced   node/helm-test-cluster-control-plane                              Updated Node Allocatable limit across pods
default                     6m21s       Normal    Starting                  node/helm-test-cluster-control-plane                              Starting kubelet.
default                     6m21s       Normal    NodeHasSufficientMemory   node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientMemory
default                     6m21s       Normal    NodeHasNoDiskPressure     node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasNoDiskPressure
default                     6m21s       Normal    NodeHasSufficientPID      node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientPID
kube-system                 6m20s       Normal    LeaderElection            lease/kube-controller-manager                                     helm-test-cluster-control-plane_77f7495e-78c9-4b49-83a4-6cc5e95c32e8 became leader
kube-system                 6m19s       Normal    LeaderElection            lease/kube-scheduler                                              helm-test-cluster-control-plane_1cdead3f-f1fb-4de6-9e9d-9a54ee794f6a became leader
default                     6m16s       Normal    RegisteredNode            node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane event: Registered Node helm-test-cluster-control-plane in Controller
kube-system                 6m15s       Normal    Scheduled                 pod/kube-proxy-j7rrw                                              Successfully assigned kube-system/kube-proxy-j7rrw to helm-test-cluster-control-plane
kube-system                 6m15s       Normal    Scheduled                 pod/kindnet-cx4hx                                                 Successfully assigned kube-system/kindnet-cx4hx to helm-test-cluster-control-plane
kube-system                 6m15s       Warning   FailedMount               pod/kindnet-cx4hx                                                 MountVolume.SetUp failed for volume "kube-api-access-wrnqf" : configmap "kube-root-ca.crt" not found
kube-system                 6m15s       Normal    SuccessfulCreate          daemonset/kindnet                                                 Created pod: kindnet-cx4hx
kube-system                 6m15s       Normal    ScalingReplicaSet         deployment/coredns                                                Scaled up replica set coredns-668d6bf9bc from 0 to 2
local-path-storage          6m15s       Normal    ScalingReplicaSet         deployment/local-path-provisioner                                 Scaled up replica set local-path-provisioner-58cc7856b6 from 0 to 1
kube-system                 6m15s       Normal    SuccessfulCreate          daemonset/kube-proxy                                              Created pod: kube-proxy-j7rrw
kube-system                 6m15s       Warning   FailedMount               pod/kube-proxy-j7rrw                                              MountVolume.SetUp failed for volume "kube-api-access-6kncc" : configmap "kube-root-ca.crt" not found
kube-system                 6m14s       Normal    SuccessfulCreate          replicaset/coredns-668d6bf9bc                                     Created pod: coredns-668d6bf9bc-hlq76
kube-system                 6m14s       Normal    Created                   pod/kindnet-cx4hx                                                 Created container: kindnet-cni
kube-system                 6m14s       Warning   FailedScheduling          pod/coredns-668d6bf9bc-hlq76                                      0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system                 6m14s       Normal    Started                   pod/kube-proxy-j7rrw                                              Started container kube-proxy
local-path-storage          6m14s       Warning   FailedScheduling          pod/local-path-provisioner-58cc7856b6-pvzps                       0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system                 6m14s       Normal    Created                   pod/kube-proxy-j7rrw                                              Created container: kube-proxy
local-path-storage          6m14s       Normal    SuccessfulCreate          replicaset/local-path-provisioner-58cc7856b6                      Created pod: local-path-provisioner-58cc7856b6-pvzps
kube-system                 6m14s       Normal    Pulled                    pod/kindnet-cx4hx                                                 Container image "docker.io/kindest/kindnetd:v20241212-9f82dd49" already present on machine
kube-system                 6m14s       Normal    SuccessfulCreate          replicaset/coredns-668d6bf9bc                                     Created pod: coredns-668d6bf9bc-fgch8
kube-system                 6m14s       Warning   FailedScheduling          pod/coredns-668d6bf9bc-fgch8                                      0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system                 6m14s       Normal    Pulled                    pod/kube-proxy-j7rrw                                              Container image "registry.k8s.io/kube-proxy:v1.32.0" already present on machine
kube-system                 6m13s       Normal    Started                   pod/kindnet-cx4hx                                                 Started container kindnet-cni
local-path-storage          6m3s        Normal    Scheduled                 pod/local-path-provisioner-58cc7856b6-pvzps                       Successfully assigned local-path-storage/local-path-provisioner-58cc7856b6-pvzps to helm-test-cluster-control-plane
kube-system                 6m3s        Normal    Scheduled                 pod/coredns-668d6bf9bc-fgch8                                      Successfully assigned kube-system/coredns-668d6bf9bc-fgch8 to helm-test-cluster-control-plane
default                     6m3s        Normal    NodeReady                 node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeReady
kube-system                 6m3s        Normal    Scheduled                 pod/coredns-668d6bf9bc-hlq76                                      Successfully assigned kube-system/coredns-668d6bf9bc-hlq76 to helm-test-cluster-control-plane
kube-system                 6m2s        Normal    Pulled                    pod/coredns-668d6bf9bc-fgch8                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
local-path-storage          6m2s        Normal    Pulled                    pod/local-path-provisioner-58cc7856b6-pvzps                       Container image "docker.io/kindest/local-path-provisioner:v20241212-8ac705d0" already present on machine
kube-system                 6m2s        Normal    Pulled                    pod/coredns-668d6bf9bc-hlq76                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 6m1s        Normal    Started                   pod/coredns-668d6bf9bc-fgch8                                      Started container coredns
local-path-storage          6m1s        Normal    Created                   pod/local-path-provisioner-58cc7856b6-pvzps                       Created container: local-path-provisioner
kube-system                 6m1s        Normal    Created                   pod/coredns-668d6bf9bc-fgch8                                      Created container: coredns
kube-system                 6m1s        Normal    Created                   pod/coredns-668d6bf9bc-hlq76                                      Created container: coredns
kube-system                 6m1s        Normal    Started                   pod/coredns-668d6bf9bc-hlq76                                      Started container coredns
local-path-storage          6m1s        Normal    Started                   pod/local-path-provisioner-58cc7856b6-pvzps                       Started container local-path-provisioner
kube-system                 5m37s       Normal    Scheduled                 pod/coredns-6f5f77fcb7-6jtst                                      Successfully assigned kube-system/coredns-6f5f77fcb7-6jtst to helm-test-cluster-control-plane
kube-system                 5m37s       Normal    SuccessfulDelete          replicaset/coredns-668d6bf9bc                                     Deleted pod: coredns-668d6bf9bc-hlq76
kube-system                 5m37s       Normal    SuccessfulCreate          replicaset/coredns-6f5f77fcb7                                     Created pod: coredns-6f5f77fcb7-fdx4j
kube-system                 5m37s       Normal    ScalingReplicaSet         deployment/coredns                                                Scaled up replica set coredns-6f5f77fcb7 from 0 to 1
kube-system                 5m37s       Normal    ScalingReplicaSet         deployment/coredns                                                Scaled down replica set coredns-668d6bf9bc from 2 to 1
kube-system                 5m37s       Normal    Scheduled                 pod/coredns-6f5f77fcb7-fdx4j                                      Successfully assigned kube-system/coredns-6f5f77fcb7-fdx4j to helm-test-cluster-control-plane
kube-system                 5m37s       Normal    ScalingReplicaSet         deployment/coredns                                                Scaled up replica set coredns-6f5f77fcb7 from 1 to 2
kube-system                 5m37s       Normal    Killing                   pod/coredns-668d6bf9bc-hlq76                                      Stopping container coredns
kube-system                 5m37s       Normal    Pulled                    pod/coredns-6f5f77fcb7-6jtst                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 5m37s       Normal    SuccessfulCreate          replicaset/coredns-6f5f77fcb7                                     Created pod: coredns-6f5f77fcb7-6jtst
kube-system                 5m36s       Normal    Started                   pod/coredns-6f5f77fcb7-6jtst                                      Started container coredns
kube-system                 5m36s       Normal    SuccessfulDelete          replicaset/coredns-668d6bf9bc                                     Deleted pod: coredns-668d6bf9bc-fgch8
kube-system                 5m36s       Normal    Created                   pod/coredns-6f5f77fcb7-6jtst                                      Created container: coredns
kube-system                 5m36s       Normal    ScalingReplicaSet         deployment/coredns                                                Scaled down replica set coredns-668d6bf9bc from 1 to 0
kube-system                 5m36s       Normal    Pulled                    pod/coredns-6f5f77fcb7-fdx4j                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 5m36s       Normal    Created                   pod/coredns-6f5f77fcb7-fdx4j                                      Created container: coredns
kube-system                 5m36s       Normal    Started                   pod/coredns-6f5f77fcb7-fdx4j                                      Started container coredns
kube-system                 5m36s       Normal    Killing                   pod/coredns-668d6bf9bc-fgch8                                      Stopping container coredns
default                     5m36s       Normal    Scheduled                 pod/dns-test                                                      Successfully assigned default/dns-test to helm-test-cluster-control-plane
default                     5m36s       Normal    Pulling                   pod/dns-test                                                      Pulling image "busybox:1.28"
default                     5m34s       Normal    Started                   pod/dns-test                                                      Started container dns-test
default                     5m34s       Normal    Pulled                    pod/dns-test                                                      Successfully pulled image "busybox:1.28" in 1.886s (1.886s including waiting). Image size: 727869 bytes.
default                     5m34s       Normal    Created                   pod/dns-test                                                      Created container: dns-test
kube-system                 5m14s       Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                            Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 5m14s       Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Started container s3-pod-reconciler
kube-system                 5m14s       Normal    SuccessfulCreate          daemonset/s3-csi-node                                             Created pod: s3-csi-node-nkx5d
kube-system                 5m14s       Normal    Pulling                   pod/s3-csi-node-nkx5d                                             Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0"
kube-system                 5m14s       Normal    Started                   pod/s3-csi-node-nkx5d                                             Started container s3-plugin
kube-system                 5m14s       Normal    Scheduled                 pod/s3-csi-controller-84f6bdfd58-7cpmf                            Successfully assigned kube-system/s3-csi-controller-84f6bdfd58-7cpmf to helm-test-cluster-control-plane
kube-system                 5m14s       Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                            Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 5m14s       Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Created container: s3-csi-controller
kube-system                 5m14s       Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Started container s3-csi-controller
kube-system                 5m14s       Normal    Created                   pod/s3-csi-node-nkx5d                                             Created container: s3-plugin
kube-system                 5m14s       Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Created container: s3-pod-reconciler
kube-system                 5m14s       Normal    Pulled                    pod/s3-csi-node-nkx5d                                             Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 5m14s       Normal    Pulling                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0"
kube-system                 5m14s       Normal    Scheduled                 pod/s3-csi-node-nkx5d                                             Successfully assigned kube-system/s3-csi-node-nkx5d to helm-test-cluster-control-plane
kube-system                 5m14s       Normal    ScalingReplicaSet         deployment/s3-csi-controller                                      Scaled up replica set s3-csi-controller-84f6bdfd58 from 0 to 1
kube-system                 5m14s       Normal    SuccessfulCreate          replicaset/s3-csi-controller-84f6bdfd58                           Created pod: s3-csi-controller-84f6bdfd58-7cpmf
kube-system                 5m11s       Normal    Pulled                    pod/s3-csi-node-nkx5d                                             Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0" in 3.083s (3.083s including waiting). Image size: 15579380 bytes.
kube-system                 5m11s       Normal    Created                   pod/s3-csi-node-nkx5d                                             Created container: node-driver-registrar
kube-system                 5m10s       Normal    Started                   pod/s3-csi-node-nkx5d                                             Started container node-driver-registrar
kube-system                 5m10s       Normal    Pulling                   pod/s3-csi-node-nkx5d                                             Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0"
kube-system                 5m8s        Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Started container csi-provisioner
kube-system                 5m8s        Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Created container: csi-provisioner
kube-system                 5m8s        Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                            Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0" in 2.701s (5.674s including waiting). Image size: 35709312 bytes.
kube-system                 5m6s        Normal    Pulled                    pod/s3-csi-node-nkx5d                                             Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0" in 2.149s (4.758s including waiting). Image size: 15557083 bytes.
kube-system                 5m6s        Normal    Created                   pod/s3-csi-node-nkx5d                                             Created container: liveness-probe
kube-system                 5m6s        Normal    Started                   pod/s3-csi-node-nkx5d                                             Started container liveness-probe
volume-3903                 5m2s        Warning   ProvisioningFailed        persistentvolumeclaim/pvc-xcdzz                                   storageclass.storage.k8s.io "volume-3903" not found
volume-1120                 5m2s        Normal    Provisioning              persistentvolumeclaim/s3.csi.scality.comsr5nc                     External provisioner is provisioning volume for claim "volume-1120/s3.csi.scality.comsr5nc"
volume-1120                 5m2s        Normal    ExternalProvisioning      persistentvolumeclaim/s3.csi.scality.comsr5nc                     Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
volume-1120                 5m2s        Normal    ProvisioningSucceeded     persistentvolumeclaim/s3.csi.scality.comsr5nc                     Successfully provisioned volume pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6
cache-6176                  5m          Normal    Scheduled                 pod/pvc-tester-tsdmc                                              Successfully assigned cache-6176/pvc-tester-tsdmc to helm-test-cluster-control-plane
volume-3903                 5m          Normal    Scheduled                 pod/s3-injector                                                   Successfully assigned volume-3903/s3-injector to helm-test-cluster-control-plane
mount-s3                    5m          Normal    Scheduled                 pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Successfully assigned mount-s3/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50 to helm-test-cluster-control-plane
mount-s3                    5m          Normal    Scheduled                 pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Successfully assigned mount-s3/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20 to helm-test-cluster-control-plane
cache-2772                  5m          Normal    Scheduled                 pod/pvc-tester-dwhq2                                              Successfully assigned cache-2772/pvc-tester-dwhq2 to helm-test-cluster-control-plane
mount-s3                    5m          Normal    Scheduled                 pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Successfully assigned mount-s3/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281 to helm-test-cluster-control-plane
mount-s3                    5m          Normal    Scheduled                 pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Successfully assigned mount-s3/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6 to helm-test-cluster-control-plane
volume-1120                 5m          Normal    Scheduled                 pod/s3-injector                                                   Successfully assigned volume-1120/s3-injector to helm-test-cluster-control-plane
mount-s3                    4m57s       Normal    Scheduled                 pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Successfully assigned mount-s3/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73 to helm-test-cluster-control-plane
cache-9720                  4m44s       Normal    Scheduled                 pod/pvc-tester-zl2k5                                              Successfully assigned cache-9720/pvc-tester-zl2k5 to helm-test-cluster-control-plane
mount-s3                    4m44s       Normal    Scheduled                 pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Successfully assigned mount-s3/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796 to helm-test-cluster-control-plane
mount-s3                    4m31s       Normal    Scheduled                 pod/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0   Successfully assigned mount-s3/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0 to helm-test-cluster-control-plane
mount-s3                    4m          Normal    Scheduled                 pod/mp-159b10046d4e6e59585206321e043d7e26ef27ddda8259914b58fa7b   Successfully assigned mount-s3/mp-159b10046d4e6e59585206321e043d7e26ef27ddda8259914b58fa7b to helm-test-cluster-control-plane
credentials-9803            4m          Normal    Scheduled                 pod/pvc-tester-67jbd                                              Successfully assigned credentials-9803/pvc-tester-67jbd to helm-test-cluster-control-plane
credentials-9438            2m45s       Normal    Scheduled                 pod/pvc-tester-7f9mh                                              Successfully assigned credentials-9438/pvc-tester-7f9mh to helm-test-cluster-control-plane
mount-s3                    2m45s       Normal    Scheduled                 pod/mp-6cf8cfb7544fed74f72a401dafc06778f89669c0a8f9bef49e463381   Successfully assigned mount-s3/mp-6cf8cfb7544fed74f72a401dafc06778f89669c0a8f9bef49e463381 to helm-test-cluster-control-plane
mount-s3                    2m15s       Normal    Scheduled                 pod/mp-a751bc662c2cfe80e7b81c216ad77118a77b6bb4aa0d8699c43889c2   Successfully assigned mount-s3/mp-a751bc662c2cfe80e7b81c216ad77118a77b6bb4aa0d8699c43889c2 to helm-test-cluster-control-plane
mount-s3                    119s        Normal    Started                   pod/mp-159b10046d4e6e59585206321e043d7e26ef27ddda8259914b58fa7b   Started container mountpoint
mount-s3                    119s        Normal    Created                   pod/mp-159b10046d4e6e59585206321e043d7e26ef27ddda8259914b58fa7b   Created container: mountpoint
mount-s3                    119s        Normal    Pulled                    pod/mp-159b10046d4e6e59585206321e043d7e26ef27ddda8259914b58fa7b   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
credentials-9803            112s        Warning   FailedMount               pod/pvc-tester-67jbd                                              MountVolume.SetUp failed for volume "s3-e2e-pv-ae989c45-3114-4a17-9b31-e019f99153b0" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-7b97gdfb9dsxvjpddlfgmdl8ps8r9zs49zcbg5hjf69gks9m" at "/var/lib/kubelet/pods/4b076a0b-7a4d-49d6-b590-669c9e25accd/volumes/kubernetes.io~csi/s3-e2e-pv-ae989c45-3114-4a17-9b31-e019f99153b0/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/4b076a0b-7a4d-49d6-b590-669c9e25accd/volumes/kubernetes.io~csi/s3-e2e-pv-ae989c45-3114-4a17-9b31-e019f99153b0/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
mount-s3                    73s         Normal    Pulled                    pod/mp-5fddffcfd437a730a3da12bbc870747f9680f8e57c90b5efd5139229   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    73s         Normal    Created                   pod/mp-5fddffcfd437a730a3da12bbc870747f9680f8e57c90b5efd5139229   Created container: mountpoint
mount-s3                    73s         Normal    Scheduled                 pod/mp-5fddffcfd437a730a3da12bbc870747f9680f8e57c90b5efd5139229   Successfully assigned mount-s3/mp-5fddffcfd437a730a3da12bbc870747f9680f8e57c90b5efd5139229 to helm-test-cluster-control-plane
mount-s3                    72s         Normal    Started                   pod/mp-5fddffcfd437a730a3da12bbc870747f9680f8e57c90b5efd5139229   Started container mountpoint
mount-s3                    58s         Warning   BackOff                   pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Back-off restarting failed container mountpoint in pod mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50_mount-s3(2d8b9a90-791c-4e15-b6b7-1c9697500231)
mount-s3                    58s         Warning   BackOff                   pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Back-off restarting failed container mountpoint in pod mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6_mount-s3(50509ecd-efd8-4c46-aac5-f0cc05c1bd6b)
mount-s3                    58s         Warning   BackOff                   pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Back-off restarting failed container mountpoint in pod mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281_mount-s3(82256f94-8524-4c93-9c15-d35ca741e9b9)
mount-s3                    58s         Warning   BackOff                   pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Back-off restarting failed container mountpoint in pod mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20_mount-s3(a21bc08c-8c39-4d9f-8af2-71fdfaabddbf)
mount-s3                    55s         Warning   BackOff                   pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Back-off restarting failed container mountpoint in pod mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73_mount-s3(e9a27433-f22a-4591-a926-fa219817d606)
volume-3903                 50s         Warning   FailedMount               pod/s3-injector                                                   MountVolume.SetUp failed for volume "s3.csi.scality.com-jrq4b" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-c6c4dqwfz72d6nmggpnx986p5252nj7vwvf8zfxgxw98dmpw" at "/var/lib/kubelet/pods/4f49525f-bef6-478b-83f9-1673c510a51a/volumes/kubernetes.io~csi/s3.csi.scality.com-jrq4b/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/4f49525f-bef6-478b-83f9-1673c510a51a/volumes/kubernetes.io~csi/s3.csi.scality.com-jrq4b/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
cache-6176                  50s         Warning   FailedMount               pod/pvc-tester-tsdmc                                              MountVolume.SetUp failed for volume "s3-e2e-pv-5a1ec614-7174-4db3-997e-1ad5aa6fb41b" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-rzx8ggxnxcj5btdxlf5blhx8sxkzv92hzpbgmxtjs9jw9gxm" at "/var/lib/kubelet/pods/11c6ba65-d874-46e5-8a9f-d48b9ddce169/volumes/kubernetes.io~csi/s3-e2e-pv-5a1ec614-7174-4db3-997e-1ad5aa6fb41b/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/11c6ba65-d874-46e5-8a9f-d48b9ddce169/volumes/kubernetes.io~csi/s3-e2e-pv-5a1ec614-7174-4db3-997e-1ad5aa6fb41b/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
cache-2772                  50s         Warning   FailedMount               pod/pvc-tester-dwhq2                                              MountVolume.SetUp failed for volume "s3-e2e-pv-9889fc2f-c808-4623-b190-7e4bcdeb4f74" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-wckmzrfgk7pn6sb2fbmws44k5n2vmx2tnjpngvnjqnnhbmfq" at "/var/lib/kubelet/pods/43cea5c5-d75d-4b6d-a669-8b9f63a8a19d/volumes/kubernetes.io~csi/s3-e2e-pv-9889fc2f-c808-4623-b190-7e4bcdeb4f74/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/43cea5c5-d75d-4b6d-a669-8b9f63a8a19d/volumes/kubernetes.io~csi/s3-e2e-pv-9889fc2f-c808-4623-b190-7e4bcdeb4f74/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
volume-1120                 50s         Warning   FailedMount               pod/s3-injector                                                   MountVolume.SetUp failed for volume "pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6" : rpc error: code = Internal desc = Could not mount "csi-s3-56fce90b-b62b-4eb0-b3ea-fafb76b9323a" at "/var/lib/kubelet/pods/72939224-f413-49d1-b6b6-bbee9aae3f22/volumes/kubernetes.io~csi/pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/72939224-f413-49d1-b6b6-bbee9aae3f22/volumes/kubernetes.io~csi/pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
mount-s3                    46s         Normal    Started                   pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Started container mountpoint
mount-s3                    46s         Normal    Created                   pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Created container: mountpoint
mount-s3                    46s         Normal    Pulled                    pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    45s         Normal    Started                   pod/mp-6cf8cfb7544fed74f72a401dafc06778f89669c0a8f9bef49e463381   Started container mountpoint
mount-s3                    45s         Normal    Created                   pod/mp-6cf8cfb7544fed74f72a401dafc06778f89669c0a8f9bef49e463381   Created container: mountpoint
mount-s3                    45s         Normal    Pulled                    pod/mp-6cf8cfb7544fed74f72a401dafc06778f89669c0a8f9bef49e463381   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    44s         Normal    Pulled                    pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    44s         Normal    Started                   pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Started container mountpoint
mount-s3                    44s         Normal    Created                   pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Created container: mountpoint
mount-s3                    43s         Normal    Pulled                    pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    43s         Normal    Created                   pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Created container: mountpoint
mount-s3                    43s         Normal    Started                   pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Started container mountpoint
mount-s3                    43s         Normal    Created                   pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Created container: mountpoint
mount-s3                    43s         Normal    Started                   pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Started container mountpoint
mount-s3                    43s         Normal    Created                   pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Created container: mountpoint
mount-s3                    43s         Normal    Pulled                    pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    43s         Normal    Pulled                    pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    43s         Normal    Started                   pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Started container mountpoint
mount-s3                    42s         Warning   BackOff                   pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Back-off restarting failed container mountpoint in pod mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796_mount-s3(b147ccd8-9e86-4e27-99e2-ccf383f68573)
credentials-9438            37s         Warning   FailedMount               pod/pvc-tester-7f9mh                                              MountVolume.SetUp failed for volume "s3-e2e-pv-ac9eff36-d5ba-451c-b10b-a151a677bfba" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-l5hfm8zshd7mh8w5l5bkmprxrqwv8rq7qdr4xtppvbv9mh5j" at "/var/lib/kubelet/pods/de74c82e-0196-41fe-8fef-8ddb9f4c2407/volumes/kubernetes.io~csi/s3-e2e-pv-ac9eff36-d5ba-451c-b10b-a151a677bfba/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/de74c82e-0196-41fe-8fef-8ddb9f4c2407/volumes/kubernetes.io~csi/s3-e2e-pv-ac9eff36-d5ba-451c-b10b-a151a677bfba/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
cache-9720                  34s         Warning   FailedMount               pod/pvc-tester-zl2k5                                              MountVolume.SetUp failed for volume "s3-e2e-pv-8166c059-6d8b-4308-a0d6-39f0993d081d" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-cv867fmkkbn4pq4c8gcr9wfdw5sd66ksnnw4wp7wzz226dzb" at "/var/lib/kubelet/pods/b1936d9d-388d-4083-96b4-56bcdf7d3711/volumes/kubernetes.io~csi/s3-e2e-pv-8166c059-6d8b-4308-a0d6-39f0993d081d/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/b1936d9d-388d-4083-96b4-56bcdf7d3711/volumes/kubernetes.io~csi/s3-e2e-pv-8166c059-6d8b-4308-a0d6-39f0993d081d/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
mount-s3                    31s         Normal    Started                   pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Started container mountpoint
mount-s3                    31s         Normal    Pulled                    pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    31s         Normal    Created                   pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Created container: mountpoint
mount-s3                    30s         Warning   BackOff                   pod/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0   Back-off restarting failed container mountpoint in pod mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0_mount-s3(c7f92196-6128-47fd-be41-12514b0a7bd4)
mount-s3                    18s         Normal    Started                   pod/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0   Started container mountpoint
mount-s3                    18s         Normal    Created                   pod/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0   Created container: mountpoint
mount-s3                    18s         Normal    Pulled                    pod/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    14s         Normal    Pulled                    pod/mp-a751bc662c2cfe80e7b81c216ad77118a77b6bb4aa0d8699c43889c2   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    14s         Normal    Created                   pod/mp-a751bc662c2cfe80e7b81c216ad77118a77b6bb4aa0d8699c43889c2   Created container: mountpoint
mount-s3                    13s         Normal    Started                   pod/mp-a751bc662c2cfe80e7b81c216ad77118a77b6bb4aa0d8699c43889c2   Started container mountpoint
mount-s3                    11s         Normal    Scheduled                 pod/mp-9867ec90814a02beada169f6ad653e6e604622f913b6167e7afe4cae   Successfully assigned mount-s3/mp-9867ec90814a02beada169f6ad653e6e604622f913b6167e7afe4cae to helm-test-cluster-control-plane
directorypermissions-7119   11s         Normal    Scheduled                 pod/pvc-tester-kf6nt                                              Successfully assigned directorypermissions-7119/pvc-tester-kf6nt to helm-test-cluster-control-plane
mount-s3                    10s         Normal    Started                   pod/mp-9867ec90814a02beada169f6ad653e6e604622f913b6167e7afe4cae   Started container mountpoint
mount-s3                    10s         Normal    Created                   pod/mp-9867ec90814a02beada169f6ad653e6e604622f913b6167e7afe4cae   Created container: mountpoint
mount-s3                    10s         Normal    Pulled                    pod/mp-9867ec90814a02beada169f6ad653e6e604622f913b6167e7afe4cae   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
directorypermissions-7119   3s          Warning   FailedMount               pod/pvc-tester-kf6nt                                              MountVolume.SetUp failed for volume "s3-e2e-pv-346cc619-8c22-4e55-8764-f75218843ad4" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-spd8hz4r8nlptd892jmjvpn8f69vb2qptbzqbs2vl65mjch7" at "/var/lib/kubelet/pods/12ee1427-a36e-4d3b-9eba-255ba65f6c19/volumes/kubernetes.io~csi/s3-e2e-pv-346cc619-8c22-4e55-8764-f75218843ad4/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/12ee1427-a36e-4d3b-9eba-255ba65f6c19/volumes/kubernetes.io~csi/s3-e2e-pv-346cc619-8c22-4e55-8764-f75218843ad4/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
