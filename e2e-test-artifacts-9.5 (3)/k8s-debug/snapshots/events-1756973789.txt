NAMESPACE                   LAST SEEN   TYPE      REASON                    OBJECT                                                        MESSAGE
default                     72s         Normal    Starting                  node/helm-test-cluster-control-plane                          
kube-system                 85s         Normal    Pulled                    pod/kube-scheduler-helm-test-cluster-control-plane            Container image "registry.k8s.io/kube-scheduler:v1.32.0" already present on machine
default                     85s         Normal    NodeAllocatableEnforced   node/helm-test-cluster-control-plane                          Updated Node Allocatable limit across pods
default                     85s         Normal    NodeHasSufficientPID      node/helm-test-cluster-control-plane                          Node helm-test-cluster-control-plane status is now: NodeHasSufficientPID
kube-system                 85s         Normal    Pulled                    pod/kube-controller-manager-helm-test-cluster-control-plane   Container image "registry.k8s.io/kube-controller-manager:v1.32.0" already present on machine
default                     85s         Normal    NodeHasSufficientMemory   node/helm-test-cluster-control-plane                          Node helm-test-cluster-control-plane status is now: NodeHasSufficientMemory
default                     85s         Normal    NodeHasNoDiskPressure     node/helm-test-cluster-control-plane                          Node helm-test-cluster-control-plane status is now: NodeHasNoDiskPressure
kube-system                 85s         Normal    Pulled                    pod/kube-apiserver-helm-test-cluster-control-plane            Container image "registry.k8s.io/kube-apiserver:v1.32.0" already present on machine
kube-system                 85s         Normal    Pulled                    pod/etcd-helm-test-cluster-control-plane                      Container image "registry.k8s.io/etcd:3.5.16-0" already present on machine
kube-system                 84s         Normal    Created                   pod/kube-apiserver-helm-test-cluster-control-plane            Created container: kube-apiserver
kube-system                 84s         Normal    Started                   pod/kube-apiserver-helm-test-cluster-control-plane            Started container kube-apiserver
kube-system                 84s         Normal    Created                   pod/kube-scheduler-helm-test-cluster-control-plane            Created container: kube-scheduler
kube-system                 84s         Normal    Created                   pod/kube-controller-manager-helm-test-cluster-control-plane   Created container: kube-controller-manager
kube-system                 84s         Normal    Started                   pod/kube-controller-manager-helm-test-cluster-control-plane   Started container kube-controller-manager
kube-system                 84s         Normal    Started                   pod/kube-scheduler-helm-test-cluster-control-plane            Started container kube-scheduler
kube-system                 83s         Normal    Started                   pod/etcd-helm-test-cluster-control-plane                      Started container etcd
kube-system                 83s         Normal    Created                   pod/etcd-helm-test-cluster-control-plane                      Created container: etcd
default                     79s         Normal    NodeHasNoDiskPressure     node/helm-test-cluster-control-plane                          Node helm-test-cluster-control-plane status is now: NodeHasNoDiskPressure
default                     79s         Normal    Starting                  node/helm-test-cluster-control-plane                          Starting kubelet.
default                     79s         Normal    NodeAllocatableEnforced   node/helm-test-cluster-control-plane                          Updated Node Allocatable limit across pods
default                     79s         Normal    NodeHasSufficientMemory   node/helm-test-cluster-control-plane                          Node helm-test-cluster-control-plane status is now: NodeHasSufficientMemory
default                     79s         Normal    NodeHasSufficientPID      node/helm-test-cluster-control-plane                          Node helm-test-cluster-control-plane status is now: NodeHasSufficientPID
kube-system                 78s         Normal    LeaderElection            lease/kube-controller-manager                                 helm-test-cluster-control-plane_77f7495e-78c9-4b49-83a4-6cc5e95c32e8 became leader
kube-system                 77s         Normal    LeaderElection            lease/kube-scheduler                                          helm-test-cluster-control-plane_1cdead3f-f1fb-4de6-9e9d-9a54ee794f6a became leader
default                     74s         Normal    RegisteredNode            node/helm-test-cluster-control-plane                          Node helm-test-cluster-control-plane event: Registered Node helm-test-cluster-control-plane in Controller
kube-system                 73s         Normal    Scheduled                 pod/kube-proxy-j7rrw                                          Successfully assigned kube-system/kube-proxy-j7rrw to helm-test-cluster-control-plane
kube-system                 73s         Warning   FailedMount               pod/kube-proxy-j7rrw                                          MountVolume.SetUp failed for volume "kube-api-access-6kncc" : configmap "kube-root-ca.crt" not found
kube-system                 73s         Normal    SuccessfulCreate          daemonset/kube-proxy                                          Created pod: kube-proxy-j7rrw
kube-system                 73s         Normal    SuccessfulCreate          daemonset/kindnet                                             Created pod: kindnet-cx4hx
kube-system                 73s         Warning   FailedMount               pod/kindnet-cx4hx                                             MountVolume.SetUp failed for volume "kube-api-access-wrnqf" : configmap "kube-root-ca.crt" not found
kube-system                 73s         Normal    Scheduled                 pod/kindnet-cx4hx                                             Successfully assigned kube-system/kindnet-cx4hx to helm-test-cluster-control-plane
kube-system                 73s         Normal    ScalingReplicaSet         deployment/coredns                                            Scaled up replica set coredns-668d6bf9bc from 0 to 2
local-path-storage          73s         Normal    ScalingReplicaSet         deployment/local-path-provisioner                             Scaled up replica set local-path-provisioner-58cc7856b6 from 0 to 1
kube-system                 72s         Normal    SuccessfulCreate          replicaset/coredns-668d6bf9bc                                 Created pod: coredns-668d6bf9bc-hlq76
kube-system                 72s         Warning   FailedScheduling          pod/coredns-668d6bf9bc-hlq76                                  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system                 72s         Normal    SuccessfulCreate          replicaset/coredns-668d6bf9bc                                 Created pod: coredns-668d6bf9bc-fgch8
kube-system                 72s         Normal    Created                   pod/kindnet-cx4hx                                             Created container: kindnet-cni
local-path-storage          72s         Normal    SuccessfulCreate          replicaset/local-path-provisioner-58cc7856b6                  Created pod: local-path-provisioner-58cc7856b6-pvzps
kube-system                 72s         Normal    Pulled                    pod/kindnet-cx4hx                                             Container image "docker.io/kindest/kindnetd:v20241212-9f82dd49" already present on machine
kube-system                 72s         Warning   FailedScheduling          pod/coredns-668d6bf9bc-fgch8                                  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system                 72s         Normal    Started                   pod/kube-proxy-j7rrw                                          Started container kube-proxy
kube-system                 72s         Normal    Created                   pod/kube-proxy-j7rrw                                          Created container: kube-proxy
kube-system                 72s         Normal    Pulled                    pod/kube-proxy-j7rrw                                          Container image "registry.k8s.io/kube-proxy:v1.32.0" already present on machine
local-path-storage          72s         Warning   FailedScheduling          pod/local-path-provisioner-58cc7856b6-pvzps                   0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system                 71s         Normal    Started                   pod/kindnet-cx4hx                                             Started container kindnet-cni
default                     61s         Normal    NodeReady                 node/helm-test-cluster-control-plane                          Node helm-test-cluster-control-plane status is now: NodeReady
kube-system                 61s         Normal    Scheduled                 pod/coredns-668d6bf9bc-hlq76                                  Successfully assigned kube-system/coredns-668d6bf9bc-hlq76 to helm-test-cluster-control-plane
kube-system                 61s         Normal    Scheduled                 pod/coredns-668d6bf9bc-fgch8                                  Successfully assigned kube-system/coredns-668d6bf9bc-fgch8 to helm-test-cluster-control-plane
local-path-storage          61s         Normal    Scheduled                 pod/local-path-provisioner-58cc7856b6-pvzps                   Successfully assigned local-path-storage/local-path-provisioner-58cc7856b6-pvzps to helm-test-cluster-control-plane
kube-system                 60s         Normal    Pulled                    pod/coredns-668d6bf9bc-hlq76                                  Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 60s         Normal    Pulled                    pod/coredns-668d6bf9bc-fgch8                                  Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
local-path-storage          60s         Normal    Pulled                    pod/local-path-provisioner-58cc7856b6-pvzps                   Container image "docker.io/kindest/local-path-provisioner:v20241212-8ac705d0" already present on machine
kube-system                 59s         Normal    Started                   pod/coredns-668d6bf9bc-hlq76                                  Started container coredns
kube-system                 59s         Normal    Started                   pod/coredns-668d6bf9bc-fgch8                                  Started container coredns
local-path-storage          59s         Normal    Created                   pod/local-path-provisioner-58cc7856b6-pvzps                   Created container: local-path-provisioner
local-path-storage          59s         Normal    Started                   pod/local-path-provisioner-58cc7856b6-pvzps                   Started container local-path-provisioner
kube-system                 59s         Normal    Created                   pod/coredns-668d6bf9bc-fgch8                                  Created container: coredns
kube-system                 59s         Normal    Created                   pod/coredns-668d6bf9bc-hlq76                                  Created container: coredns
kube-system                 35s         Normal    SuccessfulCreate          replicaset/coredns-6f5f77fcb7                                 Created pod: coredns-6f5f77fcb7-6jtst
kube-system                 35s         Normal    ScalingReplicaSet         deployment/coredns                                            Scaled up replica set coredns-6f5f77fcb7 from 1 to 2
kube-system                 35s         Normal    ScalingReplicaSet         deployment/coredns                                            Scaled down replica set coredns-668d6bf9bc from 2 to 1
kube-system                 35s         Normal    SuccessfulDelete          replicaset/coredns-668d6bf9bc                                 Deleted pod: coredns-668d6bf9bc-hlq76
kube-system                 35s         Normal    Pulled                    pod/coredns-6f5f77fcb7-6jtst                                  Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 35s         Normal    Scheduled                 pod/coredns-6f5f77fcb7-fdx4j                                  Successfully assigned kube-system/coredns-6f5f77fcb7-fdx4j to helm-test-cluster-control-plane
kube-system                 35s         Normal    ScalingReplicaSet         deployment/coredns                                            Scaled up replica set coredns-6f5f77fcb7 from 0 to 1
kube-system                 35s         Normal    Scheduled                 pod/coredns-6f5f77fcb7-6jtst                                  Successfully assigned kube-system/coredns-6f5f77fcb7-6jtst to helm-test-cluster-control-plane
kube-system                 35s         Normal    SuccessfulCreate          replicaset/coredns-6f5f77fcb7                                 Created pod: coredns-6f5f77fcb7-fdx4j
kube-system                 35s         Normal    Killing                   pod/coredns-668d6bf9bc-hlq76                                  Stopping container coredns
kube-system                 34s         Normal    Created                   pod/coredns-6f5f77fcb7-fdx4j                                  Created container: coredns
kube-system                 34s         Normal    Started                   pod/coredns-6f5f77fcb7-fdx4j                                  Started container coredns
kube-system                 34s         Normal    Pulled                    pod/coredns-6f5f77fcb7-fdx4j                                  Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
default                     34s         Normal    Pulling                   pod/dns-test                                                  Pulling image "busybox:1.28"
kube-system                 34s         Normal    SuccessfulDelete          replicaset/coredns-668d6bf9bc                                 Deleted pod: coredns-668d6bf9bc-fgch8
kube-system                 34s         Normal    ScalingReplicaSet         deployment/coredns                                            Scaled down replica set coredns-668d6bf9bc from 1 to 0
kube-system                 34s         Normal    Killing                   pod/coredns-668d6bf9bc-fgch8                                  Stopping container coredns
default                     34s         Normal    Scheduled                 pod/dns-test                                                  Successfully assigned default/dns-test to helm-test-cluster-control-plane
kube-system                 34s         Normal    Started                   pod/coredns-6f5f77fcb7-6jtst                                  Started container coredns
kube-system                 34s         Normal    Created                   pod/coredns-6f5f77fcb7-6jtst                                  Created container: coredns
default                     32s         Normal    Started                   pod/dns-test                                                  Started container dns-test
default                     32s         Normal    Created                   pod/dns-test                                                  Created container: dns-test
default                     32s         Normal    Pulled                    pod/dns-test                                                  Successfully pulled image "busybox:1.28" in 1.886s (1.886s including waiting). Image size: 727869 bytes.
kube-system                 12s         Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                        Created container: s3-csi-controller
kube-system                 12s         Normal    Pulling                   pod/s3-csi-controller-84f6bdfd58-7cpmf                        Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0"
kube-system                 12s         Normal    Scheduled                 pod/s3-csi-controller-84f6bdfd58-7cpmf                        Successfully assigned kube-system/s3-csi-controller-84f6bdfd58-7cpmf to helm-test-cluster-control-plane
kube-system                 12s         Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                        Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 12s         Normal    SuccessfulCreate          replicaset/s3-csi-controller-84f6bdfd58                       Created pod: s3-csi-controller-84f6bdfd58-7cpmf
kube-system                 12s         Normal    ScalingReplicaSet         deployment/s3-csi-controller                                  Scaled up replica set s3-csi-controller-84f6bdfd58 from 0 to 1
kube-system                 12s         Normal    Scheduled                 pod/s3-csi-node-nkx5d                                         Successfully assigned kube-system/s3-csi-node-nkx5d to helm-test-cluster-control-plane
kube-system                 12s         Normal    Pulled                    pod/s3-csi-node-nkx5d                                         Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 12s         Normal    Created                   pod/s3-csi-node-nkx5d                                         Created container: s3-plugin
kube-system                 12s         Normal    Started                   pod/s3-csi-node-nkx5d                                         Started container s3-plugin
kube-system                 12s         Normal    Pulling                   pod/s3-csi-node-nkx5d                                         Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0"
kube-system                 12s         Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                        Started container s3-csi-controller
kube-system                 12s         Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                        Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 12s         Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                        Created container: s3-pod-reconciler
kube-system                 12s         Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                        Started container s3-pod-reconciler
kube-system                 12s         Normal    SuccessfulCreate          daemonset/s3-csi-node                                         Created pod: s3-csi-node-nkx5d
kube-system                 9s          Normal    Created                   pod/s3-csi-node-nkx5d                                         Created container: node-driver-registrar
kube-system                 9s          Normal    Pulled                    pod/s3-csi-node-nkx5d                                         Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0" in 3.083s (3.083s including waiting). Image size: 15579380 bytes.
kube-system                 8s          Normal    Pulling                   pod/s3-csi-node-nkx5d                                         Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0"
kube-system                 8s          Normal    Started                   pod/s3-csi-node-nkx5d                                         Started container node-driver-registrar
kube-system                 6s          Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                        Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0" in 2.701s (5.674s including waiting). Image size: 35709312 bytes.
kube-system                 6s          Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                        Started container csi-provisioner
kube-system                 6s          Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                        Created container: csi-provisioner
kube-system                 4s          Normal    Created                   pod/s3-csi-node-nkx5d                                         Created container: liveness-probe
kube-system                 4s          Normal    Started                   pod/s3-csi-node-nkx5d                                         Started container liveness-probe
kube-system                 4s          Normal    Pulled                    pod/s3-csi-node-nkx5d                                         Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0" in 2.149s (4.758s including waiting). Image size: 15557083 bytes.
s3-advanced-patterns-1365   0s          Normal    ExternalProvisioning      persistentvolumeclaim/reclaim-pvc-fb61fbcf                    Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
s3-advanced-patterns-1365   0s          Normal    Provisioning              persistentvolumeclaim/reclaim-pvc-fb61fbcf                    External provisioner is provisioning volume for claim "s3-advanced-patterns-1365/reclaim-pvc-fb61fbcf"
s3-advanced-patterns-3404   0s          Normal    ExternalProvisioning      persistentvolumeclaim/invalid-pvc-07def630                    Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
s3-advanced-patterns-3404   0s          Normal    Provisioning              persistentvolumeclaim/invalid-pvc-07def630                    External provisioner is provisioning volume for claim "s3-advanced-patterns-3404/invalid-pvc-07def630"
s3-advanced-patterns-3404   0s          Warning   ProvisioningFailed        persistentvolumeclaim/invalid-pvc-07def630                    failed to provision volume with StorageClass "invalid-sc-74ed3e93": failed to get name and namespace template from params: either name and namespace for Provisioner secrets specified, Both must be specified
s3-advanced-patterns-5355   0s          Normal    ExternalProvisioning      persistentvolumeclaim/cred-fallback-pvc-no-secrets-58b31fee   Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
s3-advanced-patterns-5355   0s          Normal    Provisioning              persistentvolumeclaim/cred-fallback-pvc-no-secrets-58b31fee   External provisioner is provisioning volume for claim "s3-advanced-patterns-5355/cred-fallback-pvc-no-secrets-58b31fee"
s3-advanced-patterns-5355   0s          Normal    ProvisioningSucceeded     persistentvolumeclaim/cred-fallback-pvc-no-secrets-58b31fee   Successfully provisioned volume pvc-3f9eed65-a160-4b74-847e-d48e91a2d3bd
s3-advanced-patterns-610    0s          Normal    WaitForFirstConsumer      persistentvolumeclaim/wait-pvc-07c7e39e                       waiting for first consumer to be created before binding
volume-1120                 0s          Normal    Provisioning              persistentvolumeclaim/s3.csi.scality.comsr5nc                 External provisioner is provisioning volume for claim "volume-1120/s3.csi.scality.comsr5nc"
volume-1120                 0s          Normal    ExternalProvisioning      persistentvolumeclaim/s3.csi.scality.comsr5nc                 Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
volume-1120                 0s          Normal    ProvisioningSucceeded     persistentvolumeclaim/s3.csi.scality.comsr5nc                 Successfully provisioned volume pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6
volume-3903                 0s          Warning   ProvisioningFailed        persistentvolumeclaim/pvc-xcdzz                               storageclass.storage.k8s.io "volume-3903" not found
