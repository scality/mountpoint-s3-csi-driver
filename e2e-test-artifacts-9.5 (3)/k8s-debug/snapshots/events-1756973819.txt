NAMESPACE                   LAST SEEN   TYPE      REASON                    OBJECT                                                            MESSAGE
default                     102s        Normal    Starting                  node/helm-test-cluster-control-plane                              
kube-system                 115s        Normal    Pulled                    pod/kube-scheduler-helm-test-cluster-control-plane                Container image "registry.k8s.io/kube-scheduler:v1.32.0" already present on machine
default                     115s        Normal    NodeAllocatableEnforced   node/helm-test-cluster-control-plane                              Updated Node Allocatable limit across pods
kube-system                 115s        Normal    Pulled                    pod/etcd-helm-test-cluster-control-plane                          Container image "registry.k8s.io/etcd:3.5.16-0" already present on machine
kube-system                 115s        Normal    Pulled                    pod/kube-apiserver-helm-test-cluster-control-plane                Container image "registry.k8s.io/kube-apiserver:v1.32.0" already present on machine
default                     115s        Normal    NodeHasSufficientPID      node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientPID
default                     115s        Normal    NodeHasNoDiskPressure     node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasNoDiskPressure
kube-system                 115s        Normal    Pulled                    pod/kube-controller-manager-helm-test-cluster-control-plane       Container image "registry.k8s.io/kube-controller-manager:v1.32.0" already present on machine
default                     115s        Normal    NodeHasSufficientMemory   node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientMemory
kube-system                 114s        Normal    Created                   pod/kube-scheduler-helm-test-cluster-control-plane                Created container: kube-scheduler
kube-system                 114s        Normal    Started                   pod/kube-scheduler-helm-test-cluster-control-plane                Started container kube-scheduler
kube-system                 114s        Normal    Created                   pod/kube-controller-manager-helm-test-cluster-control-plane       Created container: kube-controller-manager
kube-system                 114s        Normal    Started                   pod/kube-apiserver-helm-test-cluster-control-plane                Started container kube-apiserver
kube-system                 114s        Normal    Created                   pod/kube-apiserver-helm-test-cluster-control-plane                Created container: kube-apiserver
kube-system                 114s        Normal    Started                   pod/kube-controller-manager-helm-test-cluster-control-plane       Started container kube-controller-manager
kube-system                 113s        Normal    Created                   pod/etcd-helm-test-cluster-control-plane                          Created container: etcd
kube-system                 113s        Normal    Started                   pod/etcd-helm-test-cluster-control-plane                          Started container etcd
default                     109s        Normal    Starting                  node/helm-test-cluster-control-plane                              Starting kubelet.
default                     109s        Normal    NodeAllocatableEnforced   node/helm-test-cluster-control-plane                              Updated Node Allocatable limit across pods
default                     109s        Normal    NodeHasSufficientMemory   node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientMemory
default                     109s        Normal    NodeHasNoDiskPressure     node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasNoDiskPressure
default                     109s        Normal    NodeHasSufficientPID      node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientPID
kube-system                 108s        Normal    LeaderElection            lease/kube-controller-manager                                     helm-test-cluster-control-plane_77f7495e-78c9-4b49-83a4-6cc5e95c32e8 became leader
kube-system                 107s        Normal    LeaderElection            lease/kube-scheduler                                              helm-test-cluster-control-plane_1cdead3f-f1fb-4de6-9e9d-9a54ee794f6a became leader
default                     104s        Normal    RegisteredNode            node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane event: Registered Node helm-test-cluster-control-plane in Controller
kube-system                 103s        Normal    SuccessfulCreate          daemonset/kube-proxy                                              Created pod: kube-proxy-j7rrw
kube-system                 103s        Normal    SuccessfulCreate          daemonset/kindnet                                                 Created pod: kindnet-cx4hx
kube-system                 103s        Normal    Scheduled                 pod/kindnet-cx4hx                                                 Successfully assigned kube-system/kindnet-cx4hx to helm-test-cluster-control-plane
local-path-storage          103s        Normal    ScalingReplicaSet         deployment/local-path-provisioner                                 Scaled up replica set local-path-provisioner-58cc7856b6 from 0 to 1
kube-system                 103s        Normal    ScalingReplicaSet         deployment/coredns                                                Scaled up replica set coredns-668d6bf9bc from 0 to 2
kube-system                 103s        Warning   FailedMount               pod/kindnet-cx4hx                                                 MountVolume.SetUp failed for volume "kube-api-access-wrnqf" : configmap "kube-root-ca.crt" not found
kube-system                 103s        Normal    Scheduled                 pod/kube-proxy-j7rrw                                              Successfully assigned kube-system/kube-proxy-j7rrw to helm-test-cluster-control-plane
kube-system                 103s        Warning   FailedMount               pod/kube-proxy-j7rrw                                              MountVolume.SetUp failed for volume "kube-api-access-6kncc" : configmap "kube-root-ca.crt" not found
kube-system                 102s        Warning   FailedScheduling          pod/coredns-668d6bf9bc-hlq76                                      0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system                 102s        Normal    Pulled                    pod/kindnet-cx4hx                                                 Container image "docker.io/kindest/kindnetd:v20241212-9f82dd49" already present on machine
kube-system                 102s        Normal    SuccessfulCreate          replicaset/coredns-668d6bf9bc                                     Created pod: coredns-668d6bf9bc-fgch8
kube-system                 102s        Normal    SuccessfulCreate          replicaset/coredns-668d6bf9bc                                     Created pod: coredns-668d6bf9bc-hlq76
kube-system                 102s        Normal    Started                   pod/kube-proxy-j7rrw                                              Started container kube-proxy
kube-system                 102s        Normal    Created                   pod/kube-proxy-j7rrw                                              Created container: kube-proxy
kube-system                 102s        Normal    Pulled                    pod/kube-proxy-j7rrw                                              Container image "registry.k8s.io/kube-proxy:v1.32.0" already present on machine
kube-system                 102s        Normal    Created                   pod/kindnet-cx4hx                                                 Created container: kindnet-cni
kube-system                 102s        Warning   FailedScheduling          pod/coredns-668d6bf9bc-fgch8                                      0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
local-path-storage          102s        Warning   FailedScheduling          pod/local-path-provisioner-58cc7856b6-pvzps                       0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
local-path-storage          102s        Normal    SuccessfulCreate          replicaset/local-path-provisioner-58cc7856b6                      Created pod: local-path-provisioner-58cc7856b6-pvzps
kube-system                 101s        Normal    Started                   pod/kindnet-cx4hx                                                 Started container kindnet-cni
default                     91s         Normal    NodeReady                 node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeReady
kube-system                 91s         Normal    Scheduled                 pod/coredns-668d6bf9bc-hlq76                                      Successfully assigned kube-system/coredns-668d6bf9bc-hlq76 to helm-test-cluster-control-plane
local-path-storage          91s         Normal    Scheduled                 pod/local-path-provisioner-58cc7856b6-pvzps                       Successfully assigned local-path-storage/local-path-provisioner-58cc7856b6-pvzps to helm-test-cluster-control-plane
kube-system                 91s         Normal    Scheduled                 pod/coredns-668d6bf9bc-fgch8                                      Successfully assigned kube-system/coredns-668d6bf9bc-fgch8 to helm-test-cluster-control-plane
kube-system                 90s         Normal    Pulled                    pod/coredns-668d6bf9bc-hlq76                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
local-path-storage          90s         Normal    Pulled                    pod/local-path-provisioner-58cc7856b6-pvzps                       Container image "docker.io/kindest/local-path-provisioner:v20241212-8ac705d0" already present on machine
kube-system                 90s         Normal    Pulled                    pod/coredns-668d6bf9bc-fgch8                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
local-path-storage          89s         Normal    Created                   pod/local-path-provisioner-58cc7856b6-pvzps                       Created container: local-path-provisioner
kube-system                 89s         Normal    Started                   pod/coredns-668d6bf9bc-hlq76                                      Started container coredns
kube-system                 89s         Normal    Created                   pod/coredns-668d6bf9bc-hlq76                                      Created container: coredns
local-path-storage          89s         Normal    Started                   pod/local-path-provisioner-58cc7856b6-pvzps                       Started container local-path-provisioner
kube-system                 89s         Normal    Started                   pod/coredns-668d6bf9bc-fgch8                                      Started container coredns
kube-system                 89s         Normal    Created                   pod/coredns-668d6bf9bc-fgch8                                      Created container: coredns
kube-system                 65s         Normal    Pulled                    pod/coredns-6f5f77fcb7-6jtst                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 65s         Normal    SuccessfulDelete          replicaset/coredns-668d6bf9bc                                     Deleted pod: coredns-668d6bf9bc-hlq76
kube-system                 65s         Normal    ScalingReplicaSet         deployment/coredns                                                Scaled up replica set coredns-6f5f77fcb7 from 1 to 2
kube-system                 65s         Normal    ScalingReplicaSet         deployment/coredns                                                Scaled down replica set coredns-668d6bf9bc from 2 to 1
kube-system                 65s         Normal    ScalingReplicaSet         deployment/coredns                                                Scaled up replica set coredns-6f5f77fcb7 from 0 to 1
kube-system                 65s         Normal    SuccessfulCreate          replicaset/coredns-6f5f77fcb7                                     Created pod: coredns-6f5f77fcb7-fdx4j
kube-system                 65s         Normal    SuccessfulCreate          replicaset/coredns-6f5f77fcb7                                     Created pod: coredns-6f5f77fcb7-6jtst
kube-system                 65s         Normal    Killing                   pod/coredns-668d6bf9bc-hlq76                                      Stopping container coredns
kube-system                 65s         Normal    Scheduled                 pod/coredns-6f5f77fcb7-6jtst                                      Successfully assigned kube-system/coredns-6f5f77fcb7-6jtst to helm-test-cluster-control-plane
kube-system                 65s         Normal    Scheduled                 pod/coredns-6f5f77fcb7-fdx4j                                      Successfully assigned kube-system/coredns-6f5f77fcb7-fdx4j to helm-test-cluster-control-plane
kube-system                 64s         Normal    ScalingReplicaSet         deployment/coredns                                                Scaled down replica set coredns-668d6bf9bc from 1 to 0
kube-system                 64s         Normal    Started                   pod/coredns-6f5f77fcb7-6jtst                                      Started container coredns
kube-system                 64s         Normal    Created                   pod/coredns-6f5f77fcb7-6jtst                                      Created container: coredns
default                     64s         Normal    Scheduled                 pod/dns-test                                                      Successfully assigned default/dns-test to helm-test-cluster-control-plane
kube-system                 64s         Normal    Created                   pod/coredns-6f5f77fcb7-fdx4j                                      Created container: coredns
kube-system                 64s         Normal    SuccessfulDelete          replicaset/coredns-668d6bf9bc                                     Deleted pod: coredns-668d6bf9bc-fgch8
kube-system                 64s         Normal    Pulled                    pod/coredns-6f5f77fcb7-fdx4j                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 64s         Normal    Started                   pod/coredns-6f5f77fcb7-fdx4j                                      Started container coredns
kube-system                 64s         Normal    Killing                   pod/coredns-668d6bf9bc-fgch8                                      Stopping container coredns
default                     64s         Normal    Pulling                   pod/dns-test                                                      Pulling image "busybox:1.28"
default                     62s         Normal    Started                   pod/dns-test                                                      Started container dns-test
default                     62s         Normal    Created                   pod/dns-test                                                      Created container: dns-test
default                     62s         Normal    Pulled                    pod/dns-test                                                      Successfully pulled image "busybox:1.28" in 1.886s (1.886s including waiting). Image size: 727869 bytes.
kube-system                 42s         Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Started container s3-csi-controller
kube-system                 42s         Normal    Pulling                   pod/s3-csi-node-nkx5d                                             Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0"
kube-system                 42s         Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                            Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 42s         Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                            Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 42s         Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Created container: s3-pod-reconciler
kube-system                 42s         Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Started container s3-pod-reconciler
kube-system                 42s         Normal    Pulling                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0"
kube-system                 42s         Normal    Scheduled                 pod/s3-csi-controller-84f6bdfd58-7cpmf                            Successfully assigned kube-system/s3-csi-controller-84f6bdfd58-7cpmf to helm-test-cluster-control-plane
kube-system                 42s         Normal    SuccessfulCreate          daemonset/s3-csi-node                                             Created pod: s3-csi-node-nkx5d
kube-system                 42s         Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Created container: s3-csi-controller
kube-system                 42s         Normal    SuccessfulCreate          replicaset/s3-csi-controller-84f6bdfd58                           Created pod: s3-csi-controller-84f6bdfd58-7cpmf
kube-system                 42s         Normal    ScalingReplicaSet         deployment/s3-csi-controller                                      Scaled up replica set s3-csi-controller-84f6bdfd58 from 0 to 1
kube-system                 42s         Normal    Scheduled                 pod/s3-csi-node-nkx5d                                             Successfully assigned kube-system/s3-csi-node-nkx5d to helm-test-cluster-control-plane
kube-system                 42s         Normal    Pulled                    pod/s3-csi-node-nkx5d                                             Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 42s         Normal    Created                   pod/s3-csi-node-nkx5d                                             Created container: s3-plugin
kube-system                 42s         Normal    Started                   pod/s3-csi-node-nkx5d                                             Started container s3-plugin
kube-system                 39s         Normal    Pulled                    pod/s3-csi-node-nkx5d                                             Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0" in 3.083s (3.083s including waiting). Image size: 15579380 bytes.
kube-system                 39s         Normal    Created                   pod/s3-csi-node-nkx5d                                             Created container: node-driver-registrar
kube-system                 38s         Normal    Started                   pod/s3-csi-node-nkx5d                                             Started container node-driver-registrar
kube-system                 38s         Normal    Pulling                   pod/s3-csi-node-nkx5d                                             Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0"
kube-system                 36s         Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Created container: csi-provisioner
kube-system                 36s         Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Started container csi-provisioner
kube-system                 36s         Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                            Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0" in 2.701s (5.674s including waiting). Image size: 35709312 bytes.
kube-system                 34s         Normal    Pulled                    pod/s3-csi-node-nkx5d                                             Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0" in 2.149s (4.758s including waiting). Image size: 15557083 bytes.
kube-system                 34s         Normal    Started                   pod/s3-csi-node-nkx5d                                             Started container liveness-probe
kube-system                 34s         Normal    Created                   pod/s3-csi-node-nkx5d                                             Created container: liveness-probe
volume-3903                 30s         Warning   ProvisioningFailed        persistentvolumeclaim/pvc-xcdzz                                   storageclass.storage.k8s.io "volume-3903" not found
volume-1120                 30s         Normal    ProvisioningSucceeded     persistentvolumeclaim/s3.csi.scality.comsr5nc                     Successfully provisioned volume pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6
volume-1120                 30s         Normal    ExternalProvisioning      persistentvolumeclaim/s3.csi.scality.comsr5nc                     Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
volume-1120                 30s         Normal    Provisioning              persistentvolumeclaim/s3.csi.scality.comsr5nc                     External provisioner is provisioning volume for claim "volume-1120/s3.csi.scality.comsr5nc"
s3-advanced-patterns-5355   30s         Normal    ProvisioningSucceeded     persistentvolumeclaim/cred-fallback-pvc-no-secrets-58b31fee       Successfully provisioned volume pvc-3f9eed65-a160-4b74-847e-d48e91a2d3bd
s3-advanced-patterns-5355   30s         Normal    Provisioning              persistentvolumeclaim/cred-fallback-pvc-no-secrets-58b31fee       External provisioner is provisioning volume for claim "s3-advanced-patterns-5355/cred-fallback-pvc-no-secrets-58b31fee"
s3-advanced-patterns-5355   30s         Normal    ExternalProvisioning      persistentvolumeclaim/cred-fallback-pvc-no-secrets-58b31fee       Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
mount-s3                    28s         Normal    Started                   pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Started container mountpoint
volume-1120                 28s         Normal    Scheduled                 pod/s3-injector                                                   Successfully assigned volume-1120/s3-injector to helm-test-cluster-control-plane
mount-s3                    28s         Normal    Started                   pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Started container mountpoint
cache-6176                  28s         Normal    Scheduled                 pod/pvc-tester-tsdmc                                              Successfully assigned cache-6176/pvc-tester-tsdmc to helm-test-cluster-control-plane
mount-s3                    28s         Normal    Pulled                    pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    28s         Normal    Created                   pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Created container: mountpoint
mount-s3                    28s         Normal    Pulled                    pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    28s         Normal    Scheduled                 pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Successfully assigned mount-s3/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20 to helm-test-cluster-control-plane
mount-s3                    28s         Normal    Pulled                    pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    28s         Normal    Created                   pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Created container: mountpoint
mount-s3                    28s         Normal    Started                   pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Started container mountpoint
volume-3903                 28s         Normal    Scheduled                 pod/s3-injector                                                   Successfully assigned volume-3903/s3-injector to helm-test-cluster-control-plane
cache-2772                  28s         Normal    Scheduled                 pod/pvc-tester-dwhq2                                              Successfully assigned cache-2772/pvc-tester-dwhq2 to helm-test-cluster-control-plane
mount-s3                    28s         Normal    Scheduled                 pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Successfully assigned mount-s3/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50 to helm-test-cluster-control-plane
mount-s3                    28s         Normal    Created                   pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Created container: mountpoint
mount-s3                    28s         Normal    Scheduled                 pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Successfully assigned mount-s3/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281 to helm-test-cluster-control-plane
mount-s3                    28s         Normal    Pulled                    pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    28s         Normal    Created                   pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Created container: mountpoint
mount-s3                    28s         Normal    Started                   pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Started container mountpoint
mount-s3                    28s         Normal    Scheduled                 pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Successfully assigned mount-s3/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6 to helm-test-cluster-control-plane
mount-s3                    25s         Normal    Started                   pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Started container mountpoint
mount-s3                    25s         Normal    Created                   pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Created container: mountpoint
mount-s3                    25s         Normal    Pulled                    pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    25s         Normal    Scheduled                 pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Successfully assigned mount-s3/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73 to helm-test-cluster-control-plane
s3-advanced-patterns-5355   25s         Normal    Scheduled                 pod/cred-fallback-pod-no-secrets-1969e71a                         Successfully assigned s3-advanced-patterns-5355/cred-fallback-pod-no-secrets-1969e71a to helm-test-cluster-control-plane
s3-advanced-patterns-3404   15s         Normal    Provisioning              persistentvolumeclaim/invalid-pvc-07def630                        External provisioner is provisioning volume for claim "s3-advanced-patterns-3404/invalid-pvc-07def630"
s3-advanced-patterns-3404   15s         Warning   ProvisioningFailed        persistentvolumeclaim/invalid-pvc-07def630                        failed to provision volume with StorageClass "invalid-sc-74ed3e93": failed to get name and namespace template from params: either name and namespace for Provisioner secrets specified, Both must be specified
s3-advanced-patterns-610    14s         Normal    WaitForFirstConsumer      persistentvolumeclaim/wait-pvc-07c7e39e                           waiting for first consumer to be created before binding
s3-advanced-patterns-3404   14s         Normal    ExternalProvisioning      persistentvolumeclaim/invalid-pvc-07def630                        Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
cache-6176                  12s         Warning   FailedMount               pod/pvc-tester-tsdmc                                              MountVolume.SetUp failed for volume "s3-e2e-pv-5a1ec614-7174-4db3-997e-1ad5aa6fb41b" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-rzx8ggxnxcj5btdxlf5blhx8sxkzv92hzpbgmxtjs9jw9gxm" at "/var/lib/kubelet/pods/11c6ba65-d874-46e5-8a9f-d48b9ddce169/volumes/kubernetes.io~csi/s3-e2e-pv-5a1ec614-7174-4db3-997e-1ad5aa6fb41b/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/11c6ba65-d874-46e5-8a9f-d48b9ddce169/volumes/kubernetes.io~csi/s3-e2e-pv-5a1ec614-7174-4db3-997e-1ad5aa6fb41b/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
cache-2772                  12s         Warning   FailedMount               pod/pvc-tester-dwhq2                                              MountVolume.SetUp failed for volume "s3-e2e-pv-9889fc2f-c808-4623-b190-7e4bcdeb4f74" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-wckmzrfgk7pn6sb2fbmws44k5n2vmx2tnjpngvnjqnnhbmfq" at "/var/lib/kubelet/pods/43cea5c5-d75d-4b6d-a669-8b9f63a8a19d/volumes/kubernetes.io~csi/s3-e2e-pv-9889fc2f-c808-4623-b190-7e4bcdeb4f74/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/43cea5c5-d75d-4b6d-a669-8b9f63a8a19d/volumes/kubernetes.io~csi/s3-e2e-pv-9889fc2f-c808-4623-b190-7e4bcdeb4f74/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
mount-s3                    12s         Normal    Started                   pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Started container mountpoint
volume-3903                 12s         Warning   FailedMount               pod/s3-injector                                                   MountVolume.SetUp failed for volume "s3.csi.scality.com-jrq4b" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-c6c4dqwfz72d6nmggpnx986p5252nj7vwvf8zfxgxw98dmpw" at "/var/lib/kubelet/pods/4f49525f-bef6-478b-83f9-1673c510a51a/volumes/kubernetes.io~csi/s3.csi.scality.com-jrq4b/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/4f49525f-bef6-478b-83f9-1673c510a51a/volumes/kubernetes.io~csi/s3.csi.scality.com-jrq4b/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
mount-s3                    12s         Normal    Scheduled                 pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Successfully assigned mount-s3/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796 to helm-test-cluster-control-plane
mount-s3                    12s         Normal    Pulled                    pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    12s         Normal    Created                   pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Created container: mountpoint
volume-1120                 12s         Warning   FailedMount               pod/s3-injector                                                   MountVolume.SetUp failed for volume "pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6" : rpc error: code = Internal desc = Could not mount "csi-s3-56fce90b-b62b-4eb0-b3ea-fafb76b9323a" at "/var/lib/kubelet/pods/72939224-f413-49d1-b6b6-bbee9aae3f22/volumes/kubernetes.io~csi/pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/72939224-f413-49d1-b6b6-bbee9aae3f22/volumes/kubernetes.io~csi/pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
cache-9720                  12s         Normal    Scheduled                 pod/pvc-tester-zl2k5                                              Successfully assigned cache-9720/pvc-tester-zl2k5 to helm-test-cluster-control-plane
s3-advanced-patterns-5355   9s          Warning   FailedMount               pod/cred-fallback-pod-no-secrets-1969e71a                         MountVolume.SetUp failed for volume "pvc-3f9eed65-a160-4b74-847e-d48e91a2d3bd" : rpc error: code = Internal desc = Could not mount "csi-s3-c2283e6d-e72a-4116-bdea-aef21dade34e" at "/var/lib/kubelet/pods/fe8bbe5a-c299-4460-a471-67d200642b47/volumes/kubernetes.io~csi/pvc-3f9eed65-a160-4b74-847e-d48e91a2d3bd/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/fe8bbe5a-c299-4460-a471-67d200642b47/volumes/kubernetes.io~csi/pvc-3f9eed65-a160-4b74-847e-d48e91a2d3bd/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
cache-9720                  4s          Warning   FailedMount               pod/pvc-tester-zl2k5                                              MountVolume.SetUp failed for volume "s3-e2e-pv-8166c059-6d8b-4308-a0d6-39f0993d081d" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-cv867fmkkbn4pq4c8gcr9wfdw5sd66ksnnw4wp7wzz226dzb" at "/var/lib/kubelet/pods/b1936d9d-388d-4083-96b4-56bcdf7d3711/volumes/kubernetes.io~csi/s3-e2e-pv-8166c059-6d8b-4308-a0d6-39f0993d081d/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/b1936d9d-388d-4083-96b4-56bcdf7d3711/volumes/kubernetes.io~csi/s3-e2e-pv-8166c059-6d8b-4308-a0d6-39f0993d081d/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
s3-advanced-patterns-610    0s          Normal    ProvisioningSucceeded     persistentvolumeclaim/wait-pvc-07c7e39e                           Successfully provisioned volume pvc-5d062f4f-1813-43a2-8ec0-68213b6a3c21
s3-advanced-patterns-610    0s          Normal    Provisioning              persistentvolumeclaim/wait-pvc-07c7e39e                           External provisioner is provisioning volume for claim "s3-advanced-patterns-610/wait-pvc-07c7e39e"
s3-advanced-patterns-610    0s          Normal    ExternalProvisioning      persistentvolumeclaim/wait-pvc-07c7e39e                           Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
