NAMESPACE                   LAST SEEN   TYPE      REASON                    OBJECT                                                            MESSAGE
default                     2m12s       Normal    Starting                  node/helm-test-cluster-control-plane
default                     2m26s       Normal    NodeHasSufficientMemory   node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientMemory
kube-system                 2m26s       Normal    Pulled                    pod/kube-controller-manager-helm-test-cluster-control-plane       Container image "registry.k8s.io/kube-controller-manager:v1.32.0" already present on machine
default                     2m26s       Normal    NodeAllocatableEnforced   node/helm-test-cluster-control-plane                              Updated Node Allocatable limit across pods
kube-system                 2m26s       Normal    Pulled                    pod/etcd-helm-test-cluster-control-plane                          Container image "registry.k8s.io/etcd:3.5.16-0" already present on machine
default                     2m26s       Normal    NodeHasNoDiskPressure     node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasNoDiskPressure
kube-system                 2m26s       Normal    Pulled                    pod/kube-scheduler-helm-test-cluster-control-plane                Container image "registry.k8s.io/kube-scheduler:v1.32.0" already present on machine
default                     2m26s       Normal    NodeHasSufficientPID      node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientPID
kube-system                 2m26s       Normal    Pulled                    pod/kube-apiserver-helm-test-cluster-control-plane                Container image "registry.k8s.io/kube-apiserver:v1.32.0" already present on machine
kube-system                 2m25s       Normal    Created                   pod/kube-scheduler-helm-test-cluster-control-plane                Created container: kube-scheduler
kube-system                 2m25s       Normal    Started                   pod/kube-scheduler-helm-test-cluster-control-plane                Started container kube-scheduler
kube-system                 2m25s       Normal    Started                   pod/kube-apiserver-helm-test-cluster-control-plane                Started container kube-apiserver
kube-system                 2m25s       Normal    Created                   pod/kube-controller-manager-helm-test-cluster-control-plane       Created container: kube-controller-manager
kube-system                 2m25s       Normal    Started                   pod/kube-controller-manager-helm-test-cluster-control-plane       Started container kube-controller-manager
kube-system                 2m25s       Normal    Created                   pod/kube-apiserver-helm-test-cluster-control-plane                Created container: kube-apiserver
kube-system                 2m24s       Normal    Started                   pod/etcd-helm-test-cluster-control-plane                          Started container etcd
kube-system                 2m24s       Normal    Created                   pod/etcd-helm-test-cluster-control-plane                          Created container: etcd
default                     2m20s       Normal    NodeHasSufficientPID      node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientPID
default                     2m20s       Normal    NodeHasNoDiskPressure     node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasNoDiskPressure
default                     2m20s       Normal    NodeHasSufficientMemory   node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeHasSufficientMemory
default                     2m20s       Normal    Starting                  node/helm-test-cluster-control-plane                              Starting kubelet.
default                     2m20s       Normal    NodeAllocatableEnforced   node/helm-test-cluster-control-plane                              Updated Node Allocatable limit across pods
kube-system                 2m19s       Normal    LeaderElection            lease/kube-controller-manager                                     helm-test-cluster-control-plane_77f7495e-78c9-4b49-83a4-6cc5e95c32e8 became leader
kube-system                 2m18s       Normal    LeaderElection            lease/kube-scheduler                                              helm-test-cluster-control-plane_1cdead3f-f1fb-4de6-9e9d-9a54ee794f6a became leader
default                     2m15s       Normal    RegisteredNode            node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane event: Registered Node helm-test-cluster-control-plane in Controller
kube-system                 2m14s       Normal    Scheduled                 pod/kindnet-cx4hx                                                 Successfully assigned kube-system/kindnet-cx4hx to helm-test-cluster-control-plane
kube-system                 2m14s       Normal    SuccessfulCreate          daemonset/kindnet                                                 Created pod: kindnet-cx4hx
kube-system                 2m14s       Normal    Scheduled                 pod/kube-proxy-j7rrw                                              Successfully assigned kube-system/kube-proxy-j7rrw to helm-test-cluster-control-plane
kube-system                 2m14s       Warning   FailedMount               pod/kube-proxy-j7rrw                                              MountVolume.SetUp failed for volume "kube-api-access-6kncc" : configmap "kube-root-ca.crt" not found
kube-system                 2m14s       Warning   FailedMount               pod/kindnet-cx4hx                                                 MountVolume.SetUp failed for volume "kube-api-access-wrnqf" : configmap "kube-root-ca.crt" not found
kube-system                 2m14s       Normal    SuccessfulCreate          daemonset/kube-proxy                                              Created pod: kube-proxy-j7rrw
local-path-storage          2m14s       Normal    ScalingReplicaSet         deployment/local-path-provisioner                                 Scaled up replica set local-path-provisioner-58cc7856b6 from 0 to 1
kube-system                 2m14s       Normal    ScalingReplicaSet         deployment/coredns                                                Scaled up replica set coredns-668d6bf9bc from 0 to 2
kube-system                 2m13s       Normal    Created                   pod/kube-proxy-j7rrw                                              Created container: kube-proxy
kube-system                 2m13s       Normal    SuccessfulCreate          replicaset/coredns-668d6bf9bc                                     Created pod: coredns-668d6bf9bc-hlq76
kube-system                 2m13s       Normal    SuccessfulCreate          replicaset/coredns-668d6bf9bc                                     Created pod: coredns-668d6bf9bc-fgch8
kube-system                 2m13s       Warning   FailedScheduling          pod/coredns-668d6bf9bc-hlq76                                      0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system                 2m13s       Normal    Created                   pod/kindnet-cx4hx                                                 Created container: kindnet-cni
kube-system                 2m13s       Normal    Pulled                    pod/kindnet-cx4hx                                                 Container image "docker.io/kindest/kindnetd:v20241212-9f82dd49" already present on machine
kube-system                 2m13s       Normal    Pulled                    pod/kube-proxy-j7rrw                                              Container image "registry.k8s.io/kube-proxy:v1.32.0" already present on machine
kube-system                 2m13s       Normal    Started                   pod/kube-proxy-j7rrw                                              Started container kube-proxy
kube-system                 2m13s       Warning   FailedScheduling          pod/coredns-668d6bf9bc-fgch8                                      0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
local-path-storage          2m13s       Warning   FailedScheduling          pod/local-path-provisioner-58cc7856b6-pvzps                       0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
local-path-storage          2m13s       Normal    SuccessfulCreate          replicaset/local-path-provisioner-58cc7856b6                      Created pod: local-path-provisioner-58cc7856b6-pvzps
kube-system                 2m12s       Normal    Started                   pod/kindnet-cx4hx                                                 Started container kindnet-cni
default                     2m2s        Normal    NodeReady                 node/helm-test-cluster-control-plane                              Node helm-test-cluster-control-plane status is now: NodeReady
local-path-storage          2m2s        Normal    Scheduled                 pod/local-path-provisioner-58cc7856b6-pvzps                       Successfully assigned local-path-storage/local-path-provisioner-58cc7856b6-pvzps to helm-test-cluster-control-plane
kube-system                 2m2s        Normal    Scheduled                 pod/coredns-668d6bf9bc-fgch8                                      Successfully assigned kube-system/coredns-668d6bf9bc-fgch8 to helm-test-cluster-control-plane
kube-system                 2m2s        Normal    Scheduled                 pod/coredns-668d6bf9bc-hlq76                                      Successfully assigned kube-system/coredns-668d6bf9bc-hlq76 to helm-test-cluster-control-plane
local-path-storage          2m1s        Normal    Pulled                    pod/local-path-provisioner-58cc7856b6-pvzps                       Container image "docker.io/kindest/local-path-provisioner:v20241212-8ac705d0" already present on machine
kube-system                 2m1s        Normal    Pulled                    pod/coredns-668d6bf9bc-fgch8                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 2m1s        Normal    Pulled                    pod/coredns-668d6bf9bc-hlq76                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 2m          Normal    Started                   pod/coredns-668d6bf9bc-hlq76                                      Started container coredns
local-path-storage          2m          Normal    Created                   pod/local-path-provisioner-58cc7856b6-pvzps                       Created container: local-path-provisioner
local-path-storage          2m          Normal    Started                   pod/local-path-provisioner-58cc7856b6-pvzps                       Started container local-path-provisioner
kube-system                 2m          Normal    Created                   pod/coredns-668d6bf9bc-fgch8                                      Created container: coredns
kube-system                 2m          Normal    Started                   pod/coredns-668d6bf9bc-fgch8                                      Started container coredns
kube-system                 2m          Normal    Created                   pod/coredns-668d6bf9bc-hlq76                                      Created container: coredns
kube-system                 96s         Normal    Killing                   pod/coredns-668d6bf9bc-hlq76                                      Stopping container coredns
kube-system                 96s         Normal    Scheduled                 pod/coredns-6f5f77fcb7-6jtst                                      Successfully assigned kube-system/coredns-6f5f77fcb7-6jtst to helm-test-cluster-control-plane
kube-system                 96s         Normal    ScalingReplicaSet         deployment/coredns                                                Scaled up replica set coredns-6f5f77fcb7 from 0 to 1
kube-system                 96s         Normal    SuccessfulDelete          replicaset/coredns-668d6bf9bc                                     Deleted pod: coredns-668d6bf9bc-hlq76
kube-system                 96s         Normal    SuccessfulCreate          replicaset/coredns-6f5f77fcb7                                     Created pod: coredns-6f5f77fcb7-fdx4j
kube-system                 96s         Normal    Pulled                    pod/coredns-6f5f77fcb7-6jtst                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 96s         Normal    ScalingReplicaSet         deployment/coredns                                                Scaled down replica set coredns-668d6bf9bc from 2 to 1
kube-system                 96s         Normal    ScalingReplicaSet         deployment/coredns                                                Scaled up replica set coredns-6f5f77fcb7 from 1 to 2
kube-system                 96s         Normal    Scheduled                 pod/coredns-6f5f77fcb7-fdx4j                                      Successfully assigned kube-system/coredns-6f5f77fcb7-fdx4j to helm-test-cluster-control-plane
kube-system                 96s         Normal    SuccessfulCreate          replicaset/coredns-6f5f77fcb7                                     Created pod: coredns-6f5f77fcb7-6jtst
kube-system                 95s         Normal    Created                   pod/coredns-6f5f77fcb7-fdx4j                                      Created container: coredns
kube-system                 95s         Normal    Pulled                    pod/coredns-6f5f77fcb7-fdx4j                                      Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system                 95s         Normal    Started                   pod/coredns-6f5f77fcb7-fdx4j                                      Started container coredns
kube-system                 95s         Normal    Killing                   pod/coredns-668d6bf9bc-fgch8                                      Stopping container coredns
kube-system                 95s         Normal    ScalingReplicaSet         deployment/coredns                                                Scaled down replica set coredns-668d6bf9bc from 1 to 0
default                     95s         Normal    Pulling                   pod/dns-test                                                      Pulling image "busybox:1.28"
kube-system                 95s         Normal    Started                   pod/coredns-6f5f77fcb7-6jtst                                      Started container coredns
kube-system                 95s         Normal    Created                   pod/coredns-6f5f77fcb7-6jtst                                      Created container: coredns
kube-system                 95s         Normal    SuccessfulDelete          replicaset/coredns-668d6bf9bc                                     Deleted pod: coredns-668d6bf9bc-fgch8
default                     95s         Normal    Scheduled                 pod/dns-test                                                      Successfully assigned default/dns-test to helm-test-cluster-control-plane
default                     93s         Normal    Started                   pod/dns-test                                                      Started container dns-test
default                     93s         Normal    Created                   pod/dns-test                                                      Created container: dns-test
default                     93s         Normal    Pulled                    pod/dns-test                                                      Successfully pulled image "busybox:1.28" in 1.886s (1.886s including waiting). Image size: 727869 bytes.
kube-system                 73s         Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Started container s3-csi-controller
kube-system                 73s         Normal    Scheduled                 pod/s3-csi-controller-84f6bdfd58-7cpmf                            Successfully assigned kube-system/s3-csi-controller-84f6bdfd58-7cpmf to helm-test-cluster-control-plane
kube-system                 73s         Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Started container s3-pod-reconciler
kube-system                 73s         Normal    Pulling                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0"
kube-system                 73s         Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Created container: s3-pod-reconciler
kube-system                 73s         Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                            Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 73s         Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Created container: s3-csi-controller
kube-system                 73s         Normal    SuccessfulCreate          replicaset/s3-csi-controller-84f6bdfd58                           Created pod: s3-csi-controller-84f6bdfd58-7cpmf
kube-system                 73s         Normal    ScalingReplicaSet         deployment/s3-csi-controller                                      Scaled up replica set s3-csi-controller-84f6bdfd58 from 0 to 1
kube-system                 73s         Normal    Scheduled                 pod/s3-csi-node-nkx5d                                             Successfully assigned kube-system/s3-csi-node-nkx5d to helm-test-cluster-control-plane
kube-system                 73s         Normal    Pulled                    pod/s3-csi-node-nkx5d                                             Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 73s         Normal    Created                   pod/s3-csi-node-nkx5d                                             Created container: s3-plugin
kube-system                 73s         Normal    Started                   pod/s3-csi-node-nkx5d                                             Started container s3-plugin
kube-system                 73s         Normal    Pulling                   pod/s3-csi-node-nkx5d                                             Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0"
kube-system                 73s         Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                            Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
kube-system                 73s         Normal    SuccessfulCreate          daemonset/s3-csi-node                                             Created pod: s3-csi-node-nkx5d
kube-system                 70s         Normal    Created                   pod/s3-csi-node-nkx5d                                             Created container: node-driver-registrar
kube-system                 70s         Normal    Pulled                    pod/s3-csi-node-nkx5d                                             Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0" in 3.083s (3.083s including waiting). Image size: 15579380 bytes.
kube-system                 69s         Normal    Pulling                   pod/s3-csi-node-nkx5d                                             Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0"
kube-system                 69s         Normal    Started                   pod/s3-csi-node-nkx5d                                             Started container node-driver-registrar
kube-system                 67s         Normal    Started                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Started container csi-provisioner
kube-system                 67s         Normal    Pulled                    pod/s3-csi-controller-84f6bdfd58-7cpmf                            Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0" in 2.701s (5.674s including waiting). Image size: 35709312 bytes.
kube-system                 67s         Normal    Created                   pod/s3-csi-controller-84f6bdfd58-7cpmf                            Created container: csi-provisioner
kube-system                 65s         Normal    Started                   pod/s3-csi-node-nkx5d                                             Started container liveness-probe
kube-system                 65s         Normal    Created                   pod/s3-csi-node-nkx5d                                             Created container: liveness-probe
kube-system                 65s         Normal    Pulled                    pod/s3-csi-node-nkx5d                                             Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0" in 2.149s (4.758s including waiting). Image size: 15557083 bytes.
volume-3903                 61s         Warning   ProvisioningFailed        persistentvolumeclaim/pvc-xcdzz                                   storageclass.storage.k8s.io "volume-3903" not found
volume-1120                 61s         Normal    ProvisioningSucceeded     persistentvolumeclaim/s3.csi.scality.comsr5nc                     Successfully provisioned volume pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6
volume-1120                 61s         Normal    ExternalProvisioning      persistentvolumeclaim/s3.csi.scality.comsr5nc                     Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
volume-1120                 61s         Normal    Provisioning              persistentvolumeclaim/s3.csi.scality.comsr5nc                     External provisioner is provisioning volume for claim "volume-1120/s3.csi.scality.comsr5nc"
s3-advanced-patterns-5355   61s         Normal    ProvisioningSucceeded     persistentvolumeclaim/cred-fallback-pvc-no-secrets-58b31fee       Successfully provisioned volume pvc-3f9eed65-a160-4b74-847e-d48e91a2d3bd
s3-advanced-patterns-5355   61s         Normal    Provisioning              persistentvolumeclaim/cred-fallback-pvc-no-secrets-58b31fee       External provisioner is provisioning volume for claim "s3-advanced-patterns-5355/cred-fallback-pvc-no-secrets-58b31fee"
s3-advanced-patterns-5355   61s         Normal    ExternalProvisioning      persistentvolumeclaim/cred-fallback-pvc-no-secrets-58b31fee       Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
mount-s3                    59s         Normal    Pulled                    pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    59s         Normal    Created                   pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Created container: mountpoint
mount-s3                    59s         Normal    Created                   pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Created container: mountpoint
mount-s3                    59s         Normal    Pulled                    pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
volume-3903                 59s         Normal    Scheduled                 pod/s3-injector                                                   Successfully assigned volume-3903/s3-injector to helm-test-cluster-control-plane
volume-1120                 59s         Normal    Scheduled                 pod/s3-injector                                                   Successfully assigned volume-1120/s3-injector to helm-test-cluster-control-plane
cache-6176                  59s         Normal    Scheduled                 pod/pvc-tester-tsdmc                                              Successfully assigned cache-6176/pvc-tester-tsdmc to helm-test-cluster-control-plane
cache-2772                  59s         Normal    Scheduled                 pod/pvc-tester-dwhq2                                              Successfully assigned cache-2772/pvc-tester-dwhq2 to helm-test-cluster-control-plane
mount-s3                    59s         Normal    Scheduled                 pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Successfully assigned mount-s3/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20 to helm-test-cluster-control-plane
mount-s3                    59s         Normal    Scheduled                 pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Successfully assigned mount-s3/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50 to helm-test-cluster-control-plane
mount-s3                    59s         Normal    Created                   pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Created container: mountpoint
mount-s3                    59s         Normal    Started                   pod/mp-7574e21ee3530a31c680a30cea91d393f8f1abf6dd8f9d36d9fada20   Started container mountpoint
mount-s3                    59s         Normal    Started                   pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Started container mountpoint
mount-s3                    59s         Normal    Started                   pod/mp-6094027646566d3f9a94fa4793209d8f89ccb09067362486a6f6dc50   Started container mountpoint
mount-s3                    59s         Normal    Pulled                    pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    59s         Normal    Scheduled                 pod/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6   Successfully assigned mount-s3/mp-afe8f18f9fb99bb5d149ca93619869a0fb9cefe5de1246f31b9eb9c6 to helm-test-cluster-control-plane
mount-s3                    59s         Normal    Scheduled                 pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Successfully assigned mount-s3/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281 to helm-test-cluster-control-plane
mount-s3                    59s         Normal    Pulled                    pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    59s         Normal    Created                   pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Created container: mountpoint
mount-s3                    59s         Normal    Started                   pod/mp-9b6c6c8aeead40f8164a2e67a085886b769d7b44015f99e758aea281   Started container mountpoint
mount-s3                    56s         Normal    Scheduled                 pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Successfully assigned mount-s3/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73 to helm-test-cluster-control-plane
mount-s3                    56s         Normal    Pulled                    pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    56s         Normal    Started                   pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Started container mountpoint
s3-advanced-patterns-5355   56s         Normal    Scheduled                 pod/cred-fallback-pod-no-secrets-1969e71a                         Successfully assigned s3-advanced-patterns-5355/cred-fallback-pod-no-secrets-1969e71a to helm-test-cluster-control-plane
mount-s3                    56s         Normal    Created                   pod/mp-6a85bc6e5df680f35e4431d67874b4ae3e46899cc68459346fab9e73   Created container: mountpoint
s3-advanced-patterns-610    45s         Normal    WaitForFirstConsumer      persistentvolumeclaim/wait-pvc-07c7e39e                           waiting for first consumer to be created before binding
mount-s3                    43s         Normal    Started                   pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Started container mountpoint
mount-s3                    43s         Normal    Created                   pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Created container: mountpoint
mount-s3                    43s         Normal    Pulled                    pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
mount-s3                    43s         Normal    Scheduled                 pod/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796   Successfully assigned mount-s3/mp-882387e76a89724d8a9549f945f963f64f924af00ea7a32eaa6fc796 to helm-test-cluster-control-plane
cache-9720                  43s         Normal    Scheduled                 pod/pvc-tester-zl2k5                                              Successfully assigned cache-9720/pvc-tester-zl2k5 to helm-test-cluster-control-plane
s3-advanced-patterns-610    31s         Normal    ExternalProvisioning      persistentvolumeclaim/wait-pvc-07c7e39e                           Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
s3-advanced-patterns-610    31s         Normal    Provisioning              persistentvolumeclaim/wait-pvc-07c7e39e                           External provisioner is provisioning volume for claim "s3-advanced-patterns-610/wait-pvc-07c7e39e"
s3-advanced-patterns-610    31s         Normal    ProvisioningSucceeded     persistentvolumeclaim/wait-pvc-07c7e39e                           Successfully provisioned volume pvc-5d062f4f-1813-43a2-8ec0-68213b6a3c21
mount-s3                    30s         Normal    Created                   pod/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0   Created container: mountpoint
mount-s3                    30s         Normal    Started                   pod/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0   Started container mountpoint
s3-advanced-patterns-3404   30s         Normal    Provisioning              persistentvolumeclaim/invalid-pvc-07def630                        External provisioner is provisioning volume for claim "s3-advanced-patterns-3404/invalid-pvc-07def630"
mount-s3                    30s         Normal    Scheduled                 pod/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0   Successfully assigned mount-s3/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0 to helm-test-cluster-control-plane
s3-advanced-patterns-610    30s         Normal    Scheduled                 pod/wait-pod-e3a3b9dc                                             Successfully assigned s3-advanced-patterns-610/wait-pod-e3a3b9dc to helm-test-cluster-control-plane
s3-advanced-patterns-3404   30s         Warning   ProvisioningFailed        persistentvolumeclaim/invalid-pvc-07def630                        failed to provision volume with StorageClass "invalid-sc-74ed3e93": failed to get name and namespace template from params: either name and namespace for Provisioner secrets specified, Both must be specified
mount-s3                    30s         Normal    Pulled                    pod/mp-6c4ed57bf013246d0f03369a5d8425dba1897bd3de27aeb6c0ce0bf0   Container image "ghcr.io/scality/mountpoint-s3-csi-driver:ec17ad4f838a7cec5579099a9f46d84d4e47b333" already present on machine
volume-1120                 27s         Warning   FailedMount               pod/s3-injector                                                   MountVolume.SetUp failed for volume "pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6" : rpc error: code = Internal desc = Could not mount "csi-s3-56fce90b-b62b-4eb0-b3ea-fafb76b9323a" at "/var/lib/kubelet/pods/72939224-f413-49d1-b6b6-bbee9aae3f22/volumes/kubernetes.io~csi/pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/72939224-f413-49d1-b6b6-bbee9aae3f22/volumes/kubernetes.io~csi/pvc-ed6ac51f-1a8b-4e00-9132-695ea2fa6bb6/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
cache-2772                  27s         Warning   FailedMount               pod/pvc-tester-dwhq2                                              MountVolume.SetUp failed for volume "s3-e2e-pv-9889fc2f-c808-4623-b190-7e4bcdeb4f74" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-wckmzrfgk7pn6sb2fbmws44k5n2vmx2tnjpngvnjqnnhbmfq" at "/var/lib/kubelet/pods/43cea5c5-d75d-4b6d-a669-8b9f63a8a19d/volumes/kubernetes.io~csi/s3-e2e-pv-9889fc2f-c808-4623-b190-7e4bcdeb4f74/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/43cea5c5-d75d-4b6d-a669-8b9f63a8a19d/volumes/kubernetes.io~csi/s3-e2e-pv-9889fc2f-c808-4623-b190-7e4bcdeb4f74/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
cache-6176                  27s         Warning   FailedMount               pod/pvc-tester-tsdmc                                              MountVolume.SetUp failed for volume "s3-e2e-pv-5a1ec614-7174-4db3-997e-1ad5aa6fb41b" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-rzx8ggxnxcj5btdxlf5blhx8sxkzv92hzpbgmxtjs9jw9gxm" at "/var/lib/kubelet/pods/11c6ba65-d874-46e5-8a9f-d48b9ddce169/volumes/kubernetes.io~csi/s3-e2e-pv-5a1ec614-7174-4db3-997e-1ad5aa6fb41b/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/11c6ba65-d874-46e5-8a9f-d48b9ddce169/volumes/kubernetes.io~csi/s3-e2e-pv-5a1ec614-7174-4db3-997e-1ad5aa6fb41b/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
volume-3903                 27s         Warning   FailedMount               pod/s3-injector                                                   MountVolume.SetUp failed for volume "s3.csi.scality.com-jrq4b" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-c6c4dqwfz72d6nmggpnx986p5252nj7vwvf8zfxgxw98dmpw" at "/var/lib/kubelet/pods/4f49525f-bef6-478b-83f9-1673c510a51a/volumes/kubernetes.io~csi/s3.csi.scality.com-jrq4b/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/4f49525f-bef6-478b-83f9-1673c510a51a/volumes/kubernetes.io~csi/s3.csi.scality.com-jrq4b/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
s3-advanced-patterns-5355   24s         Warning   FailedMount               pod/cred-fallback-pod-no-secrets-1969e71a                         MountVolume.SetUp failed for volume "pvc-3f9eed65-a160-4b74-847e-d48e91a2d3bd" : rpc error: code = Internal desc = Could not mount "csi-s3-c2283e6d-e72a-4116-bdea-aef21dade34e" at "/var/lib/kubelet/pods/fe8bbe5a-c299-4460-a471-67d200642b47/volumes/kubernetes.io~csi/pvc-3f9eed65-a160-4b74-847e-d48e91a2d3bd/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/fe8bbe5a-c299-4460-a471-67d200642b47/volumes/kubernetes.io~csi/pvc-3f9eed65-a160-4b74-847e-d48e91a2d3bd/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
s3-advanced-patterns-3404   15s         Normal    ExternalProvisioning      persistentvolumeclaim/invalid-pvc-07def630                        Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
s3-advanced-patterns-610    14s         Warning   FailedMount               pod/wait-pod-e3a3b9dc                                             MountVolume.SetUp failed for volume "pvc-5d062f4f-1813-43a2-8ec0-68213b6a3c21" : rpc error: code = Internal desc = Could not mount "csi-s3-51eeefbb-2468-4110-9200-2666312be6aa" at "/var/lib/kubelet/pods/1243b6af-14c2-4680-9ec8-239559f6722d/volumes/kubernetes.io~csi/pvc-5d062f4f-1813-43a2-8ec0-68213b6a3c21/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/1243b6af-14c2-4680-9ec8-239559f6722d/volumes/kubernetes.io~csi/pvc-5d062f4f-1813-43a2-8ec0-68213b6a3c21/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
cache-9720                  11s         Warning   FailedMount               pod/pvc-tester-zl2k5                                              MountVolume.SetUp failed for volume "s3-e2e-pv-8166c059-6d8b-4308-a0d6-39f0993d081d" : rpc error: code = Internal desc = Could not mount "s3-csi-k8s-e2e-cv867fmkkbn4pq4c8gcr9wfdw5sd66ksnnw4wp7wzz226dzb" at "/var/lib/kubelet/pods/b1936d9d-388d-4083-96b4-56bcdf7d3711/volumes/kubernetes.io~csi/s3-e2e-pv-8166c059-6d8b-4308-a0d6-39f0993d081d/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/b1936d9d-388d-4083-96b4-56bcdf7d3711/volumes/kubernetes.io~csi/s3-e2e-pv-8166c059-6d8b-4308-a0d6-39f0993d081d/mount": field label not supported: spec.authenticationSource. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
