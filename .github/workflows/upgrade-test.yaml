name: Upgrade Test

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - '**'
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - 'mkdocs.yml'
      - 'requirements.txt'
      - 'NOTICE'
      - '.lychee.toml'
      - '.markdownlint.yaml'
      - "CLAUDE.md"

jobs:
  upgrade-test:
    name: Test v1.2.0 → local upgrade
    runs-on: ubuntu-22.04
    steps:
      - name: Check out repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Login to Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io/scality
          username: "${{ github.repository_owner }}"
          password: "${{ github.token }}"

      - name: Setup Helm
        uses: azure/setup-helm@v4.3.1

      - name: Install Mage
        run: |
          go install github.com/magefile/mage@latest
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH

      - name: Set Environment Variables
        run: |
          HOST_IP=$(hostname -I | awk '{print $1}')
          echo "HOST_IP=$HOST_IP" >> $GITHUB_ENV
          echo "S3_ENDPOINT_URL=http://$HOST_IP:8000" >> $GITHUB_ENV
          echo "KIND_CLUSTER_NAME=upgrade-test-cluster" >> $GITHUB_ENV
          echo "Host IP: $HOST_IP"
          echo "S3 Endpoint: http://$HOST_IP:8000"
          echo "Kind Cluster: upgrade-test-cluster"

      - name: Create Kind Cluster
        uses: helm/kind-action@v1.12.0
        with:
          wait: 90s
          cluster_name: upgrade-test-cluster

      - name: Deploy CloudServer (S3 Backend)
        run: |
          set -e -o pipefail
          mkdir -p logs/s3
          chown -R runner:docker logs
          docker compose --profile s3 up -d --quiet-pull
          bash ../scripts/wait_for_local_port.bash 8000 30
        working-directory: .github/scality-storage-deployment

      - name: Start Kubernetes Event and Log Capture
        run: |
          mkdir -p artifacts/k8s-debug
          ./tests/e2e/scripts/capture-events-and-logs.sh artifacts/k8s-debug start &
          echo $! > capture.pid

      - name: Configure Host DNS for S3 Endpoint
        run: |
          echo "Adding s3.example.com -> $HOST_IP mapping to /etc/hosts for CSI driver compatibility"
          echo "$HOST_IP s3.example.com" | sudo tee -a /etc/hosts
          cat /etc/hosts | grep s3.example.com

      - name: Install CSI Driver v1.2.0 from OCI Registry
        run: SCALITY_CSI_VERSION=1.2.0 mage install

      - name: Show Status of CSI Driver
        run: mage status && mage showS3DNSStatus

      - name: Setup Upgrade Tests (Static + Dynamic Provisioning)
        run: mage setupUpgradeTests

      - name: Upgrade to Local Development Version
        run: VERBOSE=1 mage up

      - name: Show Status of CSI Driver After Upgrade
        run: mage status && mage showS3DNSStatus

      - name: Verify Upgrade Tests (Static + Dynamic Provisioning)
        run: mage verifyUpgradeTests

      - name: Test Pod Mounter Transition After Restart
        run: |
          echo "=== Testing automatic transition from systemd to pod mounter ==="

          # The upgrade test uses standalone pods (not deployments), so we need to recreate them manually
          echo "Getting test pods (standalone pods, not managed by deployment)..."
          kubectl get pods -n default | grep upgrade-test || true

          # Check if any mountpoint pods exist (v1.2.0 uses systemd, so there shouldn't be any)
          echo "Checking for existing mountpoint pods (should be none with systemd mounter)..."
          kubectl get pods -n mount-s3 2>/dev/null || echo "No mountpoint pods namespace yet (expected for systemd)"

          # Store the manifests we'll need to recreate the pods
          STATIC_POD_MANIFEST="tests/upgrade/manifests/static-pod.yaml"
          DYNAMIC_POD_MANIFEST="tests/upgrade/manifests/dynamic-pod.yaml"

          # Delete the standalone test pods to trigger transition to pod mounter
          echo "Deleting standalone test pods..."
          kubectl delete pod upgrade-test-pod -n default --wait=true --ignore-not-found=true
          kubectl delete pod upgrade-test-dynamic-pod -n default --wait=true --ignore-not-found=true

          # Recreate the pods (they will now use the pod mounter instead of systemd)
          echo "Recreating test pods (will use pod mounter now)..."
          if [ -f "$STATIC_POD_MANIFEST" ]; then
            echo "Recreating static test pod..."
            kubectl apply -f "$STATIC_POD_MANIFEST"
          fi

          if [ -f "$DYNAMIC_POD_MANIFEST" ]; then
            echo "Recreating dynamic test pod..."
            kubectl apply -f "$DYNAMIC_POD_MANIFEST"
          fi

          # Wait for pods to come up and be ready
          echo "Waiting for recreated pods to become ready..."
          kubectl wait --for=condition=Ready pod/upgrade-test-pod -n default --timeout=120s || true
          kubectl wait --for=condition=Ready pod/upgrade-test-dynamic-pod -n default --timeout=120s || true

          # Show pod status
          echo "Test pods after recreation:"
          kubectl get pods -n default | grep upgrade-test || true

          # Now check for mountpoint pods (should exist after restart with v2)
          echo "Checking for mountpoint pods after restart (should exist with pod mounter)..."
          kubectl get pods -n mount-s3 -o wide

          # Verify mountpoint pods are running
          MOUNTPOINT_PODS=$(kubectl get pods -n mount-s3 --no-headers 2>/dev/null | wc -l)
          if [ "$MOUNTPOINT_PODS" -gt 0 ]; then
            echo "✓ Found $MOUNTPOINT_PODS mountpoint pod(s) - pod mounter is active"
            kubectl get pods -n mount-s3 -o yaml | grep -E "mountpoint-version|mounted-by-csi-driver-version" | head -10
          else
            echo "✗ No mountpoint pods found - pod mounter may not be working"
            exit 1
          fi

          # Test that we can still write to the volumes
          echo "Testing write access to volumes after mounter transition..."

          # Test static provisioning pod
          if kubectl get pod upgrade-test-pod -n default &>/dev/null; then
            echo "Testing static provisioning pod..."

            # CRITICAL: Verify data written before upgrade still exists
            echo "Checking for data persistence (before-upgrade.txt should exist)..."
            if kubectl exec -n default upgrade-test-pod -- test -f /data/before-upgrade.txt; then
              echo "✓ Pre-upgrade data file exists"
              kubectl exec -n default upgrade-test-pod -- cat /data/before-upgrade.txt
            else
              echo "✗ FAILURE: Pre-upgrade data lost after pod recreation!"
              exit 1
            fi

            # Write a new test file to verify mount is working
            kubectl exec -n default upgrade-test-pod -- sh -c "echo 'Pod mounter transition test - static' > /data/transition-test.txt"

            # Read it back
            echo "Reading back new test file from static pod..."
            kubectl exec -n default upgrade-test-pod -- cat /data/transition-test.txt

            # List all files to verify
            echo "All files in static pod mount:"
            kubectl exec -n default upgrade-test-pod -- ls -la /data/
          fi

          # Test dynamic provisioning pod
          if kubectl get pod upgrade-test-dynamic-pod -n default &>/dev/null; then
            echo "Testing dynamic provisioning pod..."

            # CRITICAL: Verify data written before upgrade still exists
            echo "Checking for data persistence (before-upgrade.txt should exist)..."
            if kubectl exec -n default upgrade-test-dynamic-pod -- test -f /data/before-upgrade.txt; then
              echo "✓ Pre-upgrade data file exists"
              kubectl exec -n default upgrade-test-dynamic-pod -- cat /data/before-upgrade.txt
            else
              echo "✗ FAILURE: Pre-upgrade data lost after pod recreation!"
              exit 1
            fi

            # Write a test file to verify mount is working
            kubectl exec -n default upgrade-test-dynamic-pod -- sh -c "echo 'Pod mounter transition test - dynamic' > /data/transition-test-dynamic.txt"

            # Read it back
            echo "Reading back test file from dynamic pod..."
            kubectl exec -n default upgrade-test-dynamic-pod -- cat /data/transition-test-dynamic.txt

            # List files to verify (should include before-upgrade.txt from original test)
            echo "Files in dynamic pod mount:"
            kubectl exec -n default upgrade-test-dynamic-pod -- ls -la /data/
          fi

          echo "✓ Pod mounter transition test completed successfully"

      - name: Stop K8s Event Capture and Generate Debug Report
        if: always()
        run: |
          # Stop the capture process
          if [ -f capture.pid ]; then
            ./tests/e2e/scripts/capture-events-and-logs.sh artifacts/k8s-debug stop || true
            rm -f capture.pid
          fi

          # Compress K8s debug data
          tar -czf artifacts/k8s-debug-capture.tar.gz -C artifacts k8s-debug/ || true

      - name: Copy S3 logs to artifacts directory
        if: always()
        run: |
          mkdir -p artifacts/logs/s3
          cp -r .github/scality-storage-deployment/logs/s3/* artifacts/logs/s3/ 2>/dev/null || true

      - name: Upload Test Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: v1-to-latest-upgrade-test-artifacts
          path: artifacts

      - name: Cleanup Test Resources
        if: always()
        run: mage cleanupUpgradeTests || true

      - name: Cleanup
        if: always()
        run: mage down
