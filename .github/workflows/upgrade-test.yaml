name: Upgrade Test

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - '**'
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - 'mkdocs.yml'
      - 'requirements.txt'
      - 'NOTICE'
      - '.lychee.toml'
      - '.markdownlint.yaml'
      - "CLAUDE.md"

jobs:
  upgrade-test:
    name: Test v1.2.0 → local upgrade
    runs-on: ubuntu-22.04
    steps:
      - name: Check out repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Login to Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io/scality
          username: "${{ github.repository_owner }}"
          password: "${{ github.token }}"

      - name: Setup Helm
        uses: azure/setup-helm@v4.3.1

      - name: Install Mage
        run: |
          go install github.com/magefile/mage@latest
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH

      - name: Set Environment Variables
        run: |
          HOST_IP=$(hostname -I | awk '{print $1}')
          echo "HOST_IP=$HOST_IP" >> $GITHUB_ENV
          echo "S3_ENDPOINT_URL=http://$HOST_IP:8000" >> $GITHUB_ENV
          echo "KIND_CLUSTER_NAME=upgrade-test-cluster" >> $GITHUB_ENV
          echo "Host IP: $HOST_IP"
          echo "S3 Endpoint: http://$HOST_IP:8000"
          echo "Kind Cluster: upgrade-test-cluster"

      - name: Create Kind Cluster
        uses: helm/kind-action@v1.12.0
        with:
          wait: 90s
          cluster_name: upgrade-test-cluster

      - name: Deploy CloudServer (S3 Backend)
        run: |
          set -e -o pipefail
          mkdir -p logs/s3
          chown -R runner:docker logs
          docker compose --profile s3 up -d --quiet-pull
          bash ../scripts/wait_for_local_port.bash 8000 30
        working-directory: .github/scality-storage-deployment

      - name: Start Kubernetes Event and Log Capture
        run: |
          mkdir -p artifacts/k8s-debug
          ./tests/e2e/scripts/capture-events-and-logs.sh artifacts/k8s-debug start &
          echo $! > capture.pid

      - name: Configure Host DNS for S3 Endpoint
        run: |
          echo "Adding s3.example.com -> $HOST_IP mapping to /etc/hosts for CSI driver compatibility"
          echo "$HOST_IP s3.example.com" | sudo tee -a /etc/hosts
          cat /etc/hosts | grep s3.example.com

      - name: Install CSI Driver v1.2.0 from OCI Registry
        run: SCALITY_CSI_VERSION=1.2.0 mage install

      - name: Show Status of CSI Driver
        run: mage status && mage showS3DNSStatus

      - name: Setup Upgrade Tests (Static + Dynamic Provisioning)
        run: mage setupUpgradeTests

      - name: Upgrade to Local Development Version
        run: VERBOSE=1 mage up

      - name: Show Status of CSI Driver After Upgrade
        run: mage status && mage showS3DNSStatus

      - name: Verify Upgrade Tests (Static + Dynamic Provisioning)
        run: mage verifyUpgradeTests

      - name: Test Pod Mounter Transition After Restart
        run: |
          echo "=== Testing automatic transition from systemd to pod mounter ==="
          
          # Get test pods before restart
          echo "Getting test pods before restart..."
          kubectl get pods -n default -l app=s3-test-app -o wide
          
          # Check if any mountpoint pods exist (v1.2.0 uses systemd, so there shouldn't be any)
          echo "Checking for existing mountpoint pods (should be none with systemd mounter)..."
          kubectl get pods -n mount-s3 2>/dev/null || echo "No mountpoint pods namespace yet (expected for systemd)"
          
          # Restart the test pods to trigger transition to pod mounter
          echo "Restarting test pods to trigger mounter transition..."
          kubectl delete pods -n default -l app=s3-test-app --wait=true
          
          # Wait for pods to come back up
          echo "Waiting for pods to restart..."
          kubectl wait --for=condition=Ready pods -n default -l app=s3-test-app --timeout=120s
          
          # Now check for mountpoint pods (should exist after restart with v2)
          echo "Checking for mountpoint pods after restart (should exist with pod mounter)..."
          kubectl get pods -n mount-s3 -o wide
          
          # Verify mountpoint pods are running
          MOUNTPOINT_PODS=$(kubectl get pods -n mount-s3 --no-headers 2>/dev/null | wc -l)
          if [ "$MOUNTPOINT_PODS" -gt 0 ]; then
            echo "✓ Found $MOUNTPOINT_PODS mountpoint pod(s) - pod mounter is active"
            kubectl get pods -n mount-s3 -o yaml | grep -E "mountpoint-version|mounted-by-csi-driver-version" | head -10
          else
            echo "✗ No mountpoint pods found - pod mounter may not be working"
            exit 1
          fi
          
          # Test that we can still write to the volumes
          echo "Testing write access to volumes after mounter transition..."
          
          # Get a test pod name
          TEST_POD=$(kubectl get pods -n default -l app=s3-test-app -o jsonpath='{.items[0].metadata.name}')
          
          # Write a test file to verify mount is working
          echo "Writing test file to verify mount functionality..."
          kubectl exec -n default $TEST_POD -- sh -c "echo 'Pod mounter transition test' > /data/transition-test.txt"
          
          # Read it back
          echo "Reading back test file..."
          kubectl exec -n default $TEST_POD -- cat /data/transition-test.txt
          
          # List files to verify
          echo "Listing files in mounted volume..."
          kubectl exec -n default $TEST_POD -- ls -la /data/
          
          echo "✓ Pod mounter transition test completed successfully"

      - name: Stop K8s Event Capture and Generate Debug Report
        if: always()
        run: |
          # Stop the capture process
          if [ -f capture.pid ]; then
            ./tests/e2e/scripts/capture-events-and-logs.sh artifacts/k8s-debug stop || true
            rm -f capture.pid
          fi

          # Compress K8s debug data
          tar -czf artifacts/k8s-debug-capture.tar.gz -C artifacts k8s-debug/ || true

      - name: Copy S3 logs to artifacts directory
        if: always()
        run: |
          mkdir -p artifacts/logs/s3
          cp -r .github/scality-storage-deployment/logs/s3/* artifacts/logs/s3/ 2>/dev/null || true

      - name: Upload Test Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: v1-to-latest-upgrade-test-artifacts
          path: artifacts

      - name: Cleanup Test Resources
        if: always()
        run: mage cleanupUpgradeTests || true

      - name: Cleanup
        if: always()
        run: mage down
