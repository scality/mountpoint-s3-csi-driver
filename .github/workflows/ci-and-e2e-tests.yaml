name: CI & E2E Tests

on:
  push:
    branches:
      - '**'

env:
  CLOUDSERVER_IMAGE: ${{ vars.CLOUDSERVER_IMAGE }}
  VAULT_IMAGE: ${{ vars.VAULT_IMAGE }}
  KUBECONFIG: "/home/runner/.kube/config"
  S3_ENDPOINT: "http://$(hostname -I | awk '{print $1}'):8000"
  AWS_ACCESS_KEY_ID: "accessKey1"
  AWS_SECRET_ACCESS_KEY: "verySecretKey1"

jobs:
  dev-image:
    name: Dev Image
    permissions:
      contents: read
      packages: write
    uses: scality/workflows/.github/workflows/docker-build.yaml@v2
    with:
      context: .
      name: mountpoint-s3-csi-driver
      namespace: ${{ github.repository_owner }}
      tag: ${{ github.sha }}

  controller-e2e-tests:
    name: E2E Controller Tests
    runs-on: ubuntu-22.04
    needs: dev-image
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Run Common Setup
        uses: ./.github/actions/e2e-setup-common
        with:
          ref: ${{ github.sha }}

      - name: Run Controller Tests
        run: make e2e-controller


  systemd-mounter-e2e-tests:
    name: E2E Systemd Mounter Tests
    runs-on: ubuntu-22.04
    needs: dev-image
    steps:
      - name: Check out repository
        uses: actions/checkout@v4
      
      - name: Run Common Setup
        uses: ./.github/actions/e2e-setup-common
        with:
          ref: ${{ github.sha }}

      - name: Install helm release
        run: |
          helm upgrade --install mountpoint-s3-csi-driver --namespace kube-system ./charts/scality-mountpoint-s3-csi-driver --values \
            ./charts/scality-mountpoint-s3-csi-driver/values.yaml \
            --set image.repository=ghcr.io/scality/mountpoint-s3-csi-driver \
            --set image.tag=${{ github.sha }} \
            --set node.serviceAccount.create=true \
            --set node.podInfoOnMountCompat.enable=true \
            --set experimental.podMounter=systemd
          kubectl rollout status daemonset s3-csi-node -n kube-system --timeout=60s
          kubectl get pods -A
          echo "s3-csi-node-image: $(kubectl get daemonset s3-csi-node -n kube-system -o jsonpath="{$.spec.template.spec.containers[:1].image}")"

      - name: Setup IAM and S3 Services
        run: |-
          set -e -o pipefail;
          mkdir -p logs/s3 logs/iam logs/cosi_driver data/vaultdb
          chown -R runner:docker logs data
          chmod -R ugo+rwx logs data
          docker compose --profile iam_s3 up -d --quiet-pull
          bash ../scripts/wait_for_local_port.bash 8600 30
          bash ../scripts/wait_for_local_port.bash 8000 30
        working-directory: .github/s3_and_iam_deployment

      - name: Run E2E Tests (SystemdMounter)
        run: |
          set +e
          pushd tests/e2e-kubernetes
          KUBECONFIG=/home/runner/.kube/config ginkgo -p -vv -timeout 60m -- --bucket-region=us-east-1 --commit-id=${{ github.sha }} --bucket-prefix=helm-test-cluster
          EXIT_CODE=$?
          print_cluster_info
          popd
          cat tests/e2e-kubernetes/csi-test-artifacts/output.json
          exit $EXIT_CODE
