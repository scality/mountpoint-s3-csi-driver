NAMESPACE            LAST SEEN   TYPE      REASON                    OBJECT                                                           MESSAGE
default              9m49s       Normal    Starting                  node/upgrade-test-cluster-control-plane                          
default              10m         Normal    NodeHasSufficientMemory   node/upgrade-test-cluster-control-plane                          Node upgrade-test-cluster-control-plane status is now: NodeHasSufficientMemory
default              10m         Normal    NodeHasNoDiskPressure     node/upgrade-test-cluster-control-plane                          Node upgrade-test-cluster-control-plane status is now: NodeHasNoDiskPressure
default              10m         Normal    NodeHasSufficientPID      node/upgrade-test-cluster-control-plane                          Node upgrade-test-cluster-control-plane status is now: NodeHasSufficientPID
kube-system          10m         Normal    Pulled                    pod/kube-controller-manager-upgrade-test-cluster-control-plane   Container image "registry.k8s.io/kube-controller-manager:v1.32.0" already present on machine
kube-system          10m         Normal    Pulled                    pod/kube-scheduler-upgrade-test-cluster-control-plane            Container image "registry.k8s.io/kube-scheduler:v1.32.0" already present on machine
kube-system          10m         Normal    Pulled                    pod/etcd-upgrade-test-cluster-control-plane                      Container image "registry.k8s.io/etcd:3.5.16-0" already present on machine
kube-system          10m         Normal    Created                   pod/kube-scheduler-upgrade-test-cluster-control-plane            Created container: kube-scheduler
kube-system          10m         Normal    Started                   pod/kube-scheduler-upgrade-test-cluster-control-plane            Started container kube-scheduler
kube-system          10m         Normal    Started                   pod/kube-controller-manager-upgrade-test-cluster-control-plane   Started container kube-controller-manager
kube-system          10m         Normal    Created                   pod/kube-controller-manager-upgrade-test-cluster-control-plane   Created container: kube-controller-manager
kube-system          10m         Normal    Started                   pod/kube-apiserver-upgrade-test-cluster-control-plane            Started container kube-apiserver
kube-system          10m         Normal    Created                   pod/kube-apiserver-upgrade-test-cluster-control-plane            Created container: kube-apiserver
kube-system          10m         Normal    Pulled                    pod/kube-apiserver-upgrade-test-cluster-control-plane            Container image "registry.k8s.io/kube-apiserver:v1.32.0" already present on machine
kube-system          10m         Normal    Started                   pod/etcd-upgrade-test-cluster-control-plane                      Started container etcd
kube-system          10m         Normal    Created                   pod/etcd-upgrade-test-cluster-control-plane                      Created container: etcd
kube-system          9m56s       Normal    LeaderElection            lease/kube-controller-manager                                    upgrade-test-cluster-control-plane_37d68d4d-f39d-4b71-8cd4-b680581712e7 became leader
kube-system          9m55s       Warning   Unhealthy                 pod/kube-scheduler-upgrade-test-cluster-control-plane            Readiness probe failed: HTTP probe failed with statuscode: 500
default              9m55s       Normal    NodeHasSufficientMemory   node/upgrade-test-cluster-control-plane                          Node upgrade-test-cluster-control-plane status is now: NodeHasSufficientMemory
default              9m55s       Normal    NodeHasNoDiskPressure     node/upgrade-test-cluster-control-plane                          Node upgrade-test-cluster-control-plane status is now: NodeHasNoDiskPressure
default              9m55s       Normal    NodeHasSufficientPID      node/upgrade-test-cluster-control-plane                          Node upgrade-test-cluster-control-plane status is now: NodeHasSufficientPID
default              9m55s       Normal    NodeAllocatableEnforced   node/upgrade-test-cluster-control-plane                          Updated Node Allocatable limit across pods
default              9m55s       Normal    Starting                  node/upgrade-test-cluster-control-plane                          Starting kubelet.
kube-system          9m54s       Normal    LeaderElection            lease/kube-scheduler                                             upgrade-test-cluster-control-plane_68d3a523-290b-4140-a0dd-928cf4227243 became leader
kube-system          9m51s       Normal    ScalingReplicaSet         deployment/coredns                                               Scaled up replica set coredns-668d6bf9bc from 0 to 2
default              9m51s       Normal    RegisteredNode            node/upgrade-test-cluster-control-plane                          Node upgrade-test-cluster-control-plane event: Registered Node upgrade-test-cluster-control-plane in Controller
local-path-storage   9m51s       Normal    ScalingReplicaSet         deployment/local-path-provisioner                                Scaled up replica set local-path-provisioner-58cc7856b6 from 0 to 1
kube-system          9m50s       Normal    SuccessfulCreate          replicaset/coredns-668d6bf9bc                                    Created pod: coredns-668d6bf9bc-hjn2b
kube-system          9m50s       Normal    Pulled                    pod/kindnet-4sf8z                                                Container image "docker.io/kindest/kindnetd:v20241212-9f82dd49" already present on machine
kube-system          9m50s       Normal    Created                   pod/kube-proxy-9jhzh                                             Created container: kube-proxy
kube-system          9m50s       Normal    Pulled                    pod/kube-proxy-9jhzh                                             Container image "registry.k8s.io/kube-proxy:v1.32.0" already present on machine
kube-system          9m50s       Normal    Scheduled                 pod/kube-proxy-9jhzh                                             Successfully assigned kube-system/kube-proxy-9jhzh to upgrade-test-cluster-control-plane
local-path-storage   9m50s       Normal    SuccessfulCreate          replicaset/local-path-provisioner-58cc7856b6                     Created pod: local-path-provisioner-58cc7856b6-7rksv
local-path-storage   9m50s       Warning   FailedScheduling          pod/local-path-provisioner-58cc7856b6-7rksv                      0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system          9m50s       Normal    SuccessfulCreate          daemonset/kube-proxy                                             Created pod: kube-proxy-9jhzh
kube-system          9m50s       Warning   FailedScheduling          pod/coredns-668d6bf9bc-wcm4n                                     0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system          9m50s       Warning   FailedScheduling          pod/coredns-668d6bf9bc-hjn2b                                     0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
kube-system          9m50s       Normal    Scheduled                 pod/kindnet-4sf8z                                                Successfully assigned kube-system/kindnet-4sf8z to upgrade-test-cluster-control-plane
kube-system          9m50s       Normal    SuccessfulCreate          daemonset/kindnet                                                Created pod: kindnet-4sf8z
kube-system          9m50s       Normal    SuccessfulCreate          replicaset/coredns-668d6bf9bc                                    Created pod: coredns-668d6bf9bc-wcm4n
kube-system          9m49s       Normal    Created                   pod/kindnet-4sf8z                                                Created container: kindnet-cni
kube-system          9m49s       Normal    Started                   pod/kube-proxy-9jhzh                                             Started container kube-proxy
kube-system          9m48s       Normal    Started                   pod/kindnet-4sf8z                                                Started container kindnet-cni
default              9m38s       Normal    NodeReady                 node/upgrade-test-cluster-control-plane                          Node upgrade-test-cluster-control-plane status is now: NodeReady
kube-system          9m37s       Normal    Scheduled                 pod/coredns-668d6bf9bc-hjn2b                                     Successfully assigned kube-system/coredns-668d6bf9bc-hjn2b to upgrade-test-cluster-control-plane
kube-system          9m37s       Normal    Scheduled                 pod/coredns-668d6bf9bc-wcm4n                                     Successfully assigned kube-system/coredns-668d6bf9bc-wcm4n to upgrade-test-cluster-control-plane
local-path-storage   9m37s       Normal    Pulled                    pod/local-path-provisioner-58cc7856b6-7rksv                      Container image "docker.io/kindest/local-path-provisioner:v20241212-8ac705d0" already present on machine
kube-system          9m37s       Normal    Pulled                    pod/coredns-668d6bf9bc-wcm4n                                     Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
local-path-storage   9m37s       Normal    Scheduled                 pod/local-path-provisioner-58cc7856b6-7rksv                      Successfully assigned local-path-storage/local-path-provisioner-58cc7856b6-7rksv to upgrade-test-cluster-control-plane
kube-system          9m37s       Normal    Pulled                    pod/coredns-668d6bf9bc-hjn2b                                     Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system          9m36s       Normal    Started                   pod/coredns-668d6bf9bc-hjn2b                                     Started container coredns
kube-system          9m36s       Normal    Created                   pod/coredns-668d6bf9bc-wcm4n                                     Created container: coredns
kube-system          9m36s       Normal    Created                   pod/coredns-668d6bf9bc-hjn2b                                     Created container: coredns
local-path-storage   9m36s       Normal    Created                   pod/local-path-provisioner-58cc7856b6-7rksv                      Created container: local-path-provisioner
local-path-storage   9m36s       Normal    Started                   pod/local-path-provisioner-58cc7856b6-7rksv                      Started container local-path-provisioner
kube-system          9m36s       Normal    Started                   pod/coredns-668d6bf9bc-wcm4n                                     Started container coredns
kube-system          8m35s       Normal    SuccessfulDelete          replicaset/coredns-668d6bf9bc                                    Deleted pod: coredns-668d6bf9bc-wcm4n
kube-system          8m35s       Normal    SuccessfulCreate          replicaset/coredns-7bf54f7fcd                                    Created pod: coredns-7bf54f7fcd-psthl
kube-system          8m35s       Normal    Scheduled                 pod/coredns-7bf54f7fcd-psthl                                     Successfully assigned kube-system/coredns-7bf54f7fcd-psthl to upgrade-test-cluster-control-plane
kube-system          8m35s       Normal    ScalingReplicaSet         deployment/coredns                                               Scaled down replica set coredns-668d6bf9bc from 2 to 1
kube-system          8m35s       Normal    ScalingReplicaSet         deployment/coredns                                               Scaled up replica set coredns-7bf54f7fcd from 1 to 2
kube-system          8m35s       Normal    ScalingReplicaSet         deployment/coredns                                               Scaled up replica set coredns-7bf54f7fcd from 0 to 1
kube-system          8m35s       Normal    Killing                   pod/coredns-668d6bf9bc-wcm4n                                     Stopping container coredns
kube-system          8m34s       Normal    Started                   pod/coredns-7bf54f7fcd-psthl                                     Started container coredns
kube-system          8m34s       Normal    Created                   pod/coredns-7bf54f7fcd-psthl                                     Created container: coredns
kube-system          8m34s       Normal    ScalingReplicaSet         deployment/coredns                                               Scaled down replica set coredns-668d6bf9bc from 1 to 0
kube-system          8m34s       Normal    Pulled                    pod/coredns-7bf54f7fcd-psthl                                     Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system          8m34s       Normal    Created                   pod/coredns-7bf54f7fcd-2llkl                                     Created container: coredns
kube-system          8m34s       Normal    SuccessfulDelete          replicaset/coredns-668d6bf9bc                                    Deleted pod: coredns-668d6bf9bc-hjn2b
kube-system          8m34s       Normal    Pulled                    pod/coredns-7bf54f7fcd-2llkl                                     Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system          8m34s       Normal    Killing                   pod/coredns-668d6bf9bc-hjn2b                                     Stopping container coredns
kube-system          8m34s       Normal    Started                   pod/coredns-7bf54f7fcd-2llkl                                     Started container coredns
kube-system          8m34s       Normal    Scheduled                 pod/coredns-7bf54f7fcd-2llkl                                     Successfully assigned kube-system/coredns-7bf54f7fcd-2llkl to upgrade-test-cluster-control-plane
kube-system          8m34s       Normal    SuccessfulCreate          replicaset/coredns-7bf54f7fcd                                    Created pod: coredns-7bf54f7fcd-2llkl
default              8m33s       Normal    SuccessfulCreate          replicaset/s3-csi-controller-557bcd8dcd                          Created pod: s3-csi-controller-557bcd8dcd-v9sqv
default              8m33s       Normal    Pulling                   pod/s3-csi-controller-557bcd8dcd-v9sqv                           Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver:1.2.0"
default              8m33s       Normal    Pulling                   pod/s3-csi-node-j5n6f                                            Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver:1.2.0"
default              8m33s       Normal    ScalingReplicaSet         deployment/s3-csi-controller                                     Scaled up replica set s3-csi-controller-557bcd8dcd from 0 to 1
default              8m33s       Normal    Scheduled                 pod/s3-csi-node-j5n6f                                            Successfully assigned default/s3-csi-node-j5n6f to upgrade-test-cluster-control-plane
default              8m33s       Normal    SuccessfulCreate          daemonset/s3-csi-node                                            Created pod: s3-csi-node-j5n6f
default              8m33s       Normal    Scheduled                 pod/s3-csi-controller-557bcd8dcd-v9sqv                           Successfully assigned default/s3-csi-controller-557bcd8dcd-v9sqv to upgrade-test-cluster-control-plane
default              8m28s       Normal    Pulling                   pod/s3-csi-controller-557bcd8dcd-v9sqv                           Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0"
default              8m28s       Normal    Created                   pod/s3-csi-controller-557bcd8dcd-v9sqv                           Created container: s3-csi-controller
default              8m28s       Normal    Pulled                    pod/s3-csi-controller-557bcd8dcd-v9sqv                           Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver:1.2.0" in 4.466s (4.466s including waiting). Image size: 164897674 bytes.
default              8m28s       Normal    Started                   pod/s3-csi-node-j5n6f                                            Started container install-mountpoint
default              8m28s       Normal    Started                   pod/s3-csi-controller-557bcd8dcd-v9sqv                           Started container s3-csi-controller
default              8m28s       Normal    Pulled                    pod/s3-csi-node-j5n6f                                            Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver:1.2.0" in 219ms (4.668s including waiting). Image size: 164897674 bytes.
default              8m28s       Normal    Created                   pod/s3-csi-node-j5n6f                                            Created container: install-mountpoint
default              8m27s       Normal    Pulling                   pod/s3-csi-node-j5n6f                                            Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0"
default              8m27s       Normal    Pulled                    pod/s3-csi-node-j5n6f                                            Container image "ghcr.io/scality/mountpoint-s3-csi-driver:1.2.0" already present on machine
default              8m27s       Normal    Created                   pod/s3-csi-node-j5n6f                                            Created container: s3-plugin
default              8m27s       Normal    Started                   pod/s3-csi-node-j5n6f                                            Started container s3-plugin
default              8m26s       Normal    Pulled                    pod/s3-csi-controller-557bcd8dcd-v9sqv                           Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0" in 1.591s (1.715s including waiting). Image size: 35709312 bytes.
default              8m26s       Normal    Started                   pod/s3-csi-controller-557bcd8dcd-v9sqv                           Started container csi-provisioner
default              8m26s       Normal    Created                   pod/s3-csi-controller-557bcd8dcd-v9sqv                           Created container: csi-provisioner
default              8m25s       Normal    Created                   pod/s3-csi-node-j5n6f                                            Created container: node-driver-registrar
default              8m25s       Normal    Pulled                    pod/s3-csi-node-j5n6f                                            Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0" in 1.152s (1.545s including waiting). Image size: 15579380 bytes.
default              8m25s       Normal    Started                   pod/s3-csi-node-j5n6f                                            Started container node-driver-registrar
default              8m25s       Normal    Pulling                   pod/s3-csi-node-j5n6f                                            Pulling image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0"
default              8m24s       Normal    Created                   pod/s3-csi-node-j5n6f                                            Created container: liveness-probe
default              8m24s       Normal    Pulled                    pod/s3-csi-node-j5n6f                                            Successfully pulled image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0" in 920ms (920ms including waiting). Image size: 15557083 bytes.
default              8m24s       Normal    Started                   pod/s3-csi-node-j5n6f                                            Started container liveness-probe
default              8m23s       Normal    Pulling                   pod/dns-show-test                                                Pulling image "busybox:1.36"
default              8m23s       Normal    Scheduled                 pod/dns-show-test                                                Successfully assigned default/dns-show-test to upgrade-test-cluster-control-plane
default              8m22s       Normal    Pulled                    pod/dns-show-test                                                Successfully pulled image "busybox:1.36" in 924ms (924ms including waiting). Image size: 2220000 bytes.
default              8m22s       Normal    Created                   pod/dns-show-test                                                Created container: dns-show-test
default              8m22s       Normal    Started                   pod/dns-show-test                                                Started container dns-show-test
default              8m20s       Normal    Scheduled                 pod/s3-test                                                      Successfully assigned default/s3-test to upgrade-test-cluster-control-plane
default              8m19s       Normal    Pulling                   pod/s3-test                                                      Pulling image "curlimages/curl:latest"
default              8m18s       Normal    Started                   pod/s3-test                                                      Started container s3-test
default              8m18s       Normal    Pulled                    pod/s3-test                                                      Successfully pulled image "curlimages/curl:latest" in 969ms (969ms including waiting). Image size: 9853525 bytes.
default              8m18s       Normal    Created                   pod/s3-test                                                      Created container: s3-test
default              8m16s       Normal    Scheduled                 pod/upgrade-test-pod                                             Successfully assigned default/upgrade-test-pod to upgrade-test-cluster-control-plane
default              8m15s       Normal    Pulling                   pod/upgrade-test-pod                                             Pulling image "busybox:latest"
default              8m14s       Normal    Started                   pod/upgrade-test-pod                                             Started container test
default              8m14s       Normal    Created                   pod/upgrade-test-pod                                             Created container: test
default              8m14s       Normal    Pulled                    pod/upgrade-test-pod                                             Successfully pulled image "busybox:latest" in 950ms (950ms including waiting). Image size: 2223685 bytes.
default              8m13s       Normal    Scheduled                 pod/upgrade-test-dynamic-pod                                     Successfully assigned default/upgrade-test-dynamic-pod to upgrade-test-cluster-control-plane
default              8m13s       Normal    Pulling                   pod/upgrade-test-dynamic-pod                                     Pulling image "busybox:latest"
default              8m13s       Normal    Pulled                    pod/upgrade-test-dynamic-pod                                     Successfully pulled image "busybox:latest" in 231ms (231ms including waiting). Image size: 2223685 bytes.
default              8m13s       Normal    Created                   pod/upgrade-test-dynamic-pod                                     Created container: test
default              8m13s       Normal    ExternalProvisioning      persistentvolumeclaim/upgrade-test-dynamic-pvc                   Waiting for a volume to be created either by the external provisioner 's3.csi.scality.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
default              8m13s       Normal    Provisioning              persistentvolumeclaim/upgrade-test-dynamic-pvc                   External provisioner is provisioning volume for claim "default/upgrade-test-dynamic-pvc"
default              8m13s       Normal    ProvisioningSucceeded     persistentvolumeclaim/upgrade-test-dynamic-pvc                   Successfully provisioned volume pvc-f1c2b928-8a77-4050-a83c-47f9c93ed077
default              8m12s       Normal    Started                   pod/upgrade-test-dynamic-pod                                     Started container test
kube-system          3m28s       Normal    ScalingReplicaSet         deployment/coredns                                               Scaled up replica set coredns-86f89cc758 from 0 to 1
kube-system          3m27s       Normal    Pulled                    pod/coredns-86f89cc758-bs58n                                     Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system          3m27s       Normal    SuccessfulDelete          replicaset/coredns-7bf54f7fcd                                    Deleted pod: coredns-7bf54f7fcd-psthl
kube-system          3m27s       Normal    ScalingReplicaSet         deployment/coredns                                               Scaled up replica set coredns-86f89cc758 from 1 to 2
kube-system          3m27s       Normal    ScalingReplicaSet         deployment/coredns                                               Scaled down replica set coredns-7bf54f7fcd from 2 to 1
kube-system          3m27s       Normal    SuccessfulCreate          replicaset/coredns-86f89cc758                                    Created pod: coredns-86f89cc758-ztmcp
kube-system          3m27s       Normal    SuccessfulCreate          replicaset/coredns-86f89cc758                                    Created pod: coredns-86f89cc758-bs58n
kube-system          3m27s       Normal    Started                   pod/coredns-86f89cc758-ztmcp                                     Started container coredns
kube-system          3m27s       Normal    Created                   pod/coredns-86f89cc758-ztmcp                                     Created container: coredns
kube-system          3m27s       Normal    Pulled                    pod/coredns-86f89cc758-ztmcp                                     Container image "registry.k8s.io/coredns/coredns:v1.11.3" already present on machine
kube-system          3m27s       Normal    Scheduled                 pod/coredns-86f89cc758-ztmcp                                     Successfully assigned kube-system/coredns-86f89cc758-ztmcp to upgrade-test-cluster-control-plane
kube-system          3m27s       Normal    Started                   pod/coredns-86f89cc758-bs58n                                     Started container coredns
kube-system          3m27s       Normal    Created                   pod/coredns-86f89cc758-bs58n                                     Created container: coredns
kube-system          3m27s       Normal    Killing                   pod/coredns-7bf54f7fcd-psthl                                     Stopping container coredns
kube-system          3m27s       Normal    Scheduled                 pod/coredns-86f89cc758-bs58n                                     Successfully assigned kube-system/coredns-86f89cc758-bs58n to upgrade-test-cluster-control-plane
kube-system          3m26s       Normal    SuccessfulDelete          replicaset/coredns-7bf54f7fcd                                    Deleted pod: coredns-7bf54f7fcd-2llkl
kube-system          3m26s       Normal    ScalingReplicaSet         deployment/coredns                                               Scaled down replica set coredns-7bf54f7fcd from 1 to 0
kube-system          3m26s       Normal    Killing                   pod/coredns-7bf54f7fcd-2llkl                                     Stopping container coredns
default              3m25s       Normal    Created                   pod/s3-csi-controller-5659dd8fcf-bv94v                           Created container: s3-csi-controller
default              3m25s       Normal    SuccessfulCreate          daemonset/s3-csi-node                                            Created pod: s3-csi-node-8x5jl
default              3m25s       Normal    Started                   pod/s3-csi-controller-5659dd8fcf-bv94v                           Started container s3-csi-controller
default              3m25s       Normal    SuccessfulCreate          replicaset/s3-csi-controller-5659dd8fcf                          Created pod: s3-csi-controller-5659dd8fcf-bv94v
default              3m25s       Normal    ScalingReplicaSet         deployment/s3-csi-controller                                     Scaled up replica set s3-csi-controller-5659dd8fcf from 0 to 1
default              3m25s       Normal    Scheduled                 pod/s3-csi-node-8x5jl                                            Successfully assigned default/s3-csi-node-8x5jl to upgrade-test-cluster-control-plane
default              3m25s       Normal    Killing                   pod/s3-csi-node-j5n6f                                            Stopping container s3-plugin
default              3m25s       Normal    Pulled                    pod/s3-csi-controller-5659dd8fcf-bv94v                           Container image "ghcr.io/scality/mountpoint-s3-csi-driver:local" already present on machine
default              3m25s       Normal    Scheduled                 pod/s3-csi-controller-5659dd8fcf-bv94v                           Successfully assigned default/s3-csi-controller-5659dd8fcf-bv94v to upgrade-test-cluster-control-plane
default              3m25s       Normal    Killing                   pod/s3-csi-node-j5n6f                                            Stopping container liveness-probe
default              3m25s       Normal    Killing                   pod/s3-csi-node-j5n6f                                            Stopping container node-driver-registrar
default              3m25s       Normal    SuccessfulDelete          daemonset/s3-csi-node                                            Deleted pod: s3-csi-node-j5n6f
default              3m24s       Normal    Pulled                    pod/s3-csi-controller-5659dd8fcf-bv94v                           Container image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-provisioner:v5.3.0" already present on machine
default              3m24s       Normal    ScalingReplicaSet         deployment/s3-csi-controller                                     Scaled down replica set s3-csi-controller-557bcd8dcd from 1 to 0
default              3m24s       Normal    Killing                   pod/s3-csi-controller-557bcd8dcd-v9sqv                           Stopping container csi-provisioner
default              3m24s       Normal    Killing                   pod/s3-csi-controller-557bcd8dcd-v9sqv                           Stopping container s3-csi-controller
default              3m24s       Normal    SuccessfulDelete          replicaset/s3-csi-controller-557bcd8dcd                          Deleted pod: s3-csi-controller-557bcd8dcd-v9sqv
kube-system          3m24s       Warning   Unhealthy                 pod/coredns-7bf54f7fcd-psthl                                     Readiness probe failed: Get "http://10.244.0.5:8181/ready": dial tcp 10.244.0.5:8181: connect: connection refused
default              3m24s       Normal    Created                   pod/s3-csi-controller-5659dd8fcf-bv94v                           Created container: csi-provisioner
kube-system          3m24s       Warning   Unhealthy                 pod/coredns-7bf54f7fcd-2llkl                                     Readiness probe failed: Get "http://10.244.0.6:8181/ready": dial tcp 10.244.0.6:8181: connect: connection refused
default              3m24s       Normal    Started                   pod/s3-csi-controller-5659dd8fcf-bv94v                           Started container csi-provisioner
default              3m24s       Normal    Started                   pod/s3-csi-node-8x5jl                                            Started container liveness-probe
default              3m24s       Normal    Created                   pod/s3-csi-node-8x5jl                                            Created container: liveness-probe
default              3m24s       Normal    Pulled                    pod/s3-csi-node-8x5jl                                            Container image "ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.16.0" already present on machine
default              3m24s       Normal    Started                   pod/s3-csi-node-8x5jl                                            Started container node-driver-registrar
default              3m24s       Normal    Created                   pod/s3-csi-node-8x5jl                                            Created container: node-driver-registrar
default              3m24s       Normal    Pulled                    pod/s3-csi-node-8x5jl                                            Container image "ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0" already present on machine
default              3m24s       Normal    Started                   pod/s3-csi-node-8x5jl                                            Started container s3-plugin
default              3m24s       Normal    Created                   pod/s3-csi-node-8x5jl                                            Created container: s3-plugin
default              3m24s       Normal    Pulled                    pod/s3-csi-node-8x5jl                                            Container image "ghcr.io/scality/mountpoint-s3-csi-driver:local" already present on machine
default              3m8s        Warning   FailedMount               pod/upgrade-test-pod                                             MountVolume.SetUp failed for volume "upgrade-test-pv" : rpc error: code = Internal desc = Could not mount "upgrade-test-static" at "/var/lib/kubelet/pods/25a2aead-29b9-4ad3-9a78-b48b897bc0e7/volumes/kubernetes.io~csi/upgrade-test-pv/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/25a2aead-29b9-4ad3-9a78-b48b897bc0e7/volumes/kubernetes.io~csi/upgrade-test-pv/mount": unable to retrieve the complete list of server APIs: s3.csi.scality.com/v2: no matches for s3.csi.scality.com/v2, Resource=. You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
default              82s         Normal    Created                   pod/dns-show-test                                                Created container: dns-show-test
default              82s         Normal    Started                   pod/dns-show-test                                                Started container dns-show-test
default              82s         Normal    Scheduled                 pod/dns-show-test                                                Successfully assigned default/dns-show-test to upgrade-test-cluster-control-plane
default              82s         Normal    Pulled                    pod/dns-show-test                                                Container image "busybox:1.36" already present on machine
default              80s         Normal    Scheduled                 pod/s3-test                                                      Successfully assigned default/s3-test to upgrade-test-cluster-control-plane
default              79s         Normal    Pulling                   pod/s3-test                                                      Pulling image "curlimages/curl:latest"
default              79s         Normal    Created                   pod/s3-test                                                      Created container: s3-test
default              79s         Normal    Started                   pod/s3-test                                                      Started container s3-test
default              79s         Normal    Pulled                    pod/s3-test                                                      Successfully pulled image "curlimages/curl:latest" in 265ms (265ms including waiting). Image size: 9853525 bytes.
default              76s         Normal    Killing                   pod/upgrade-test-pod                                             Stopping container test
default              60s         Warning   FailedMount               pod/upgrade-test-pod                                             MountVolume.SetUp failed for volume "upgrade-test-pv" : rpc error: code = Internal desc = Could not mount "upgrade-test-static" at "/var/lib/kubelet/pods/25a2aead-29b9-4ad3-9a78-b48b897bc0e7/volumes/kubernetes.io~csi/upgrade-test-pv/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/25a2aead-29b9-4ad3-9a78-b48b897bc0e7/volumes/kubernetes.io~csi/upgrade-test-pv/mount": no matches for kind "MountpointS3PodAttachment" in version "s3.csi.scality.com/v2". You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
default              55s         Warning   FailedMount               pod/upgrade-test-dynamic-pod                                     MountVolume.SetUp failed for volume "pvc-f1c2b928-8a77-4050-a83c-47f9c93ed077" : rpc error: code = Internal desc = Could not mount "csi-s3-211863c7-81e1-46ce-bdff-e221af5d962a" at "/var/lib/kubelet/pods/64236a3f-f3ca-4db4-bda7-476e10084905/volumes/kubernetes.io~csi/pvc-f1c2b928-8a77-4050-a83c-47f9c93ed077/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/64236a3f-f3ca-4db4-bda7-476e10084905/volumes/kubernetes.io~csi/pvc-f1c2b928-8a77-4050-a83c-47f9c93ed077/mount": no matches for kind "MountpointS3PodAttachment" in version "s3.csi.scality.com/v2". You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
default              46s         Normal    Killing                   pod/upgrade-test-dynamic-pod                                     Stopping container test
default              24s         Normal    Pulled                    pod/s3-csi-controller-5659dd8fcf-bv94v                           Container image "ghcr.io/scality/mountpoint-s3-csi-driver:local" already present on machine
default              24s         Normal    Created                   pod/s3-csi-controller-5659dd8fcf-bv94v                           Created container: s3-pod-reconciler
default              24s         Normal    Started                   pod/s3-csi-controller-5659dd8fcf-bv94v                           Started container s3-pod-reconciler
default              15s         Normal    Scheduled                 pod/upgrade-test-dynamic-pod                                     Successfully assigned default/upgrade-test-dynamic-pod to upgrade-test-cluster-control-plane
default              15s         Normal    Scheduled                 pod/upgrade-test-pod                                             Successfully assigned default/upgrade-test-pod to upgrade-test-cluster-control-plane
default              10s         Warning   BackOff                   pod/s3-csi-controller-5659dd8fcf-bv94v                           Back-off restarting failed container s3-pod-reconciler in pod s3-csi-controller-5659dd8fcf-bv94v_default(d0a44ea9-0a0e-48f2-90b0-16deada2fca8)
default              7s          Warning   FailedMount               pod/upgrade-test-pod                                             MountVolume.SetUp failed for volume "upgrade-test-pv" : rpc error: code = Internal desc = Could not mount "upgrade-test-static" at "/var/lib/kubelet/pods/d845e34a-01a4-434d-9b55-8659eaeee207/volumes/kubernetes.io~csi/upgrade-test-pv/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/d845e34a-01a4-434d-9b55-8659eaeee207/volumes/kubernetes.io~csi/upgrade-test-pv/mount": no matches for kind "MountpointS3PodAttachment" in version "s3.csi.scality.com/v2". You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
default              7s          Warning   FailedMount               pod/upgrade-test-dynamic-pod                                     MountVolume.SetUp failed for volume "pvc-f1c2b928-8a77-4050-a83c-47f9c93ed077" : rpc error: code = Internal desc = Could not mount "csi-s3-211863c7-81e1-46ce-bdff-e221af5d962a" at "/var/lib/kubelet/pods/5c69bd42-d7fe-4ccb-a8e6-dbc89ad63b23/volumes/kubernetes.io~csi/pvc-f1c2b928-8a77-4050-a83c-47f9c93ed077/mount": failed to wait for MountpointS3PodAttachment for "/var/lib/kubelet/pods/5c69bd42-d7fe-4ccb-a8e6-dbc89ad63b23/volumes/kubernetes.io~csi/pvc-f1c2b928-8a77-4050-a83c-47f9c93ed077/mount": no matches for kind "MountpointS3PodAttachment" in version "s3.csi.scality.com/v2". You can see the controller logs by running `kubectl logs -n kube-system -lapp=s3-csi-controller`.
