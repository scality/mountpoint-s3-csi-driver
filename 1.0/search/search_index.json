{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scality Container Storage Interface Driver for S3 Documentation","text":"<p>The Scality Container Storage Interface (CSI) Driver for S3allows Kubernetes applications to access Scality S3 objects through a file system interface. This driver is a fork of the Mountpoint for Amazon S3 CSI Driver. It has been engineered and optimized specifically for use with Scality S3-compatible storage solutions.</p> <p>Scality CSI driver presents an S3 bucket as a storage volume accessible by containers in Kubernetes clusters using Mountpoint for Amazon S3. It implements the CSI specification for container orchestrators to manage storage volumes.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Static Provisioning Only: Integrate existing S3 buckets as persistent storage in Kubernetes. Dynamic provisioning is not supported.</li> <li>Familiar File Access: Access S3 objects as files and directories, simplifying application integration.</li> <li>Customizable Mounts: Fine-tune volume mounts with a variety of supported options for performance and behavior.</li> <li>Scality Integration: Optimized for Scality S3 storage solutions like Scality RING.</li> </ul>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"Topic Description Documentation Driver Deployment Prerequisites Kubernetes cluster, RING storage, credentials, and network requirements before installation Prerequisites Quick\u00a0Start Three commands to install the driver and mount a test bucket Quick\u00a0Start\u00a0Guide Installation\u00a0Guide Step\u2011by\u2011step Helm install with custom values, upgrades, and rollbacks Installation\u00a0Guide Uninstallation Safely remove driver pods, CRDs, and secrets from the cluster Uninstallation\u00a0Guide"},{"location":"#container-images","title":"Container Images","text":"<p>Container images for the Scality CSI Driver for S3 are hosted on GHCR:</p> Driver Version Image URL 1.0.1 <code>ghcr.io/scality/mountpoint-s3-csi-driver:1.0.1</code> <p>Note: Please check the releases page for the latest available versions.</p>"},{"location":"#support-and-community","title":"Support and Community","text":"<p>For issues follow the Troubleshooting Guide or contact Scality Support</p>"},{"location":"admin-user-guide/","title":"Administrator and User Guide","text":"<p>This guide defines roles and responsibilities for administrators managing the Scality CSI Driver for S3 and users consuming S3 storage in Kubernetes.</p>"},{"location":"admin-user-guide/#administrator-responsibilities","title":"Administrator Responsibilities","text":"<ul> <li>Install and configure the driver (Helm, global settings, upgrades)</li> <li>Manage network connectivity to S3</li> <li>Create PersistentVolumes for S3 buckets</li> <li>Set mount options and manage credentials</li> <li>Rotate credentials, implement least-privilege, monitor logs</li> </ul>"},{"location":"admin-user-guide/#user-responsibilities","title":"User Responsibilities","text":"<ul> <li>Create PVCs referencing provided PVs</li> <li>Mount volumes in pods</li> <li>Understand S3 consistency and error handling</li> <li>Follow naming conventions and data management best practices</li> </ul>"},{"location":"admin-user-guide/#communication-workflows","title":"Communication Workflows","text":"<ol> <li>User requests storage (bucket, access, requirements)</li> <li>Admin reviews, creates bucket/PV, provides PV name</li> <li>User creates PVC and deploys application</li> </ol>"},{"location":"glossary/","title":"Glossary","text":"<p>This glossary defines acronyms, technical terms, and concepts used throughout the Scality CSI Driver for S3 documentation.</p>"},{"location":"glossary/#acronyms-and-abbreviations","title":"Acronyms and Abbreviations","text":"Acronym Full Form Definition API Application Programming Interface A set of protocols and tools for building software applications CLI Command Line Interface A text-based interface for interacting with software CRD Custom Resource Definition Kubernetes extension mechanism for defining custom resources CRT Common Runtime AWS Common Runtime library used for S3 operations CSI Container Storage Interface A standard for exposing storage systems to containerized workloads DNS Domain Name System System that translates domain names to IP addresses GID Group Identifier Numeric identifier for a group in Unix-like systems GHCR GitHub Container Registry GitHub's container image registry service HTTP Hypertext Transfer Protocol Protocol for transferring data over the web HTTPS HTTP Secure Secure version of HTTP using encryption IAM Identity and Access Management System for managing user identities and permissions JSON JavaScript Object Notation Lightweight data interchange format KMS Key Management Service Service for managing encryption keys POSIX Portable Operating System Interface Set of standards for Unix-like operating systems PV PersistentVolume Kubernetes resource representing a piece of storage PVC PersistentVolumeClaim Kubernetes resource requesting storage from a PV RBAC Role-Based Access Control Method of restricting access based on user roles S3 Simple Storage Service Object storage service protocol SDK Software Development Kit Collection of tools for developing applications SSE Server-Side Encryption Encryption of data at rest on the server TTL Time To Live Duration for which data is considered valid UID User Identifier Numeric identifier for a user in Unix-like systems URL Uniform Resource Locator Web address identifying a resource YAML YAML Ain't Markup Language Human-readable data serialization standard"},{"location":"glossary/#technical-terms","title":"Technical Terms","text":""},{"location":"glossary/#container-and-kubernetes-terms","title":"Container and Kubernetes Terms","text":"Term Definition ClusterRole Kubernetes resource defining permissions across the entire cluster ClusterRoleBinding Kubernetes resource binding a ClusterRole to users or service accounts ConfigMap Kubernetes resource for storing configuration data DaemonSet Kubernetes workload that runs one pod per node Deployment Kubernetes workload for managing stateless applications Helm Package manager for Kubernetes applications initContainer Container that runs before main containers in a pod Kubelet Kubernetes agent running on each node Namespace Kubernetes mechanism for isolating resources Secret Kubernetes resource for storing sensitive data ServiceAccount Kubernetes identity for pods and processes sidecar Additional container running alongside the main container StatefulSet Kubernetes workload for managing stateful applications"},{"location":"glossary/#storage-and-file-system-terms","title":"Storage and File System Terms","text":"Term Definition fsync System call to synchronize file data to storage Mount Options Parameters controlling how a file system is mounted Mount Point Directory where a file system is attached Static Provisioning Manual creation of storage resources subPath Kubernetes feature for mounting a subdirectory of a volume volumeHandle Unique identifier for a CSI volume"},{"location":"glossary/#s3-and-storage-terms","title":"S3 and Storage Terms","text":"Term Definition Access Key ID Public identifier for S3 authentication Bucket Container for objects in S3 storage Bucket Policy JSON document defining access permissions for an S3 bucket Endpoint URL where S3 API requests are sent Mountpoint for Amazon S3 Tool for mounting S3 buckets as file systems Object Basic unit of data stored in S3 Prefix String used to filter objects in an S3 bucket RING Scality's distributed storage platform S3-compatible Storage systems that implement the S3 API Secret Access Key Private key for S3 authentication Session Token Temporary credential for S3 access"},{"location":"glossary/#scality-specific-terms","title":"Scality-Specific Terms","text":"Term Definition Scality RING Scality's software-defined storage platform Scality CSI Driver for S3 Container Storage Interface driver for Scality S3 storage"},{"location":"glossary/#operational-terms","title":"Operational Terms","text":"Term Definition Caching Storing frequently accessed data locally for faster access Consistency Guarantee about the state of data across different operations Metadata Data that describes other data (file attributes, timestamps, etc.) Node Selector Kubernetes mechanism for constraining pods to specific nodes Taints and Tolerations Kubernetes mechanism for controlling pod scheduling Troubleshooting Process of diagnosing and resolving problems"},{"location":"glossary/#common-mount-options","title":"Common Mount Options","text":"Option Description <code>allow-delete</code> Allows deletion of files and objects <code>allow-other</code> Allows other users to access the mounted file system <code>allow-overwrite</code> Allows overwriting existing files <code>cache</code> Enables local caching of file data <code>gid</code> Sets the group ID for file ownership <code>metadata-ttl</code> Sets cache duration for file metadata <code>prefix</code> Limits access to objects with a specific prefix <code>uid</code> Sets the user ID for file ownership"},{"location":"glossary/#error-messages-and-status-codes","title":"Error Messages and Status Codes","text":"Status/Error Meaning <code>ContainerCreating</code> Pod is being created but containers haven't started <code>Running</code> Pod and containers are running successfully <code>Terminating</code> Pod is being shut down <code>Access Denied</code> Insufficient permissions for the requested operation <code>Transport endpoint not connected</code> Network connectivity issue to S3 endpoint"},{"location":"release-notes/","title":"Release Notes","text":""},{"location":"release-notes/#101","title":"1.0.1","text":"<p>August 5, 2025</p>"},{"location":"release-notes/#whats-changed","title":"What's Changed","text":"<ul> <li>Documentation update: Renamed from \"Scality S3 CSI Driver\" to \"Scality CSI Driver for S3\" to comply with AWS copyright policies.</li> <li>No functional changes to the service or driver behavior.</li> </ul>"},{"location":"release-notes/#100","title":"1.0.0","text":"<p>June 13, 2025</p>"},{"location":"release-notes/#whats-new","title":"What's New","text":"<ul> <li>General Availability (GA) release of the Scality\u202fS3\u202fCSI\u202fDriver.</li> <li>Production\u2011ready Helm chart for production deployment.</li> <li>Static provisioning allows seamless integration of existing S3 buckets as Kubernetes PersistentVolumes.</li> <li>Flexible credential strategies: driver\u2011level credentials and per\u2011volume credentials managed with Kubernetes\u202fSecrets.</li> <li>Optimized for Scality\u202fRING with advanced configuration options.</li> <li>Documentation site: https://scality.github.io/mountpoint-s3-csi-driver/.</li> </ul>"},{"location":"release-notes/#current-limitations","title":"Current Limitations","text":"<ul> <li>Static provisioning only; dynamic provisioning is planned for a future release.</li> <li>Optimized for sequential writes, random writes and file modifications follow S3 semantics.</li> <li>Single\u2011writer semantics per object ensure data consistency.</li> <li>Symbolic and hard links are not supported due to S3 limitations.</li> <li>Empty directories require at least one object in order to persist in S3.</li> <li>SELinux enforcing mode on the Kubernetes host is not yet supported.</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>This guide helps diagnose and resolve common issues with the Scality CSI Driver for S3.</p>"},{"location":"troubleshooting/#quick-diagnostics","title":"Quick Diagnostics","text":""},{"location":"troubleshooting/#1-check-driver-health","title":"1. Check Driver Health","text":"<pre><code># Check driver pods status\nkubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver\n\n# View driver logs\nkubectl logs -n ${NAMESPACE} -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver -c s3-plugin --tail=50\n</code></pre>"},{"location":"troubleshooting/#2-check-s3-connectivity","title":"2. Check S3 Connectivity","text":"<pre><code># Test endpoint connectivity\ncurl -I https://your-s3-endpoint.com\n\n# Test S3 access with AWS CLI\naws s3 ls s3://your-bucket --endpoint-url https://your-s3-endpoint.com\n</code></pre>"},{"location":"troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"troubleshooting/#pod-issues","title":"Pod Issues","text":"Symptom Cause Solution Pod stuck in <code>ContainerCreating</code> Mount operation failed 1. Check driver logs2. Check S3 credentials3. Check mount options4. Ensure unique <code>volumeHandle</code> Pod stuck in <code>Terminating</code> Mount point busy or corrupted 1. Force delete pod: <code>kubectl delete pod &lt;name&gt; --force</code>2. Check for <code>subPath</code> issues (see below) Pod fails with \"Permission denied\" Missing mount permissions Add <code>allow-other</code> to PV <code>mountOptions</code> Pod cannot write/delete files Missing write permissions Add <code>allow-delete</code> and/or <code>allow-overwrite</code> to PV <code>mountOptions</code>"},{"location":"troubleshooting/#mount-issues","title":"Mount Issues","text":"Error Message Cause Solution \"Transport endpoint not connected\" S3 endpoint unreachable 1. Check network connectivity2. Check endpoint URL configuration3. Check security groups/firewall rules \"Failed to create mount process\" Mountpoint binary issue 1. Check initContainer logs2. Check <code>/opt/mountpoint-s3-csi/bin/mount-s3</code> exists on node \"Access Denied\" Invalid S3 credentials 1. Check secret contains <code>access_key_id</code> and <code>secret_access_key</code>2. Test credentials with AWS CLI3. Check bucket policy \"InvalidBucketName\" Bucket name issue 1. Check bucket exists2. Check bucket name format3. Ensure no typos \"AWS_ENDPOINT_URL environment variable must be set\" Missing endpoint configuration Set <code>s3EndpointUrl</code> in Helm values or driver configuration"},{"location":"troubleshooting/#volume-issues","title":"Volume Issues","text":"Issue Description Solution Multiple volumes fail in same pod Duplicate <code>volumeHandle</code> Ensure each PV has unique <code>volumeHandle</code> value <code>subPath</code> returns \"No such file or directory\" Empty directory removed by Mountpoint Use <code>prefix</code> mount option instead of <code>subPath</code> (see below) Volume not mounting Misconfigured PV/PVC Check <code>storageClassName: \"\"</code> for static provisioning"},{"location":"troubleshooting/#known-limitations-and-workarounds","title":"Known Limitations and Workarounds","text":""},{"location":"troubleshooting/#subpath-behavior","title":"SubPath Behavior","text":"<p>When using <code>subPath</code> with S3 volumes, deleting all files in the directory causes the directory itself to disappear, making the mount unusable.</p> <p>Instead of:</p> <pre><code>volumeMounts:\n  - mountPath: \"/data\"\n    subPath: some-prefix\n    name: vol\n</code></pre> <p>Use prefix mount option:</p> <pre><code># In PersistentVolume\nmountOptions:\n  - prefix=some-prefix/\n</code></pre>"},{"location":"troubleshooting/#multiple-volumes-in-same-pod","title":"Multiple Volumes in Same Pod","text":"<p>Each PersistentVolume must have a unique <code>volumeHandle</code>:</p> <pre><code># \u274c WRONG - Duplicate volumeHandle\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv-1\nspec:\n  csi:\n    volumeHandle: s3-csi-driver-volume # Duplicate!\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv-2\nspec:\n  csi:\n    volumeHandle: s3-csi-driver-volume # Duplicate!\n\n# \u2705 CORRECT - Unique volumeHandles\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv-1\nspec:\n  csi:\n    volumeHandle: s3-csi-driver-volume-1 # Unique\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv-2\nspec:\n  csi:\n    volumeHandle: s3-csi-driver-volume-2 # Unique\n</code></pre>"},{"location":"troubleshooting/#uninstallation-issues","title":"Uninstallation Issues","text":""},{"location":"troubleshooting/#namespace-stuck-terminating","title":"Namespace Stuck Terminating","text":"<pre><code># Check blocking conditions\nkubectl get namespace ${NAMESPACE} -o json | jq '.status.conditions'\n\n# Force remove finalizers (use with caution)\nkubectl get namespace ${NAMESPACE} -o json | \\\n  jq '.spec = {\"finalizers\":[]}' | \\\n  kubectl replace --raw /api/v1/namespaces/${NAMESPACE}/finalize -f -\n</code></pre>"},{"location":"troubleshooting/#persistentvolumes-stuck-terminating","title":"PersistentVolumes Stuck Terminating","text":"<pre><code># Check PV status\nkubectl describe pv &lt;pv-name&gt;\n\n# Remove finalizers if needed\nkubectl patch pv &lt;pv-name&gt; -p '{\"metadata\":{\"finalizers\":null}}'\n</code></pre>"},{"location":"troubleshooting/#orphaned-helm-release","title":"Orphaned Helm Release","text":"<pre><code># List all releases\nhelm list --all-namespaces\n\n# Manual cleanup if release is orphaned\nkubectl delete all -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver --all-namespaces\nkubectl delete sa,clusterrole,clusterrolebinding -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver --all-namespaces\n</code></pre>"},{"location":"troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging for detailed diagnostics:</p> <pre><code># In PersistentVolume\nspec:\n  mountOptions:\n    - debug\n    - debug-crt  # For AWS CRT client logs\n</code></pre> <p>View debug logs:</p> <pre><code># On the node\njournalctl -u mount-s3-* -f\n</code></pre>"},{"location":"troubleshooting/#performance-troubleshooting","title":"Performance Troubleshooting","text":"Symptom Possible Cause Action Slow file operations High S3 latency 1. Check network latency to S32. Enable caching with <code>cache</code> mount option3. Consider using closer S3 region High memory usage Large cache size Limit cache with <code>max-cache-size</code> mount option Slow directory listings No metadata caching Add <code>metadata-ttl</code> mount option (e.g., <code>metadata-ttl=60</code>)"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If issues persist after following this guide:</p> <ol> <li> <p>Collect diagnostic information:</p> <pre><code># CSI driver logs (all containers)\nkubectl logs -n kube-system -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver --all-containers=true &gt; csi-driver-logs.txt\n\n# Node plugin logs specifically\nkubectl logs -n kube-system -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver -c s3-plugin &gt; node-plugin-logs.txt\n\n# CSI node driver registrar logs\nkubectl logs -n kube-system -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver -c node-driver-registrar &gt; registrar-logs.txt\n\n# Your pod description and events\nkubectl describe pod &lt;your-pod&gt; &gt; pod-description.txt\n\n# PV and PVC details\nkubectl describe pv &lt;your-pv&gt; &gt; pv-description.txt\nkubectl describe pvc &lt;your-pvc&gt; &gt; pvc-description.txt\n\n# S3 bucket configuration (if accessible)\naws s3api get-bucket-location --bucket &lt;bucket-name&gt; --endpoint-url &lt;endpoint&gt; &gt; bucket-location.txt\naws s3api get-bucket-versioning --bucket &lt;bucket-name&gt; --endpoint-url &lt;endpoint&gt; &gt; bucket-versioning.txt\naws s3api get-bucket-policy --bucket &lt;bucket-name&gt; --endpoint-url &lt;endpoint&gt; &gt; bucket-policy.txt 2&gt;&amp;1\naws s3api list-objects-v2 --bucket &lt;bucket-name&gt; --max-items 10 --endpoint-url &lt;endpoint&gt; &gt; bucket-list-sample.txt\n</code></pre> </li> <li> <p>Contact Scality Support with, Driver version, Kubernetes version, Error messages and all collected information in Step 1, PV/PVC/Pod YAML manifests (sanitized)</p> </li> </ol>"},{"location":"concepts-and-reference/compatibility-matrix/","title":"Compatibility Matrix","text":"<p>This page documents version compatibility between the Scality CSI Driver for S3, Kubernetes, and related components.</p> <p>Note</p> <p>This compatibility matrix is updated as new versions are tested.</p>"},{"location":"concepts-and-reference/compatibility-matrix/#s3-api-compatibility","title":"S3 API Compatibility","text":"S3 Implementation Tested Versions Notes Scality RING 9.4.2 or newer Full support with driver v1.0"},{"location":"concepts-and-reference/compatibility-matrix/#kubernetes-compatibility","title":"Kubernetes Compatibility","text":"Kubernetes Version Notes 1.30 and above Full support with driver v1.0"},{"location":"concepts-and-reference/filesystem-semantics/","title":"Mountpoint for Amazon S3 File System Behavior","text":"<p>The Scality CSI Driver for S3 uses Mountpoint for Amazon S3 to present S3-compatible buckets as filesystems. All file system semantics are inherited from Mountpoint for Amazon S3, and are compatible with Scality S3 and any S3-compatible storage. This documentation describes only the behavior relevant to S3 Standard buckets. Features specific to AWS services other than S3, such as S3 Express One Zone, Glacier, or Archive storage classes, are not supported nor relevant.</p>"},{"location":"concepts-and-reference/filesystem-semantics/#behavior-tenets","title":"Behavior Tenets","text":"<ol> <li>Mountpoint does not support file behaviors that cannot be implemented efficiently against S3's object APIs.    It does not emulate operations like <code>rename</code> that would require many API calls to S3 to perform.</li> <li>Mountpoint presents a common view of S3 object data through both file and object APIs.    It does not emulate POSIX file features that have no close analog in S3's object APIs, such as mutable ownership and permissions.</li> <li>When these tenets conflict with POSIX requirements, Mountpoint fails early and explicitly.    It will cause end user applications to fail with IO errors rather than silently accept operations that will never succeed, such as extended attributes.</li> </ol>"},{"location":"concepts-and-reference/filesystem-semantics/#reading-and-writing-files","title":"Reading and Writing Files","text":"<ul> <li>Supports opening and reading existing objects, optimized for large sequential reads.</li> <li>Supports random reads and seeking within existing objects.</li> <li>Supports creating new objects by writing new files.</li> <li>With <code>--allow-overwrite</code>, supports replacing existing objects (must use <code>O_TRUNC</code> to truncate).</li> <li>Writes must always start from the beginning and be sequential.</li> <li>Uploads are asynchronous and optimized for throughput; use <code>fsync</code> to guarantee upload before closing.</li> <li>Cannot continue writing after <code>fsync</code>.</li> <li>New or overwritten objects are visible to other S3 clients only after closing or <code>fsync</code>.</li> <li>By default, deleting objects (<code>rm</code>) is not allowed; enable with <code>--allow-delete</code>.</li> <li>Delete operations immediately remove the object from S3.</li> <li>Cannot delete a file while it is being written.</li> <li>Renaming files is not supported.</li> </ul>"},{"location":"concepts-and-reference/filesystem-semantics/#directories","title":"Directories","text":"<ul> <li>S3 is flat; Mountpoint infers directories from <code>/</code> in object keys.</li> <li>Not all S3 object keys correspond to valid file names; some objects may not be accessible.</li> <li>If a directory and file share a name, only the directory is accessible.</li> <li>Creating directories (<code>mkdir</code>) is local and not persisted to S3 until a file is written inside.</li> <li>Cannot remove or rename existing directories; can remove new local directories if empty.</li> <li>No support for hard or symbolic links.</li> </ul>"},{"location":"concepts-and-reference/filesystem-semantics/#permissions-and-metadata","title":"Permissions and Metadata","text":"<ul> <li>By default, files/directories are readable only by the mounting user.</li> <li>Use <code>--allow-other</code> to allow access by other users.</li> <li>Default permissions and owners can be overridden at mount time with <code>--uid</code>, <code>--gid</code>, <code>--file-mode</code>, <code>--dir-mode</code>.</li> <li>Permissions are emulated and cannot be changed after mount.</li> <li>Mountpoint respects S3 IAM, bucket policies, and ACLs.</li> <li>Limited support for file metadata (modification times, sizes); cannot modify metadata.</li> </ul>"},{"location":"concepts-and-reference/filesystem-semantics/#consistency-and-concurrency","title":"Consistency and Concurrency","text":"<ul> <li>S3 provides strong read-after-write consistency for PUT and DELETE.</li> <li>Mountpoint provides strong read-after-write consistency for file writes, directory listings, and new object creation.</li> <li>Modifying/deleting an object with another client may result in stale metadata for up to 1 second.</li> <li>Directory listings are always up-to-date.</li> <li>Multiple readers can access the same object, but only one writer can do so at a time.</li> <li>Files being written are not available for reading until closed.</li> <li>No coordination between multiple Mountpoint mounts for the same bucket; do not write to the same object from multiple instances.</li> <li>New file uploads are atomic by default.</li> </ul>"},{"location":"concepts-and-reference/filesystem-semantics/#caching","title":"Caching","text":"<ul> <li>Optional metadata and object content caching is available.</li> <li>With caching, strong consistency is relaxed; may see stale data for up to the cache's TTL.</li> <li>Use <code>O_DIRECT</code> to force up-to-date reads.</li> <li>Caching does not affect write behavior; files being written are unavailable until closed.</li> </ul>"},{"location":"concepts-and-reference/filesystem-semantics/#durability","title":"Durability","text":"<ul> <li>Mountpoint translates file operations into S3 API calls, relying on S3's data integrity mechanisms.</li> <li>For end-to-end integrity, use SDKs and checksums.</li> </ul>"},{"location":"concepts-and-reference/filesystem-semantics/#error-handling","title":"Error Handling","text":"<ul> <li>File operations may fail due to network or S3 errors; Mountpoint uses retries and backoff.</li> <li>Use <code>fsync</code> to ensure files are uploaded; errors on <code>fsync</code> mean the file may not be uploaded.</li> </ul>"},{"location":"concepts-and-reference/filesystem-semantics/#detailed-semantics","title":"Detailed Semantics","text":""},{"location":"concepts-and-reference/filesystem-semantics/#mapping-s3-object-keys-to-files-and-directories","title":"Mapping S3 Object Keys to Files and Directories","text":"<ul> <li>Keys are split on <code>/</code> to form file system paths.</li> <li>Some S3 keys (null bytes, <code>.</code>/<code>..</code>, trailing <code>/</code>, conflicts) are not accessible.</li> <li>Directories shadow files of the same name.</li> <li>Remote directories shadow local directories/files.</li> <li>Windows-style path delimiters (<code>\\</code>) are not supported.</li> </ul>"},{"location":"concepts-and-reference/filesystem-semantics/#file-operations","title":"File Operations","text":"<ul> <li>Reads: Fully supported, including sequential and random reads.</li> <li>Writes: Sequential only; must start at beginning. Overwrites require <code>O_TRUNC</code> and <code>--allow-overwrite</code>.</li> <li>Deletes: Enabled with <code>--allow-delete</code>; immediate removal from S3.</li> <li>Renames: Not supported.</li> <li>Directory operations: Read-only operations supported; <code>mkdir</code> is local until a file is written. <code>rmdir</code> only deletes empty local directories.</li> <li>Metadata: Reading supported with limitations; modifying not supported.</li> <li>Links: Hard and symbolic links are not supported.</li> </ul>"},{"location":"concepts-and-reference/filesystem-semantics/#consistency","title":"Consistency","text":"<ul> <li>Strong read-after-write for new objects and writes.</li> <li>Stale metadata possible for up to 1 second after concurrent modification/deletion by another client.</li> </ul> <p>For more details, see the Mountpoint for Amazon S3 documentation.</p>"},{"location":"concepts-and-reference/helm-chart-configuration-reference/","title":"Helm Chart Configuration Reference","text":"<p>The Scality CSI Driver for S3 is configured primarily through the <code>values.yaml</code> file when deploying via Helm. These parameters configure the overall behavior of the CSI driver components.</p>"},{"location":"concepts-and-reference/helm-chart-configuration-reference/#global-helm-configuration","title":"Global Helm Configuration","text":"Parameter Description Default Required <code>nameOverride</code> Override the chart name. <code>\"\"</code> No <code>fullnameOverride</code> Override the full name of the release. <code>\"\"</code> No <code>imagePullSecrets</code> Secrets for pulling images from private registries. <code>[]</code> No"},{"location":"concepts-and-reference/helm-chart-configuration-reference/#container-image-configuration","title":"Container Image Configuration","text":"Parameter Description Default Required <code>image.repository</code> The container image repository for the CSI driver. <code>ghcr.io/scality/mountpoint-s3-csi-driver</code> No <code>image.pullPolicy</code> The image pull policy. <code>IfNotPresent</code> No <code>image.tag</code> The image tag for the CSI driver. Overrides the chart's <code>appVersion</code> if set. <code>1.0.1</code> No"},{"location":"concepts-and-reference/helm-chart-configuration-reference/#s3-credentials-secret-configuration","title":"S3 Credentials Secret Configuration","text":"<p>Security Note</p> <p>The Helm chart does not create secrets automatically. A Kubernetes Secret containing S3 credentials must be created before installing the chart. The secret must contain the following keys:</p> <ul> <li><code>access_key_id</code>: S3 Access Key ID.</li> <li><code>secret_access_key</code>: S3 Secret Access Key.</li> <li><code>session_token</code> (optional): S3 Session Token, if using temporary credentials.</li> </ul> Parameter Description Default Required <code>s3CredentialSecret.name</code> Name of the Kubernetes Secret containing AWS credentials (<code>access_key_id</code>, <code>secret_access_key</code>, optionally <code>session_token</code>). The secret must be created manually. <code>s3-secret</code> No <code>s3CredentialSecret.accessKeyId</code> Key within the secret for Access Key ID. <code>access_key_id</code> No <code>s3CredentialSecret.secretAccessKey</code> Key within the secret for Secret Access Key. <code>secret_access_key</code> No <code>s3CredentialSecret.sessionToken</code> Key within the secret for Session Token (optional). <code>session_token</code> No"},{"location":"concepts-and-reference/helm-chart-configuration-reference/#node-plugin-configuration","title":"Node Plugin Configuration","text":"<p>SELinux Context Note</p> <p>The <code>node.seLinuxOptions.*</code> parameters define the SELinux security context for the CSI driver containers. These settings are applied to CSI Node DaemonSet containers and allow the containers to interact with systemd and manage mount points in SELinux-enforced environments. Only the default SELinux values are tested and supported. Custom SELinux configurations are not supported. The default values are:</p> <ul> <li><code>user</code>: <code>system_u</code></li> <li><code>type</code>: <code>super_t</code></li> <li><code>role</code>: <code>system_r</code></li> <li><code>level</code>: <code>s0</code></li> </ul> Parameter Description Default Required <code>node.kubeletPath</code> The path to the kubelet directory on the host node. Used by the node plugin to register itself and manage mount points. <code>/var/lib/kubelet</code> No <code>node.logLevel</code> Log verbosity level for the CSI driver (higher numbers = more verbose). 1-2: Basic operational info (recommended for production), 3: Credential authentication info, 4: All CSI operations and mount details (default), 5: Very detailed debug info. <code>4</code> No <code>node.s3EndpointUrl</code> The RING S3 endpoint URL to be used by the driver for all mount operations. <code>\"http://s3.example.com:8000\"</code> Yes <code>node.s3Region</code> The default AWS region to use for S3 requests. Can be overridden per-volume via PV <code>mountOptions</code>. <code>us-east-1</code> No <code>node.mountpointInstallPath</code> Path on the host where the <code>mount-s3</code> binary will be installed by the initContainer. Should end with a <code>/</code>. Only used with SystemD mounter (default). <code>/opt/mountpoint-s3-csi/bin/</code> No <code>node.seLinuxOptions.user</code> SELinux user for the CSI driver container security context. <code>system_u</code> No <code>node.seLinuxOptions.type</code> SELinux type for the CSI driver container security context. <code>super_t</code> No <code>node.seLinuxOptions.role</code> SELinux role for the CSI driver container security context. <code>system_r</code> No <code>node.seLinuxOptions.level</code> SELinux level for the CSI driver container security context. <code>s0</code> No <code>node.serviceAccount.create</code> Specifies whether a ServiceAccount should be created for the node plugin. <code>true</code> No <code>node.serviceAccount.name</code> Name of the ServiceAccount to use for the node plugin. <code>s3-csi-driver-sa</code> No <code>node.nodeSelector</code> Node selector for scheduling the node plugin DaemonSet. <code>{}</code> No <code>node.resources.requests.cpu</code> CPU resource requests for the node plugin container. <code>10m</code> No <code>node.resources.requests.memory</code> Memory resource requests for the node plugin container. <code>40Mi</code> No <code>node.resources.limits.memory</code> Memory resource limits for the node plugin container. <code>256Mi</code> No <code>node.tolerateAllTaints</code> If true, the node plugin DaemonSet will tolerate all taints. Overrides <code>defaultTolerations</code>. <code>false</code> No <code>node.defaultTolerations</code> If true, adds default tolerations (<code>CriticalAddonsOnly</code>, <code>NoExecute</code> for 300s) to the node plugin. <code>true</code> No <code>node.tolerations</code> Custom tolerations for the node plugin DaemonSet. <code>[]</code> No <code>node.podInfoOnMountCompat.enable</code> Enable <code>podInfoOnMount</code> for older Kubernetes versions (&lt;1.30) if the API server supports it but Kubelet version in Helm doesn't reflect it. <code>false</code> No"},{"location":"concepts-and-reference/helm-chart-configuration-reference/#sidecar-and-init-container-configuration","title":"Sidecar and Init Container Configuration","text":"Parameter Description Default Required <code>sidecars.nodeDriverRegistrar.image.repository</code> Image repository for the <code>csi-node-driver-registrar</code> sidecar. <code>ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar</code> No <code>sidecars.nodeDriverRegistrar.image.tag</code> Image tag for the <code>csi-node-driver-registrar</code> sidecar. <code>v2.14.0</code> No <code>sidecars.nodeDriverRegistrar.image.pullPolicy</code> Image pull policy for the <code>csi-node-driver-registrar</code> sidecar. <code>IfNotPresent</code> No <code>sidecars.nodeDriverRegistrar.resources</code> Resource requests and limits for the <code>csi-node-driver-registrar</code> sidecar. <code>{}</code> (inherits from <code>node.resources</code> if not set) No <code>sidecars.livenessProbe.image.repository</code> Image repository for the <code>livenessprobe</code> sidecar. <code>ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe</code> No <code>sidecars.livenessProbe.image.tag</code> Image tag for the <code>livenessprobe</code> sidecar. <code>v2.16.0</code> No <code>sidecars.livenessProbe.image.pullPolicy</code> Image pull policy for the <code>livenessprobe</code> sidecar. <code>IfNotPresent</code> No <code>sidecars.livenessProbe.resources</code> Resource requests and limits for the <code>livenessprobe</code> sidecar. <code>{}</code> (inherits from <code>node.resources</code> if not set) No <code>initContainer.installMountpoint.resources</code> Resource requests and limits for the <code>install-mountpoint</code> initContainer. Only used with SystemD mounter (default). <code>{}</code> (inherits from <code>node.resources</code> if not set) No"},{"location":"concepts-and-reference/helm-chart-configuration-reference/#experimental-features-unsupported","title":"Experimental Features (Unsupported)","text":"<p>Important: The Pod Mounter feature is experimental and not supported for production use. It should only be used in development environments. The default SystemD mounter is the only supported configuration.</p> Parameter Description Default Required <code>experimental.podMounter</code> EXPERIMENTAL, DO NOT USE: Enables the Pod Mounter feature instead of the default SystemD mounter. Should be <code>false</code> for standard configurations. <code>false</code> No <code>controller.serviceAccount.create</code> Specifies whether a ServiceAccount should be created for the controller. Only used if <code>experimental.podMounter</code> is true. <code>true</code> No <code>controller.serviceAccount.name</code> Name of the ServiceAccount to use for the controller. Only used if <code>experimental.podMounter</code> is true. <code>s3-csi-driver-controller-sa</code> No <code>mountpointPod.namespace</code> Namespace for Mountpoint pods spawned by the controller. Only used if <code>experimental.podMounter</code> is true. <code>mount-s3</code> No <code>mountpointPod.priorityClassName</code> Priority class name for Mountpoint pods. Only used if <code>experimental.podMounter</code> is true. <code>mount-s3-critical</code> No"},{"location":"driver-deployment/installation-guide/","title":"Installation Guide","text":"<p>This guide provides comprehensive instructions for installing the Scality CSI Driver for S3 in a Kubernetes cluster with production-ready configurations and security best practices.</p>"},{"location":"driver-deployment/installation-guide/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure all requirements outlined in the Prerequisites guide are met.</p>"},{"location":"driver-deployment/installation-guide/#installation-overview","title":"Installation Overview","text":"<p>The installation process consists of:</p> <ol> <li>Setting configuration variables</li> <li>Creating a namespace for the driver (recommended for production)</li> <li>Creating S3 credentials as a Kubernetes Secret</li> <li>Configuring and installing the Helm chart</li> <li>Checking the installation of the driver</li> </ol>"},{"location":"driver-deployment/installation-guide/#step-1-set-configuration-variables","title":"Step 1. Set Configuration Variables","text":"<ul> <li> <p>Set the namespace in which the s3 credentials secret will be created and the driver will be deployed. Replace <code>scality-s3-csi</code> with the preferred namespace name.</p> <pre><code>export NAMESPACE=\"scality-s3-csi\"\n</code></pre> </li> <li> <p>Set the secret name in which the s3 credentials will be stored. Replace <code>s3-secret</code> with the preferred secret name.</p> <pre><code>export SECRET_NAME=\"s3-secret\"\n</code></pre> </li> <li> <p>Set the access key ID. Replace <code>YOUR_ACCESS_KEY_ID</code> with the actual access key ID.</p> <pre><code>export ACCESS_KEY_ID=\"YOUR_ACCESS_KEY_ID\"\n</code></pre> </li> <li> <p>Set the secret access key. Replace <code>YOUR_SECRET_ACCESS_KEY</code> with the actual secret access key.</p> <p>Note</p> <p>To avoid storing sensitive credentials in your shell history, history can be temporarily disabled before running commands with sensitive information:</p> <pre><code>set +o history # temporarily turn off history\n\n# export SECRET_ACCESS_KEY=\n\nset -o history # turn it back on\n</code></pre> <pre><code>export SECRET_ACCESS_KEY=\"YOUR_SECRET_ACCESS_KEY\"\n</code></pre> </li> <li> <p>Set the session token (optional). Replace <code>YOUR_SESSION_TOKEN</code> with the actual session token. The driver does not communicate with RING STS service to refresh the session token.</p> <pre><code># export SESSION_TOKEN=\"YOUR_SESSION_TOKEN\"\n</code></pre> </li> </ul>"},{"location":"driver-deployment/installation-guide/#step-2-create-namespace","title":"Step 2. Create Namespace","text":"<p>Creating a dedicated namespace provides better security isolation and resource management:</p> <pre><code>kubectl create namespace ${NAMESPACE}\n</code></pre>"},{"location":"driver-deployment/installation-guide/#step-3-create-s3-credentials-secret","title":"Step 3. Create S3 Credentials Secret","text":"<pre><code>kubectl create secret generic ${SECRET_NAME} \\\n  --from-literal=access_key_id=\"${ACCESS_KEY_ID}\" \\\n  --from-literal=secret_access_key=\"${SECRET_ACCESS_KEY}\" \\\n  --namespace ${NAMESPACE}\n</code></pre> <p>Temporary Credentials</p> <p>The driver does not communicate with RING S3 Connector's STS service. If session tokens are used, the credentials will not be refreshed automatically.</p> <p>OR with session token (if needed):</p> <pre><code>kubectl create secret generic ${SECRET_NAME} \\\n  --from-literal=access_key_id=\"${ACCESS_KEY_ID}\" \\\n  --from-literal=secret_access_key=\"${SECRET_ACCESS_KEY}\" \\\n  --from-literal=session_token=\"${SESSION_TOKEN}\" \\\n  --namespace ${NAMESPACE}\n</code></pre>"},{"location":"driver-deployment/installation-guide/#step-4-install-the-driver","title":"Step 4. Install the Driver","text":"<p>Choose one of the following installation options:</p>"},{"location":"driver-deployment/installation-guide/#option-a-minimal-installation","title":"Option A: Minimal Installation","text":"<p>S3 Endpoint URL</p> <p>For S3 endpoint URL, port number can be added if needed; example: <code>http://s3.example.com:8000</code> Port number can be omitted for default port <code>80</code> for HTTP or <code>443</code> for HTTPS</p> <p>Set the S3 endpoint URL:</p> <p>Replace <code>https://s3.example.com</code> with the actual RING S3 endpoint URL.</p> <pre><code>export S3_ENDPOINT_URL=\"https://s3.example.com\"\n</code></pre> <p>Install the Helm Chart:</p> <p>Deploy the driver with minimal configuration.</p> <pre><code>helm install scality-mountpoint-s3-csi-driver \\\n  oci://ghcr.io/scality/mountpoint-s3-csi-driver/helm-charts/scality-mountpoint-s3-csi-driver \\\n  --set node.s3EndpointUrl=\"${S3_ENDPOINT_URL}\" \\\n  --set s3CredentialSecret.name=\"${SECRET_NAME}\" \\\n  --namespace ${NAMESPACE}\n</code></pre>"},{"location":"driver-deployment/installation-guide/#option-b-advanced-installation","title":"Option B: Advanced Installation","text":"<p>For environments requiring custom configuration:</p> <p>Create Custom Values File:</p> <p>Create a <code>values-production.yaml</code> file with preferred configuration.</p> <pre><code># values-production.yaml\nnode:\n  # REQUIRED: Scality RING S3 endpoint URL\n  # For S3 endpoint URL, port number can be added if needed; example: `http://s3.example.com:8000`\n  # Port number can be omitted for default port `80` for HTTP or `443` for HTTPS\n  s3EndpointUrl: \"https://s3.example.com\"  # Replace with the actual endpoint\n\n  # Optional: Default AWS region for S3 requests\n  # Can be overridden per-volume at PersistentVolume level.\n  # Must match the region configured in the RING setup\n  s3Region: \"us-east-1\"  # Adjust based on the RING configuration, default is `us-east-1`\n\n  # Optional: Log verbosity level for the CSI driver (higher numbers = more verbose)\n  # Default is 4.\n  # 1-2: Basic operational info (recommended for production)\n  # 3: Credential authentication info\n  # 4: All CSI operations and mount details (default)\n  # 5: Very detailed debug info (systemd signals, mount-s3 output)\n  logLevel: 2\n\n  # Resource limits for the CSI node DaemonSet pods (one pod per worker node)\n  # These apply to the main s3-plugin container that handles volume mount operations\n  # https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n  # resources:\n\n  # Node selector for the CSI node DaemonSet - controls which nodes run the driver\n  # nodeSelector:\n  #   node-role.kubernetes.io/worker: \"true\"     # Only run on worker nodes\n  #\n  # Tolerations for the CSI node DaemonSet - allows driver to run on tainted nodes\n  # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n  # tolerations: []\n\ns3CredentialSecret:\n  # Reference the Kubernetes Secret containing S3 credentials created in Step 2\n  # This secret must exist in the same namespace as the driver installation\n  name: \"s3-secret\"\n\n# Sidecar container resources - these run alongside the main s3-plugin in each node pod\n# Each node pod contains: s3-plugin (main) + 2 sidecars (node-driver-registrar &amp; livenessprobe)\n# Resources are not set by default and are inherited from the node.resources configuration.\n# sidecars:\n  # nodeDriverRegistrar:\n  #   # Registers the CSI driver with kubelet on each node - required for volume operations\n  #   resources:\n  # livenessProbe:\n  #   # Monitors health of the CSI driver and reports to Kubernetes\n  #   resources:\n</code></pre> <p>For a complete list of configurable parameters, see the Helm Chart Configuration Reference reference.</p> <p>Install the Helm Chart:</p> <p>Deploy the driver using the custom values file.</p> <pre><code>helm install scality-mountpoint-s3-csi-driver \\\n  oci://ghcr.io/scality/mountpoint-s3-csi-driver/helm-charts/scality-mountpoint-s3-csi-driver \\\n  --values values-production.yaml \\\n  --namespace ${NAMESPACE}\n</code></pre>"},{"location":"driver-deployment/installation-guide/#step-5-verification","title":"Step 5. Verification","text":""},{"location":"driver-deployment/installation-guide/#check-driver-pods","title":"Check Driver Pods","text":"<p>Check that the driver pods are running:</p> <pre><code>kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver\n</code></pre> <p>Expected output: One <code>s3-csi-node-*</code> pod per eligible worker node, all in <code>Running</code> state.</p>"},{"location":"driver-deployment/installation-guide/#check-csi-driver-registration","title":"Check CSI Driver Registration","text":"<pre><code>kubectl get csidriver s3.csi.scality.com\n</code></pre>"},{"location":"driver-deployment/installation-guide/#check-driver-logs-optional","title":"Check Driver Logs (Optional)","text":"<p>To troubleshoot or check driver operation:</p> <pre><code># Get logs from a node plugin pod\nkubectl logs -n ${NAMESPACE} -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver -c s3-plugin\n</code></pre> <p>You should see the following output:</p> <pre><code>Using systemd mounter\nListening for connections on address: &amp;net.UnixAddr{Name:\"/csi/csi.sock\", Net:\"unix\"}\nNodeGetInfo: called with args {}\n</code></pre>"},{"location":"driver-deployment/installation-guide/#uninstallation","title":"Uninstallation","text":"<p>If Volumes Were Provisioned</p> <p>If any applications (Kubernetes pods) were using PersistentVolumes or PersistentVolumeClaims provisioned using the S3 CSI driver, follow the complete uninstallation guide to properly clean up all resources.</p> <p>If no volumes were provisioned, you can uninstall the driver with these simple steps:</p> <p>These steps assume that environment variables, <code>NAMESPACE</code> and <code>SECRET_NAME</code> are set per the installation steps above.</p> <p>Step 1. Uninstall the Helm release:</p> <pre><code>helm uninstall scality-mountpoint-s3-csi-driver -n ${NAMESPACE}\n</code></pre> <p>Step 2. Delete the S3 credentials secret:</p> <pre><code>kubectl delete secret ${SECRET_NAME} -n ${NAMESPACE}\n</code></pre> <p>Step 3. Delete the namespace (if created):</p> <pre><code>kubectl delete namespace ${NAMESPACE}  # Only if you created a dedicated namespace\n</code></pre> <p>Step 4. Check removal:</p> <p>Check that CSI driver is removed:</p> <pre><code>kubectl get csidriver s3.csi.scality.com\n</code></pre> <p>Check that no driver pods remain:</p> <pre><code>kubectl get pods --all-namespaces -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver\n</code></pre>"},{"location":"driver-deployment/installation-guide/#next-steps","title":"Next Steps","text":"<p>Volume Provisioning: See the volume provisioning guides to learn how to use S3 buckets as volumes with kubernetes applications.</p>"},{"location":"driver-deployment/prerequisites/","title":"Prerequisites","text":"<p>Before installing the Scality CSI Driver for S3, ensure the environment meets the following requirements:</p>"},{"location":"driver-deployment/prerequisites/#kubernetes-requirements","title":"Kubernetes Requirements","text":"<p>Kubernetes Version:</p> <ul> <li>Kubernetes v1.30.0 or newer is required. The driver relies on features and API versions available in these Kubernetes releases.</li> </ul> <p>Tools:</p> <ul> <li><code>kubectl</code> configured to communicate with the cluster.</li> <li>Helm v3 installed.</li> <li><code>jq</code> (optional but recommended) for parsing JSON output in troubleshooting commands.</li> </ul> <p>RBAC (Role-Based Access Control):</p> <ul> <li>The Helm chart will create the necessary <code>ServiceAccount</code>, <code>ClusterRole</code>, and <code>ClusterRoleBinding</code> for the driver to function.</li> <li>Ensure the user or tool performing the Helm installation has sufficient permissions to create these RBAC resources at the cluster scope.</li> </ul>"},{"location":"driver-deployment/prerequisites/#container-image-requirements","title":"Container Image Requirements","text":"<p>The deployment of the Scality CSI Driver for S3 requires access to several container images. Ensure the Kubernetes cluster can pull images from the following registries:</p> Component Image Registry Purpose Scality CSI Driver for S3 <code>ghcr.io/scality/mountpoint-s3-csi-driver:1.0.1</code> GitHub Container Registry (GHCR) Main CSI driver functionality CSI Node Driver Registrar <code>ghcr.io/scality/mountpoint-s3-csi-driver/csi-node-driver-registrar:v2.14.0</code> GitHub Container Registry (GHCR) Registers CSI driver with kubelet Liveness Probe <code>ghcr.io/scality/mountpoint-s3-csi-driver/livenessprobe:v2.15.0</code> GitHub Container Registry (GHCR) Health monitoring for CSI driver pods <p>Private Registry Configuration</p> <p>If using a private container registry or image mirroring, update the <code>image.repository</code> values in the Helm chart configuration accordingly. Ensure appropriate <code>imagePullSecrets</code> are configured if authentication is required.</p>"},{"location":"driver-deployment/prerequisites/#ring-storage-requirements","title":"RING Storage Requirements","text":"<p>Scality Support</p> <p>The CSI driver is only officially supported by Scality when used with Scality RING S3.</p> <p>RING version: RING v9.4.2 or newer is required.</p> <p>S3 Resources: RING S3 endpoint URL is required for CSI driver installation using Helm.</p> <p>IAM Credentials:</p> <ul> <li>IAM credentials consisting of access key ID and secret access key for an IAM entity. These credentials will be stored as a Kubernetes Secret and accessed by the driver.</li> <li>The IAM entity whose credentials are used must have appropriate permissions for the S3 operations the driver will perform.</li> <li>Optional: Session Token (required only when using temporary credentials).</li> </ul> <p>Credentials Refresh</p> <p>The driver does not automatically refresh credentials when using session token (temporary credentials).</p> <p>Network Connectivity:</p> <ul> <li>Kubernetes worker nodes must have network connectivity to the Scality RING S3 endpoint.</li> <li>This includes DNS resolution of the S3 endpoint hostname and network access to the S3 service on the appropriate ports   (typically 80 for HTTP or 443 for HTTPS, unless a specific port is specified in the S3 endpoint URL).</li> </ul>"},{"location":"driver-deployment/prerequisites/#next-steps","title":"Next Steps","text":"<p>Once all prerequisites are verified and met, proceed with:</p> <ul> <li>Quick Start Guide \u2013 Fast deployment for testing</li> <li>Installation Guide \u2013 Step-by-step installation with custom configuration</li> </ul> <p>Testing vs Production</p> <p>The quick start guide demonstrates basic installation and is recommended for testing purposes only. For production deployments follow the steps outlined in the installation guide.</p>"},{"location":"driver-deployment/quick-start/","title":"Quick Start Guide","text":"<p>This guide provides a fast way to deploy the Scality CSI Driver for S3 using Helm. It's designed for testing and evaluation purposes.</p>"},{"location":"driver-deployment/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure all requirements outlined in the Prerequisites guide are met.</p> <p>For Testing Only</p> <p>The quick start guide is intended for testing purposes only. The installation uses default values including:</p> <ul> <li>Kubernetes Namespace for driver installation: <code>default</code></li> <li>Kubernetes S3 Credentials Secret name: <code>s3-secret</code></li> <li>Default S3 Region (can be overridden at volume level): <code>us-east-1</code></li> </ul> <p>For production deployments and to customize these values or use a different namespace, see the installation guide.</p>"},{"location":"driver-deployment/quick-start/#installation","title":"Installation","text":"<p>Step 1. Set configuration variables:</p> <p>S3 Endpoint URL</p> <p>For S3 endpoint URL, port number can be added if needed; example: <code>http://s3.example.com:8000</code> Port number can be omitted for default port <code>80</code> for HTTP or <code>443</code> for HTTPS</p> <p>Replace these values with actual S3 endpoint and credentials.</p> <pre><code>export S3_ENDPOINT_URL=\"https://s3.example.com\"\nexport ACCESS_KEY_ID=\"YOUR_ACCESS_KEY_ID\"\nexport SECRET_ACCESS_KEY=\"YOUR_SECRET_ACCESS_KEY\"\n</code></pre> <p>Step 2. Create S3 credentials secret:</p> <pre><code>kubectl create secret generic s3-secret \\\n  --from-literal=access_key_id=\"${ACCESS_KEY_ID}\" \\\n  --from-literal=secret_access_key=\"${SECRET_ACCESS_KEY}\"\n</code></pre> <p>Step 3. Install the Scality S3 CSI driver:</p> <pre><code>helm install \\\n  scality-mountpoint-s3-csi-driver \\\n  oci://ghcr.io/scality/mountpoint-s3-csi-driver/helm-charts/scality-mountpoint-s3-csi-driver \\\n  --set node.s3EndpointUrl=\"${S3_ENDPOINT_URL}\"\n</code></pre> <p>Step 4. Check installation:</p> <p>Check the status of the Helm release:</p> <pre><code>helm status scality-mountpoint-s3-csi-driver\n</code></pre> <p>Check if the CSI driver pods are running:</p> <pre><code>kubectl get pods -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver\n</code></pre> <p>Expected output: One <code>s3-csi-node-*</code> pod per worker node, all in <code>Running</code> state.</p> <p>Check CSI driver registration:</p> <pre><code>kubectl get csidriver s3.csi.scality.com\n</code></pre>"},{"location":"driver-deployment/quick-start/#uninstallation","title":"Uninstallation","text":"<p>If Volumes Were Provisioned</p> <p>If any applications (Kubernetes pods) were using PersistentVolumes or PersistentVolumeClaims provisioned using the S3 CSI driver, follow the complete uninstallation guide to properly clean up all resources.</p> <p>For a quick start installation with no volumes provisioned, the driver can uninstall the driver with these simple steps:</p> <p>Step 1. Uninstall the Helm release:</p> <pre><code>helm uninstall scality-mountpoint-s3-csi-driver\n</code></pre> <p>Step 2. Delete the S3 credentials secret:</p> <pre><code>kubectl delete secret s3-secret\n</code></pre> <p>Step 3. Check removal:</p> <ul> <li> <p>Check that CSI driver is removed</p> <pre><code>kubectl get csidriver s3.csi.scality.com\n</code></pre> </li> <li> <p>Check that no driver pods remain</p> <pre><code>kubectl get pods -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver\n</code></pre> </li> </ul>"},{"location":"driver-deployment/quick-start/#next-steps","title":"Next Steps","text":"<p>Volume Provisioning: See the volume provisioning guides to learn how to use S3 buckets as volumes with applications.</p> <p>For Production Deployments: Follow the installation guide for:</p> <ul> <li>Namespace isolation</li> <li>Secure credential management</li> <li>Custom helm configurations</li> </ul>"},{"location":"driver-deployment/uninstallation/","title":"Uninstallation Guide","text":"<p>This guide provides instructions for completely removing the Scality CSI Driver for S3 and all associated resources from a Kubernetes cluster.</p>"},{"location":"driver-deployment/uninstallation/#before-you-begin","title":"Before You Begin","text":"<p>Data Persistence</p> <ul> <li>Uninstalling the CSI driver does not delete data in S3 buckets</li> <li>Existing PersistentVolumes with <code>Retain</code> policy will preserve bucket data</li> <li>Kubernetes pod applications using S3 buckets as volumes will still be able to access their data after the driver is uninstalled deleted as the driver is responsible for mounting S3 when the pod starts.</li> <li>If the driver is re-installed, pods which lost access to S3 will be able to access their data again.</li> </ul> <p>Access to Data</p> <p>If the driver is uninstalled while applications are still using S3 volumes, those applications will lose access to their to S3 if the kubernetes pods are deleted. This is due to orphaned FUSE processes.</p>"},{"location":"driver-deployment/uninstallation/#uninstallation-steps","title":"Uninstallation Steps","text":""},{"location":"driver-deployment/uninstallation/#step-1-remove-workloads-using-s3-volumes","title":"Step 1: Remove Workloads Using S3 Volumes","text":"<p>First, identify and delete all pods using S3 volumes:</p> <pre><code># Find pods with S3 PVCs\nkubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.volumes[]?.persistentVolumeClaim) | \"\\(.metadata.namespace)/\\(.metadata.name)\"'\n\n# Delete application pods that use S3 volumes\n</code></pre>"},{"location":"driver-deployment/uninstallation/#step-2-remove-pvcs-and-pvs","title":"Step 2: Remove PVCs and PVs","text":"<p>Delete all PersistentVolumeClaims using the S3 CSI driver:</p> <pre><code># List PVCs using S3 volumes\nkubectl get pvc --all-namespaces -o json | jq -r '.items[] | select(.spec.volumeName | startswith(\"s3-\")) | \"\\(.metadata.namespace)/\\(.metadata.name)\"'\n\n# Delete PVCs as needed\n</code></pre> <p>Delete PersistentVolumes:</p> <pre><code># List PVs using the S3 CSI driver\nkubectl get pv -o json | jq -r '.items[] | select(.spec.csi.driver == \"s3.csi.scality.com\") | .metadata.name'\n\n# Delete PVs as needed\n</code></pre>"},{"location":"driver-deployment/uninstallation/#step-3-uninstall-the-s3-csi-driver-helm-release","title":"Step 3: Uninstall the S3 CSI Driver Helm Release","text":"<p>Detect the namespace where the driver is installed and export it as an environment variable:</p> <pre><code>export NAMESPACE=$(kubectl get pods --all-namespaces -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver -o jsonpath='{.items[0].metadata.namespace}')\necho \"Scality CSI Driver for S3 found in namespace: ${NAMESPACE}\"\n</code></pre> <p>Get the secret name from the Helm release:</p> <pre><code>export SECRET_NAME=$(helm get values scality-mountpoint-s3-csi-driver -n ${NAMESPACE} -o json | jq -r '.s3CredentialSecret.name // \"s3-secret\"')\necho \"Secret name: ${SECRET_NAME}\"\n</code></pre> <p>Uninstall the release:</p> <pre><code>helm uninstall scality-mountpoint-s3-csi-driver -n ${NAMESPACE}\n</code></pre> <p>Delete the S3 credentials secret:</p> <pre><code>kubectl delete secret ${SECRET_NAME} -n ${NAMESPACE}\n</code></pre>"},{"location":"driver-deployment/uninstallation/#step-4-remove-namespace-optional","title":"Step 4: Remove Namespace (Optional)","text":"<p>If a dedicated namespace was created and is no longer needed:</p> <pre><code># Check namespace is empty first\nkubectl get all -n ${NAMESPACE}\n\n# Delete namespace\nkubectl delete namespace ${NAMESPACE}\n</code></pre>"},{"location":"driver-deployment/uninstallation/#step-5-check-complete-removal","title":"Step 5: Check Complete Removal","text":"<p>Ensure all CSI driver components are removed:</p> <pre><code># Check for remaining pods\nkubectl get pods --all-namespaces | grep s3-csi\n\n# Check CSI driver registration\nkubectl get csidriver s3.csi.scality.com\n\n# Check for remaining service accounts\nkubectl get sa --all-namespaces | grep s3-csi\n\n# Check for remaining cluster roles\nkubectl get clusterrole | grep s3-csi\nkubectl get clusterrolebinding | grep s3-csi\n</code></pre>"},{"location":"driver-deployment/upgrade-guide/","title":"Upgrade Guide","text":"<p>This guide provides instructions for upgrading the Scality CSI Driver for S3 to newer versions.</p>"},{"location":"driver-deployment/upgrade-guide/#prerequisites","title":"Prerequisites","text":"<p>Before upgrading, ensure all requirements outlined in the Prerequisites guide are met for the target version.</p>"},{"location":"driver-deployment/upgrade-guide/#pre-upgrade-steps","title":"Pre-Upgrade Steps","text":"<p>Step 1. Set namespace variable:</p> <p>Set the namespace where the driver is currently installed:</p> <pre><code>export NAMESPACE=\"scality-s3-csi\"  # Replace with your actual namespace\n</code></pre> <p>Step 2. Check current installation:</p> <pre><code># Get current release info\nhelm list --all-namespaces | grep scality-mountpoint-s3-csi-driver\n\n# Check current version\nkubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver -o jsonpath='{.items[0].spec.containers[0].image}'\n</code></pre> <p>Step 3. Review changes:</p> <p>Check the Release Notes for version-specific changes and breaking changes.</p> <p>Step 4. Dry run upgrade (recommended):</p> <pre><code># Test upgrade without applying changes\nhelm upgrade scality-mountpoint-s3-csi-driver \\\n  oci://ghcr.io/scality/mountpoint-s3-csi-driver/helm-charts/scality-mountpoint-s3-csi-driver \\\n  --namespace ${NAMESPACE} \\\n  --reuse-values \\\n  --dry-run\n</code></pre>"},{"location":"driver-deployment/upgrade-guide/#upgrade-options","title":"Upgrade Options","text":"<p>Warning</p> <p>If any application pods using the S3 buckets as filesystems are restarted during the upgrade they will lose access to the buckets. Once the upgrade is complete, the application pods will automatically regain access to the buckets.</p> <p>Choose one of the following upgrade options:</p>"},{"location":"driver-deployment/upgrade-guide/#option-a-upgrade-with-default-values","title":"Option A: Upgrade with Default Values","text":"<p>For installations using existing configuration:</p> <pre><code>helm upgrade scality-mountpoint-s3-csi-driver \\\n  oci://ghcr.io/scality/mountpoint-s3-csi-driver/helm-charts/scality-mountpoint-s3-csi-driver \\\n  --namespace ${NAMESPACE} \\\n  --reuse-values\n</code></pre>"},{"location":"driver-deployment/upgrade-guide/#option-b-upgrade-with-custom-values","title":"Option B: Upgrade with Custom Values","text":"<p>For installations with custom configuration file:</p> <pre><code>helm upgrade scality-mountpoint-s3-csi-driver \\\n  oci://ghcr.io/scality/mountpoint-s3-csi-driver/helm-charts/scality-mountpoint-s3-csi-driver \\\n  --values values-production.yaml \\\n  --namespace ${NAMESPACE}\n</code></pre>"},{"location":"driver-deployment/upgrade-guide/#post-upgrade-verification","title":"Post-Upgrade Verification","text":"<p>Step 1. Check upgrade status:</p> <pre><code>helm status scality-mountpoint-s3-csi-driver -n ${NAMESPACE}\n</code></pre> <p>Step 2. Verify pods are running:</p> <pre><code>kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver\n</code></pre> <p>Step 3. Check driver version:</p> <pre><code>kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver -o jsonpath='{.items[0].spec.containers[0].image}'\n</code></pre>"},{"location":"driver-deployment/upgrade-guide/#rollback-if-needed","title":"Rollback (If Needed)","text":"<p>Warning</p> <p>If any application pods using the S3 buckets as filesystems are restarted during the rollback they will lose access to the buckets. Once the rollback is complete, the application pods will automatically regain access to the buckets.</p> <p>If issues occur after upgrade you can rollback to the previous version using the following steps:</p> <pre><code># Check rollback history\nhelm history scality-mountpoint-s3-csi-driver -n ${NAMESPACE}\n\n# Rollback to previous version\nhelm rollback scality-mountpoint-s3-csi-driver -n ${NAMESPACE}\n</code></pre>"},{"location":"driver-deployment/upgrade-guide/#troubleshooting","title":"Troubleshooting","text":"<p>These are quick checks to verify the upgrade was successful. For detailed troubleshooting, refer to the troubleshooting guide.</p> <p>Check pod status:</p> <p>The driver pod should be in a <code>Running</code> state.</p> <pre><code>kubectl describe pods -n ${NAMESPACE} -l app.kubernetes.io/name=scality-mountpoint-s3-csi-driver\n</code></pre> <p>Check CSI driver registration:</p> <pre><code>kubectl get csidriver s3.csi.scality.com\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/mount-options/","title":"Mount Options Deep Dive","text":"<p>The Scality CSI Driver for S3 allows you to customize how S3 buckets are mounted by specifying mount options in the <code>PersistentVolume</code> (PV) specification. These options are passed directly to the underlying Mountpoint for Amazon S3 client.</p>"},{"location":"volume-provisioning/static-provisioning/mount-options/#how-mount-options-are-applied","title":"How Mount Options are Applied","text":"<p>Mount options are defined in the <code>spec.mountOptions</code> array within a <code>PersistentVolume</code> manifest.</p> <p>Example:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-s3-volume\nspec:\n  capacity:\n    storage: 1Pi\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\" # For static provisioning\n  persistentVolumeReclaimPolicy: Retain\n  mountOptions:\n    - \"allow-delete\"         # Allows deleting objects\n    - \"uid=1000\"             # Sets mounted files/dirs UID to 1000\n    - \"gid=1000\"             # Sets mounted files/dirs GID to 1000\n    - \"allow-other\"          # Allows non-root users to access the mount\n    - \"file-mode=0640\"       # Sets file permissions to rw-r-----\n    - \"dir-mode=0750\"        # Sets directory permissions to rwxr-x---\n    - \"region=eu-west-1\"     # Overrides S3 region for this PV\n    - \"prefix=app-data/\"     # Mounts only the 'app-data/' prefix from the bucket\n    - \"cache /mnt/host_cache/my-s3-volume\" # Enables caching to a host path\n    - \"debug\"                # Enables Mountpoint debug logging\n  csi:\n    driver: s3.csi.scality.com\n    volumeHandle: \"my-s3-bucket-unique-handle\"\n    volumeAttributes:\n      bucketName: \"my-application-bucket\"\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/mount-options/#common-mount-options","title":"Common Mount Options","text":"<p>Here's a list of commonly used Mountpoint S3 options relevant for the CSI driver:</p> Option Description Notes <code>allow-delete</code> Permit <code>unlink</code> and <code>rmdir</code> operations. Without this, operations that would delete S3 objects will fail. Crucial for read-write workloads that need to delete files/objects. <code>allow-overwrite</code> Permit overwriting existing S3 objects. Mountpoint for S3 performs all writes by <code>PUT</code>ing new objects. If an object key already exists, this flag allows replacing it. Required if your application updates files in place. <code>allow-other</code> Allow users other than the mounting user (typically root for the CSI driver process) to access the filesystem. Essential for pods running as non-root users. Must be used with appropriate <code>uid</code> and <code>gid</code> options. <code>allow-root</code> Allow the root user to access the filesystem even if <code>uid</code> and <code>gid</code> are set to non-root values. By default, if <code>uid</code>/<code>gid</code> are set, root access is restricted. Useful in specific scenarios; <code>allow-other</code> is more common for general non-root access. <code>uid=&lt;ID&gt;</code> Set the User ID for all files and directories in the mount. Must match the <code>runAsUser</code> of your pod's container if <code>allow-other</code> is not used, or the user your application expects. <code>gid=&lt;ID&gt;</code> Set the Group ID for all files and directories in the mount. Must match the <code>runAsGroup</code> or <code>fsGroup</code> of your pod's container if <code>allow-other</code> is not used, or the group your application expects. <code>file-mode=&lt;octal&gt;</code> Set the permission bits for files (e.g., <code>0644</code>). Default is <code>0644</code>. <code>dir-mode=&lt;octal&gt;</code> Set the permission bits for directories (e.g., <code>0755</code>). Default is <code>0755</code>. <code>region=&lt;value&gt;</code> Specify the S3 region for this bucket. Overrides the driver's global <code>s3Region</code> setting. Ensure this matches the actual region of your bucket. <code>prefix=&lt;value&gt;/</code> Mount only a specific \"folder\" (prefix) within the bucket. The prefix itself becomes the root of the mount. Must end with a <code>/</code>. Example: <code>prefix=myapp/data/</code>. <code>cache &lt;path&gt;</code> Enable local disk caching for S3 objects. <code>&lt;path&gt;</code> is a directory on the host node's filesystem. <code>&lt;path&gt;</code> must be unique per volume on each node. Performance and consistency implications should be understood. Requires disk space on the node. <code>metadata-ttl &lt;sec&gt;</code> Time-to-live (in seconds) for cached metadata. Default is Mountpoint's own default (typically low, e.g., 1 second). Increase for improved performance on listings if eventual consistency is acceptable. <code>max-cache-size &lt;MB&gt;</code> Maximum size (in MiB) of the local disk cache specified by <code>cache &lt;path&gt;</code>. Helps manage disk usage on nodes. <code>debug</code> Enable Mountpoint's debug logging. Logs appear in the systemd journal on the node where the pod is running and the volume is mounted. Useful for troubleshooting. <code>debug-crt</code> Enable verbose logging for the AWS Common Runtime (CRT) S3 client, which AWS mountpoint-s3 uses internally. Logs also go to systemd journal. Provides even more detailed S3 client logs. <code>aws-max-attempts &lt;N&gt;</code> Sets the <code>AWS_MAX_ATTEMPTS</code> environment variable for the Mountpoint process, configuring S3 request retries. Useful for tuning resiliency in unstable network conditions. <p>For a comprehensive list and explanation of all available Mountpoint S3 client options, refer to the official Mountpoint for Amazon S3 documentation.</p>"},{"location":"volume-provisioning/static-provisioning/mount-options/#configuration-precedence-for-mount-options","title":"Configuration Precedence for Mount Options","text":"<p>Mount options are determined by a combination of factors. Understanding their precedence is key:</p> <ol> <li><code>PersistentVolume.spec.mountOptions</code>: These have the highest precedence for volume-specific behavior. Options defined here will be directly passed to the Mountpoint client for that specific volume.</li> <li>CSI Driver Defaults: The Scality CSI Driver for S3 may apply certain default options or interpret some PV/PVC parameters to derive mount options. For example:<ul> <li>If a volume is marked as <code>readOnly: true</code> in the PV or PVC, the driver implicitly adds a read-only behavior (conceptually similar to a <code>--read-only</code> flag for</li> <li>Mountpoint, although Mountpoint's actual flag might be managed differently by the driver).</li> <li>The driver adds a <code>--user-agent-prefix</code> for telemetry.</li> </ul> </li> <li>Mountpoint Client Defaults: If an option is not specified by the PV or the CSI driver, the Mountpoint S3 client's own internal defaults will apply.</li> </ol>"},{"location":"volume-provisioning/static-provisioning/mount-options/#s3-endpoint-url-configuration","title":"S3 Endpoint URL Configuration","text":"<p>For security and consistency reasons, if <code>--endpoint-url</code> is specified in the <code>mountOptions</code> of a PersistentVolume, it will be ignored by the driver. This is enforced in both systemd and pod mounters to prevent potential security risks like endpoint redirection attacks.</p> <p>To configure a custom endpoint URL for S3 requests, set it at the driver level using one of the following methods:</p>"},{"location":"volume-provisioning/static-provisioning/mount-options/#using-helm","title":"Using Helm","text":"<pre><code># values.yaml for Helm chart\nnode:\n  s3EndpointUrl: \"https://s3.example.com:8000\"\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/mount-options/#examples","title":"Examples","text":""},{"location":"volume-provisioning/static-provisioning/mount-options/#non-root-user-access","title":"Non-Root User Access","text":"<p>To allow a pod running as a non-root user (e.g., UID 1001, GID 2002) to access the S3 mount:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\n# ...\nspec:\n  mountOptions:\n    - \"uid=1001\"\n    - \"gid=2002\"\n    - \"allow-other\" # Crucial for non-root access\n    - \"file-mode=0664\" # Example: rw-rw-r--\n    - \"dir-mode=0775\"  # Example: rwxrwxr-x\n  csi:\n    # ...\n</code></pre> <p>Your pod's <code>securityContext</code> should then specify <code>runAsUser: 1001</code> and <code>runAsGroup: 2002</code> (or <code>fsGroup: 2002</code>).</p>"},{"location":"volume-provisioning/static-provisioning/mount-options/#mounting-a-bucket-prefix","title":"Mounting a Bucket Prefix","text":"<p>To mount only the <code>my-data/raw/</code> prefix from a bucket:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\n# ...\nspec:\n  mountOptions:\n    - \"prefix=my-data/raw/\" # Note the trailing slash\n  csi:\n    # ...\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/mount-options/#enabling-local-caching","title":"Enabling Local Caching","text":"<p>To cache S3 objects on the node's local disk at <code>/mnt/s3_cache_vol_abc</code>:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\n# ...\nspec:\n  mountOptions:\n    - \"cache /mnt/s3_cache_vol_abc\"\n    - \"metadata-ttl 600\"      # Cache metadata for 10 minutes\n    - \"max-cache-size 5120\"   # Limit cache to 5 GiB\n  csi:\n    # ...\n</code></pre> <p>Cache Path Uniqueness</p> <p>The path specified for <code>cache</code> (e.g., <code>/mnt/s3_cache_vol_abc</code>) must be unique on each node for every volume that uses caching. If multiple S3 volumes on the same node attempt to use the same cache path, it will lead to undefined behavior and potential data corruption. The CSI driver does not automatically manage the uniqueness or lifecycle of these host cache paths beyond passing the option to Mountpoint. Node-level disk space and permissions for the cache path are also your responsibility.</p> <p>For guidance on filesystem behavior and permissions, see the Filesystem Semantics page</p>"},{"location":"volume-provisioning/static-provisioning/overview/","title":"Static Provisioning","text":"<p>Bucket Pre-Creation Required</p> <p>For static provisioning, the S3 bucket must be pre-created and bucket name must be provided in the PV specification.</p> <p>Static provisioning allows using an existing S3 bucket as a persistent volume in a Kubernetes cluster. The S3 bucket must be pre-created and the PersistentVolume (PV) resource manually defined.</p>"},{"location":"volume-provisioning/static-provisioning/overview/#key-requirements","title":"Key Requirements","text":"Configuration Location Value Required Description Storage Capacity <code>spec.capacity.storage</code> (PV)<code>spec.resources.requests.storage</code> (PVC) Example: <code>1200Gi</code> Yes Can be any arbitrary value as S3 is not block storage. Required by Kubernetes but ignored Access Mode <code>spec.accessModes</code> <code>ReadWriteMany</code> Yes Only access mode supported. Required for both PV and PVC Storage Class <code>spec.storageClassName</code> <code>\"\"</code> (empty) Yes Must be empty for static provisioning. Required for both PV and PVC Volume Name <code>spec.volumeName</code> (PVC only) PV name Yes Must match PV <code>metadata.name</code>. Links PVC to specific PV Claim Reference <code>spec.claimRef</code> (PV only) PVC reference Yes Binds PV to specific PVC to prevent other PVCs from claiming it"},{"location":"volume-provisioning/static-provisioning/overview/#csi-configuration","title":"CSI Configuration","text":""},{"location":"volume-provisioning/static-provisioning/overview/#speccsi-attributes","title":"<code>spec.csi</code> Attributes","text":"<p>These attributes are specific to the CSI driver and control how it interacts with the S3 bucket.</p> Attribute (<code>spec.csi.*</code>) Description Example Value Required <code>driver</code> The name of the CSI driver. Must be <code>s3.csi.scality.com</code> <code>s3.csi.scality.com</code> Yes <code>volumeHandle</code> A unique identifier for this volume within the driver. Can be any string, but it's common practice to use the bucket name or a descriptive ID <code>my-s3-bucket-pv</code> Yes <code>volumeAttributes.bucketName</code> The name of the S3 bucket to mount. Bucket must be pre-created <code>\"my-application-data\"</code> Yes <code>volumeAttributes.authenticationSource</code> Specifies the source of AWS credentials for this volume. If set to <code>\"secret\"</code>, <code>nodePublishSecretRef</code> must also be provided. If omitted or set to <code>\"driver\"</code>, global driver credentials are used <code>\"secret\"</code> or <code>\"driver\"</code> (or omit) No <code>nodePublishSecretRef.name</code> The name of the Kubernetes Secret containing S3 credentials (<code>access_key_id</code>, <code>secret_access_key</code>) for this specific volume. Used when <code>authenticationSource</code> is <code>\"secret\"</code> <code>\"my-volume-credentials\"</code> Conditionally <code>nodePublishSecretRef.namespace</code> The namespace of the Kubernetes Secret specified in <code>name</code>. Must be the same namespace as the PersistentVolumeClaim that will bind to this PV <code>\"my-secret-namespace\"</code> Conditionally"},{"location":"volume-provisioning/static-provisioning/overview/#specmountoptions","title":"<code>spec.mountOptions</code>","text":"<p>Additional options to customize S3 mounting behavior. See mount-options.md for the complete list of supported options.</p>"},{"location":"volume-provisioning/static-provisioning/overview/#basic-structure","title":"Basic Structure","text":"<p>Static provisioning workflow uses three Kubernetes resources:</p> <ol> <li>PersistentVolume (PV) - Defines the S3 bucket and configuration</li> <li>PersistentVolumeClaim (PVC) - Requests the PV for pod usage</li> <li>Pod/Application - Consumes the storage by mounting the PVC</li> </ol>"},{"location":"volume-provisioning/static-provisioning/overview/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # Arbitrary value - not used for S3\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\"\n  claimRef:\n    namespace: default\n    name: s3-pvc\n  mountOptions:\n    - allow-delete\n    - region us-west-2\n    # See mount-options.md for all available options\n  csi:\n    driver: s3.csi.scality.com\n    volumeHandle: s3-csi-driver-volume-basic # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\"\n  resources:\n    requests:\n      storage: 1200Gi # Ignored but required\n  volumeName: s3-pv # Must match PV `metadata.name`\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Hello!' &gt;&gt; /data/$(date -u).txt; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/overview/#check-if-the-bucket-can-be-accessed-in-the-pod","title":"Check if the bucket can be accessed in the pod","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- ls -al /data\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/overview/#examples","title":"Examples","text":"<ul> <li>Basic Static Provisioning - Simple S3 bucket mounting</li> <li>Shared Cache Configuration - Basic shared cache setup</li> <li>Secret-Based Authentication - Volume-level credentials</li> <li>Non-Root User Access - Non-root user configuration</li> <li>Multiple Pods Sharing Volume - Shared volume across pods</li> <li>Multiple Buckets in One Pod - Multiple buckets in single pod</li> <li>Local Caching - Advanced caching with size limits</li> <li>KMS Server-Side Encryption - AWS KMS encryption</li> <li>Retry Configuration - S3 request retry settings</li> <li>Debug Logging - Enable debug and verbose logging</li> <li>File and Directory Permissions - Custom file/directory permissions</li> <li>Allow Root Access - Root access with non-root UID/GID</li> <li>Bucket Prefix Mounting - Mount specific bucket prefix/folder</li> <li>Override S3 Region - Override S3 region for specific volumes</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/allow-root/","title":"Allow Root Access","text":"<p>This example demonstrates the <code>allow-root</code> mount option, which permits root access to the filesystem even when non-root UID/GID are specified.</p>"},{"location":"volume-provisioning/static-provisioning/examples/allow-root/#features","title":"Features","text":"<ul> <li>Root access enabled despite non-root UID/GID settings</li> <li>Files owned by specified non-root user (1000:2000)</li> <li>Root can still read/write to the volume</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/allow-root/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - uid=1000\n    - gid=2000\n    - allow-root # Allow root access even with non-root uid/gid\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-allow-root-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver-test\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Root access test' &gt; /data/root-test.txt; ls -la /data; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/allow-root/#key-mount-option","title":"Key Mount Option","text":"<ul> <li><code>allow-root</code> - Permits root user access even when <code>uid</code> and <code>gid</code> are set to non-root values</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/allow-root/#behavior","title":"Behavior","text":"<p>Without <code>allow-root</code>:</p> <ul> <li>When <code>uid=1000</code> and <code>gid=2000</code> are set, only user 1000 can access the mount</li> <li>Root access is restricted</li> </ul> <p>With <code>allow-root</code>:</p> <ul> <li>User 1000 can access the mount (as specified by uid/gid)</li> <li>Root can also access the mount despite the non-root uid/gid settings</li> <li>Files are still owned by 1000:2000</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/allow-root/#use-cases","title":"Use Cases","text":"<ul> <li>Administrative access to volumes with non-root ownership</li> <li>Debugging and troubleshooting scenarios</li> <li>Mixed access patterns where both root and specific users need access</li> <li>Container init processes that run as root but application runs as non-root</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/allow-root/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- ls -la /data\n# Files should be owned by 1000:2000 but accessible by root (UID 0)\nkubectl exec s3-app -- id\n# Should show uid=0(root) gid=0(root)\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/allow-root/#security-considerations","title":"Security Considerations","text":"<ul> <li>Use <code>allow-root</code> carefully in security-sensitive environments</li> <li>Consider using <code>allow-other</code> instead for broader non-root access</li> <li>Ensure your security policies permit root access to the volume</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/allow-root/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/allow-root/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 allow-root.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/basic-static-provisioning/","title":"Basic Static Provisioning","text":"<p>This example demonstrates the simplest static provisioning setup with an S3 bucket.</p>"},{"location":"volume-provisioning/static-provisioning/examples/basic-static-provisioning/#features","title":"Features","text":"<ul> <li>Basic S3 bucket mounting</li> <li>Read-write access with delete permissions</li> <li>Specific S3 region and prefix configuration</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/basic-static-provisioning/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # Ignored, required\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - region us-west-2\n    - prefix some-s3-prefix/\n  csi:\n    driver: s3.csi.scality.com # Required\n    volumeHandle: s3-csi-basic-static-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Hello from the container!' &gt;&gt; /data/$(date -u).txt; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/basic-static-provisioning/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- ls -al /data\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/basic-static-provisioning/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/basic-static-provisioning/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 static_provisioning.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/bucket-prefix/","title":"Bucket Prefix Mounting","text":"<p>This example demonstrates how to mount only a specific prefix (folder) from an S3 bucket using the <code>prefix</code> mount option.</p>"},{"location":"volume-provisioning/static-provisioning/examples/bucket-prefix/#features","title":"Features","text":"<ul> <li>Mounts only the <code>app-data/</code> prefix from the bucket</li> <li>The prefix becomes the root of the mount</li> <li>Isolates access to a specific \"folder\" within the bucket</li> <li>Useful for multi-tenant scenarios</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/bucket-prefix/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - allow-overwrite\n    - prefix=app-data/ # Mount only the 'app-data/' prefix from the bucket\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-prefix-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver-test\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Data in app-data prefix' &gt; /data/test-file.txt; mkdir -p /data/subdir; echo 'Nested data' &gt; /data/subdir/nested.txt; ls -la /data; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/bucket-prefix/#key-mount-option","title":"Key Mount Option","text":"<ul> <li><code>prefix=app-data/</code> - Mounts only objects with the <code>app-data/</code> prefix from the bucket</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/bucket-prefix/#how-it-works","title":"How It Works","text":"<p>Bucket Structure:</p> <pre><code>my-bucket/\n\u251c\u2500\u2500 app-data/           \u2190 This becomes the mount root\n\u2502   \u251c\u2500\u2500 file1.txt\n\u2502   \u2514\u2500\u2500 subdir/\n\u2502       \u2514\u2500\u2500 file2.txt\n\u251c\u2500\u2500 other-data/         \u2190 Not visible in the mount\n\u2502   \u2514\u2500\u2500 file3.txt\n\u2514\u2500\u2500 root-file.txt       \u2190 Not visible in the mount\n</code></pre> <p>Mount View:</p> <pre><code>/data/                  \u2190 Mount point\n\u251c\u2500\u2500 file1.txt           \u2190 Actually app-data/file1.txt in S3\n\u2514\u2500\u2500 subdir/\n    \u2514\u2500\u2500 file2.txt       \u2190 Actually app-data/subdir/file2.txt in S3\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/bucket-prefix/#important-notes","title":"Important Notes","text":"<ul> <li>The prefix must end with a forward slash (<code>/</code>)</li> <li>Files created in the mount will be stored with the prefix in S3</li> <li>Only objects with the specified prefix are visible</li> <li>The prefix itself becomes the root directory of the mount</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/bucket-prefix/#use-cases","title":"Use Cases","text":"<ul> <li>Multi-tenancy: Different applications accessing different prefixes of the same bucket</li> <li>Data organization: Isolating different types of data within a bucket</li> <li>Security: Restricting access to specific parts of a bucket</li> <li>Migration: Gradually moving data by mounting specific prefixes</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/bucket-prefix/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- ls -la /data\nkubectl exec s3-app -- cat /data/test-file.txt\n# Files will be stored as app-data/test-file.txt in the S3 bucket\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/bucket-prefix/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/bucket-prefix/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 bucket-prefix.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/debug-logging/","title":"Debug Logging","text":"<p>This example demonstrates how to enable debug logging for troubleshooting S3 CSI driver issues.</p>"},{"location":"volume-provisioning/static-provisioning/examples/debug-logging/#features","title":"Features","text":"<ul> <li>Mountpoint debug logging enabled</li> <li>CRT client verbose logging</li> <li>Useful for troubleshooting connectivity and performance issues</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/debug-logging/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - region us-west-2\n    - debug # Enable Mountpoint debug logging\n    - debug-crt # Enable verbose AWS CRT client logging\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-debug-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver-test\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Hello from the container!' &gt;&gt; /data/$(date -u).txt; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/debug-logging/#key-mount-options","title":"Key Mount Options","text":"<ul> <li><code>debug</code> - Enables Mountpoint debug logging to systemd journal</li> <li><code>debug-crt</code> - Enables verbose AWS Common Runtime (CRT) S3 client logging</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/debug-logging/#viewing-debug-logs","title":"Viewing Debug Logs","text":"<p>Debug logs are written to the systemd journal on the node where the pod is running:</p> <pre><code># Find the node where the pod is running\nkubectl get pod s3-app -o wide\n\n# SSH to the node and view logs\nsudo journalctl -u kubelet -f | grep mountpoint\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/debug-logging/#use-cases","title":"Use Cases","text":"<ul> <li>Troubleshooting mount failures</li> <li>Debugging S3 connectivity issues</li> <li>Performance analysis</li> <li>Understanding S3 request patterns</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/debug-logging/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- ls -la /data\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/debug-logging/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/debug-logging/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 debug-logging.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/file-permissions/","title":"File and Directory Permissions","text":"<p>This example demonstrates how to configure custom file and directory permissions using mount options.</p>"},{"location":"volume-provisioning/static-provisioning/examples/file-permissions/#features","title":"Features","text":"<ul> <li>Custom file permissions (0640 - rw-r-----)</li> <li>Custom directory permissions (0750 - rwxr-x---)</li> <li>File overwrite capability enabled</li> <li>Non-root user access with specific UID/GID</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/file-permissions/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - allow-overwrite\n    - uid=1000\n    - gid=2000\n    - allow-other\n    - file-mode=0640 # Files: rw-r-----\n    - dir-mode=0750  # Directories: rwxr-x---\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-file-perms-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver-test\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  securityContext:\n    runAsUser: 1000\n    runAsGroup: 2000\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Hello from the container!' &gt; /data/test.txt; mkdir -p /data/testdir; ls -la /data; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/file-permissions/#key-mount-options","title":"Key Mount Options","text":"<ul> <li><code>file-mode=0640</code> - Sets file permissions to rw-r----- (owner: read/write, group: read, others: none)</li> <li><code>dir-mode=0750</code> - Sets directory permissions to rwxr-x--- (owner: full, group: read/execute, others: none)</li> <li><code>allow-overwrite</code> - Permits overwriting existing S3 objects</li> <li><code>uid=1000</code> / <code>gid=2000</code> - Sets ownership to specific user and group IDs</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/file-permissions/#permission-breakdown","title":"Permission Breakdown","text":"Permission Files (0640) Directories (0750) Owner rw- (read/write) rwx (read/write/execute) Group r-- (read only) r-x (read/execute) Others --- (no access) --- (no access)"},{"location":"volume-provisioning/static-provisioning/examples/file-permissions/#check-pod-level-access-to-the-mounted-s3-volume-permissions","title":"Check Pod-Level Access to the Mounted S3 Volume Permissions","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- ls -la /data\n# Should show:\n# -rw-r----- 1 1000 2000 ... test.txt\n# drwxr-x--- 2 1000 2000 ... testdir\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/file-permissions/#use-cases","title":"Use Cases","text":"<ul> <li>Restricting file access to specific users/groups</li> <li>Compliance with security policies</li> <li>Multi-tenant environments with shared storage</li> <li>Applications requiring specific permission models</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/file-permissions/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/file-permissions/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 file-permissions.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/kms-encryption/","title":"KMS Server-Side Encryption","text":"<p>This example demonstrates using KMS for server-side encryption of S3 objects.</p>"},{"location":"volume-provisioning/static-provisioning/examples/kms-encryption/#features","title":"Features","text":"<ul> <li>Server-side encryption with KMS</li> <li>Customer-managed KMS key</li> <li>Secure data encryption at rest</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/kms-encryption/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - region us-west-2\n    - sse aws:kms # Use customer managed KMS key for server side encryption\n    - sse-kms-key-id arn:aws:kms:us-west-2:012345678900:key/00000000-0000-0000-0000-000000000000 # set key id (optional)\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-kms-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver-test\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Hello from the container!' &gt;&gt; /data/$(date -u).txt; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/kms-encryption/#key-mount-options","title":"Key Mount Options","text":"<ul> <li><code>sse aws:kms</code> - Enables KMS server-side encryption</li> <li><code>sse-kms-key-id &lt;ARN&gt;</code> - Specifies the KMS key ARN (optional)</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/kms-encryption/#prerequisites","title":"Prerequisites","text":"<ul> <li>KMS key must exist in the same region as the S3 bucket</li> <li>IAM permissions for KMS key usage</li> <li>Replace the KMS key ARN with your actual key</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/kms-encryption/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- echo \"encrypted data\" &gt; /data/test.txt\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/kms-encryption/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/kms-encryption/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 kms_sse.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/local-caching/","title":"Local Caching","text":"<p>This example demonstrates how to enable local disk caching for improved S3 performance.</p>"},{"location":"volume-provisioning/static-provisioning/examples/local-caching/#features","title":"Features","text":"<ul> <li>Local disk caching at <code>/tmp/s3-pv1-cache</code></li> <li>Configurable cache size (500 MB)</li> <li>Metadata TTL of 3 seconds</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/local-caching/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - cache /tmp/s3-pv1-cache # specify cache directory, relative to root host filesystem\n                              # the directory must be unique per mount on a host\n    - metadata-ttl 3 # 3 second time to live\n    - max-cache-size 500 # 500 MB maximum size\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-caching-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Hello from the container!' &gt;&gt; /data/$(date -u).txt; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/local-caching/#key-mount-options","title":"Key Mount Options","text":"<ul> <li><code>cache /tmp/s3-pv1-cache</code> - Cache directory path (must be unique per volume)</li> <li><code>metadata-ttl 3</code> - Cache metadata for 3 seconds</li> <li><code>max-cache-size 500</code> - Limit cache to 500 MB</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/local-caching/#important-notes","title":"Important Notes","text":"<p>\u26a0\ufe0f Cache Path Uniqueness: Each volume must use a unique cache path on each node to avoid conflicts.</p>"},{"location":"volume-provisioning/static-provisioning/examples/local-caching/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-app\n# Check cache directory on the node\nkubectl exec s3-app -- ls -la /data\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/local-caching/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/local-caching/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 caching.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-buckets/","title":"Multiple Buckets in One Pod","text":"<p>This example shows how to mount multiple S3 buckets in a single pod.</p>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-buckets/#features","title":"Features","text":"<ul> <li>Two separate S3 buckets mounted in one pod</li> <li>Unique volume handles for each bucket</li> <li>Different mount paths (<code>/data</code> and <code>/data2</code>)</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-buckets/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - region us-west-2\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-multi-bucket-1-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv-2\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc-2 # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - region us-west-2\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-multi-bucket-2-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver-2\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc-2\nspec:\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # ignored, required\n  volumeName: s3-pv-2 # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Hello from the container!' &gt;&gt; /data/$(date -u).txt; echo 'Hello from the container!' &gt;&gt; /data2/$(date -u).txt; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n        - name: persistent-storage-2\n          mountPath: /data2\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\n    - name: persistent-storage-2\n      persistentVolumeClaim:\n        claimName: s3-pvc-2\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-buckets/#key-points","title":"Key Points","text":"<ul> <li>Each PV must have a unique <code>volumeHandle</code></li> <li>Each bucket requires separate PV and PVC resources</li> <li>Pod mounts both volumes at different paths</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-buckets/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- ls -la /data\nkubectl exec s3-app -- ls -la /data2\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-buckets/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc s3-pvc-2\nkubectl delete pv s3-pv s3-pv-2\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-buckets/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 multiple_buckets_one_pod.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-pods-shared-volume/","title":"Multiple Pods Sharing One Volume","text":"<p>This example shows how to share a single S3 volume across multiple pods using a Deployment.</p>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-pods-shared-volume/#features","title":"Features","text":"<ul> <li>Single PV shared across multiple pods</li> <li>Uses Deployment for pod management</li> <li>3 replicas accessing the same S3 bucket</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-pods-shared-volume/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - region us-east-1\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-shared-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-bucket-name\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s3-app\n  labels:\n    app: s3-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: s3-app\n  template:\n    metadata:\n      labels:\n        app: s3-app\n    spec:\n      containers:\n      - name: s3-app\n        image: ubuntu\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"echo 'Hello from the container!' &gt;&gt; /data/$(date -u).txt; tail -f /dev/null\"]\n        volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n        ports:\n        - containerPort: 80\n      volumes:\n      - name: persistent-storage\n        persistentVolumeClaim:\n          claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-pods-shared-volume/#key-points","title":"Key Points","text":"<ul> <li><code>ReadWriteMany</code> access mode allows multiple pods to mount the volume</li> <li>All pods share the same S3 bucket data</li> <li>Deployment manages pod lifecycle and scaling</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-pods-shared-volume/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get deployment s3-app\nkubectl get pods -l app=s3-app\nkubectl exec deployment/s3-app -- ls -la /data\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-pods-shared-volume/#scale-the-deployment","title":"Scale the Deployment","text":"<pre><code>kubectl scale deployment s3-app --replicas=5\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-pods-shared-volume/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete deployment s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/multiple-pods-shared-volume/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 multiple_pods_one_pv.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/non-root-user/","title":"Non-Root User Access","text":"<p>This example shows how to configure S3 volumes for pods running as non-root users.</p>"},{"location":"volume-provisioning/static-provisioning/examples/non-root-user/#features","title":"Features","text":"<ul> <li>Pod runs as non-root user (UID 1000, GID 2000)</li> <li>Mount options configured for non-root access</li> <li>Uses <code>allow-other</code> for proper permissions</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/non-root-user/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # Ignored, required\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - uid=1000\n    - gid=2000\n    - allow-other\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-non-root-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  securityContext:\n    runAsUser: 1000\n    runAsGroup: 2000\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Hello from the container!' &gt;&gt; /data/$(date -u).txt; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/non-root-user/#key-mount-options","title":"Key Mount Options","text":"<ul> <li><code>uid=1000</code> - Sets file ownership to user ID 1000</li> <li><code>gid=2000</code> - Sets file ownership to group ID 2000  </li> <li><code>allow-other</code> - Allows non-root users to access the mount</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/non-root-user/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- id\nkubectl exec s3-app -- ls -la /data\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/non-root-user/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/non-root-user/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 non_root.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/override-region/","title":"Override S3 Region","text":"<p>This example demonstrates how to override the S3 region for a specific volume using the <code>region</code> mount option.</p>"},{"location":"volume-provisioning/static-provisioning/examples/override-region/#features","title":"Features","text":"<ul> <li>Override driver's global S3 region setting</li> <li>Access buckets in different regions from same cluster</li> <li>Useful for multi-region deployments</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/override-region/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-region-override-pv\nspec:\n  capacity:\n    storage: 1200Gi # Arbitrary value - not used for S3\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\"\n  claimRef:\n    namespace: default\n    name: s3-region-override-pvc\n  mountOptions:\n    - allow-delete\n    - region=eu-west-1  # Override S3 region for this specific volume\n  csi:\n    driver: s3.csi.scality.com\n    volumeHandle: s3-csi-region-override-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: my-eu-west-bucket\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-region-override-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\"\n  resources:\n    requests:\n      storage: 1200Gi # Ignored but required\n  volumeName: s3-region-override-pv # Must match PV `metadata.name`\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-region-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Data from EU West region!' &gt;&gt; /data/$(date -u).txt; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-region-override-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/override-region/#key-mount-option","title":"Key Mount Option","text":"<ul> <li><code>region=eu-west-1</code> - Override S3 region for this specific volume</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/override-region/#important-notes","title":"Important Notes","text":"<p>\u26a0\ufe0f Region Must Match: The region specified must match the actual region where your S3 bucket is located.</p>"},{"location":"volume-provisioning/static-provisioning/examples/override-region/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-region-app\nkubectl exec s3-region-app -- ls -la /data\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/override-region/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-region-app\nkubectl delete pvc s3-region-override-pvc\nkubectl delete pv s3-region-override-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/override-region/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 override-region.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/retry-configuration/","title":"Retry Configuration","text":"<p>This example shows how to configure S3 request retry behavior using the <code>aws-max-attempts</code> mount option.</p>"},{"location":"volume-provisioning/static-provisioning/examples/retry-configuration/#features","title":"Features","text":"<ul> <li>Configures S3 request retries to 5 attempts</li> <li>Useful for unstable network conditions</li> <li>Improves resilience for S3 operations</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/retry-configuration/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # ignored, required\n  accessModes:\n    - ReadWriteMany # supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - region us-west-2\n    - aws-max-attempts 5 # Configure number of retries for S3 requests\n  csi:\n    driver: s3.csi.scality.com # required\n    volumeHandle: s3-csi-retry-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver-test\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Hello from the container!' &gt;&gt; /data/$(date -u).txt; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/retry-configuration/#key-mount-option","title":"Key Mount Option","text":"<ul> <li><code>aws-max-attempts 5</code> - Sets the maximum number of retry attempts for S3 requests</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/retry-configuration/#use-cases","title":"Use Cases","text":"<ul> <li>Unstable network connections</li> <li>High-latency environments</li> <li>Improved fault tolerance</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/retry-configuration/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- ls -al /data\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/retry-configuration/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/retry-configuration/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 aws_max_attempts.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/secret-authentication/","title":"Secret-Based Authentication","text":"<p>This example demonstrates using Kubernetes Secrets to provide S3 credentials for volume-level authentication.</p>"},{"location":"volume-provisioning/static-provisioning/examples/secret-authentication/#features","title":"Features","text":"<ul> <li>Volume-level S3 credentials using Kubernetes Secrets</li> <li>Different credentials per PV</li> <li>Secure credential management</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/secret-authentication/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\n# Secret Authentication Example\n# This example demonstrates using a Kubernetes Secret to provide S3 credentials for the Mountpoint S3 CSI Driver.\n# This authentication method is particularly useful for:\n# 1. The user wants to set their own credentials which are different than the driver level credentials\n# 2. Using different credentials for different persistent volumes\n\n# First, create a Secret containing the S3 credentials\napiVersion: v1\nkind: Secret\nmetadata:\n  name: s3-credentials\n  namespace: default\ntype: Opaque\nstringData:\n  # Using plain text values\n  access_key_id: \"AKIAXXXXXXXXXXXXXXXXX\"\n  secret_access_key: \"SECRETXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n\n  # You can also create the secret using kubectl:\n  # kubectl create secret generic s3-credentials \\\n  #     --from-literal=access_key_id=\"ACCESS_KEY_ID\" \\\n  #     --from-literal=secret_access_key=\"SECRET_ACCESS_KEY\"\n\n---\n# Next, create a PersistentVolume that references the Secret\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1000Gi # ignored, required\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: \"\" # Required for static provisioning\n  mountOptions:\n    - allow-delete\n  csi:\n    driver: s3.csi.scality.com\n    volumeHandle: s3-csi-secret-auth-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: my-bucket\n      authenticationSource: secret # Set auth source to use the Secret\n    nodePublishSecretRef:\n      name: s3-credentials # Reference to the Secret containing credentials\n      namespace: default\n---\n# Create a PersistentVolumeClaim that references the PV\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\n  namespace: default\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\"\n  resources:\n    requests:\n      storage: 1000Gi # Ignored, required\n  volumeName: s3-pv\n---\n# Finally, create a Pod that uses the volume\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\n  namespace: default\nspec:\n  containers:\n  - name: app\n    image: busybox\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n    volumeMounts:\n    - name: s3-storage\n      mountPath: /data\n  volumes:\n  - name: s3-storage\n    persistentVolumeClaim:\n      claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/secret-authentication/#alternative-create-secret-with-kubectl","title":"Alternative: Create Secret with kubectl","text":"<pre><code>kubectl create secret generic s3-credentials \\\n    --from-literal=access_key_id=\"YOUR_ACCESS_KEY_ID\" \\\n    --from-literal=secret_access_key=\"YOUR_SECRET_ACCESS_KEY\"\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/secret-authentication/#key-configuration","title":"Key Configuration","text":"<ul> <li><code>authenticationSource: secret</code> - Enables secret-based authentication</li> <li><code>nodePublishSecretRef</code> - References the Kubernetes Secret</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/secret-authentication/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get secret s3-credentials\nkubectl get pod s3-app\nkubectl exec s3-app -- ls -la /data\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/secret-authentication/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\nkubectl delete secret s3-credentials\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/secret-authentication/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 secret_authentication.yaml</p>"},{"location":"volume-provisioning/static-provisioning/examples/shared-cache/","title":"Shared Cache Configuration","text":"<p>This example demonstrates basic shared cache configuration for improved S3 performance.</p>"},{"location":"volume-provisioning/static-provisioning/examples/shared-cache/#features","title":"Features","text":"<ul> <li>Shared cache directory at <code>/tmp/s3-csi-driver-cache</code></li> <li>Improved read performance through local caching</li> <li>Simple cache setup without size limits</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/shared-cache/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: s3-pv\nspec:\n  capacity:\n    storage: 1200Gi # Ignored, required\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  claimRef: # To ensure no other PVCs can claim this PV\n    namespace: default # Namespace is required even though it's in \"default\" namespace.\n    name: s3-pvc # Name of your PVC\n  mountOptions:\n    - allow-delete\n    - region us-west-2\n    - cache /tmp/s3-csi-driver-cache # Local cache option for improved performance. More information: https://github.com/awslabs/mountpoint-s3/blob/main/doc/CONFIGURATION.md#caching-configuration\n  csi:\n    driver: s3.csi.scality.com # Required\n    volumeHandle: s3-csi-shared-cache-volume # Must be unique across all PVs\n    volumeAttributes:\n      bucketName: s3-csi-driver\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: s3-pvc\nspec:\n  accessModes:\n    - ReadWriteMany # Supported options: ReadWriteMany\n  storageClassName: \"\" # Required for static provisioning\n  resources:\n    requests:\n      storage: 1200Gi # Ignored, required\n  volumeName: s3-pv # Name of your PV\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-app\nspec:\n  containers:\n    - name: app\n      image: ubuntu\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"echo 'Hello from the container!' &gt;&gt; /data/$(date -u).txt; echo 'Shared cache test' &gt;&gt; /data/shared_cache_test.txt; cat /data/shared_cache_test.txt; tail -f /dev/null\"]\n      volumeMounts:\n        - name: persistent-storage\n          mountPath: /data\n  volumes:\n    - name: persistent-storage\n      persistentVolumeClaim:\n        claimName: s3-pvc\nEOF\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/shared-cache/#key-mount-option","title":"Key Mount Option","text":"<ul> <li><code>cache /tmp/s3-csi-driver-cache</code> - Enables shared caching at specified directory</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/shared-cache/#benefits","title":"Benefits","text":"<ul> <li>Faster read access for frequently accessed files</li> <li>Reduced S3 API calls</li> <li>Improved application performance</li> </ul>"},{"location":"volume-provisioning/static-provisioning/examples/shared-cache/#important-notes","title":"Important Notes","text":"<p>\u26a0\ufe0f Cache Directory: Ensure the cache directory has sufficient disk space and proper permissions.</p>"},{"location":"volume-provisioning/static-provisioning/examples/shared-cache/#check-pod-level-access-to-the-mounted-s3-volume","title":"Check Pod-Level Access to the Mounted S3 Volume","text":"<pre><code>kubectl get pod s3-app\nkubectl exec s3-app -- cat /data/shared_cache_test.txt\n# Check cache directory on the node\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/shared-cache/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete pod s3-app\nkubectl delete pvc s3-pvc\nkubectl delete pv s3-pv\n</code></pre>"},{"location":"volume-provisioning/static-provisioning/examples/shared-cache/#download-yaml","title":"Download YAML","text":"<p>\ud83d\udcc1 static_provisioning_with_shared_cache.yaml</p>"}]}